# api.py

from io import BytesIO
import json
import time
from fastapi import FastAPI, Query, UploadFile, File, Form, HTTPException, BackgroundTasks, Request, logger
from fastapi.responses import JSONResponse, HTMLResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
import os
import glob
import shutil
from typing import List, Optional
import numpy as np
import pandas as pd
from sqlalchemy import text
import uvicorn
import datetime
from pred_reader import PowerDataImporter
from database import DatabaseManager

app = FastAPI(
    title="Excel2SQL API",
    description="API for importing Excel data to SQL database",
    version="1.0.0"
)

# æŒ‚è½½é™æ€æ–‡ä»¶
app.mount("/static", StaticFiles(directory="static"), name="static")

# è®¾ç½®æ¨¡æ¿
templates = Jinja2Templates(directory="templates")

# åˆå§‹åŒ–å¯¼å…¥å™¨å’Œæ•°æ®åº“ç®¡ç†å™¨
importer = PowerDataImporter()
db_manager = DatabaseManager()

@app.get("/", response_class=HTMLResponse)
async def root(request: Request):
    """è¿”å›å‰ç«¯é¡µé¢"""
    return templates.TemplateResponse("index.html", {"request": request})

# æ–°å¢ï¼šè¡¨æŸ¥è¯¢é¡µé¢
@app.get("/table_query", response_class=HTMLResponse)
async def table_query_page(request: Request, table_name: str):
    """è¿”å›è¡¨æŸ¥è¯¢é¡µé¢"""
    return templates.TemplateResponse("table_query.html", {"request": request, "table_name": table_name})

# æ–°å¢ï¼šè”è¡¨æŸ¥è¯¢é¡µé¢
@app.get("/join_query", response_class=HTMLResponse)
async def join_query_page(request: Request):
    """è¿”å›è”è¡¨æŸ¥è¯¢é¡µé¢"""
    return templates.TemplateResponse("join_query.html", {"request": request})

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    db_status = db_manager.test_connection()
    return {
        "status": "healthy" if db_status else "unhealthy",
        "database": "connected" if db_status else "disconnected"
    }

@app.get("/files")
async def list_files():
    """åˆ—å‡ºdataç›®å½•ä¸­çš„æ‰€æœ‰Excelæ–‡ä»¶"""
    data_folder = "data"
    os.makedirs(data_folder, exist_ok=True)
    excel_files = glob.glob(os.path.join(data_folder, "*.xlsx"))
    excel_files.sort(reverse=True)  # æŒ‰æ–‡ä»¶åå€’åºæ’åˆ—ï¼ˆæœ€æ–°æ—¥æœŸåœ¨å‰ï¼‰
    
    return {
        "total": len(excel_files),
        "files": [os.path.basename(file) for file in excel_files]
    }

@app.post("/upload")
async def upload_file(file: UploadFile = File(...)):
    """ä¸Šä¼ Excelæ–‡ä»¶åˆ°dataç›®å½•"""
    data_folder = "data"
    os.makedirs(data_folder, exist_ok=True)
    
    # æ£€æŸ¥æ–‡ä»¶ç±»å‹
    if not file.filename.endswith(('.xlsx')):
        raise HTTPException(status_code=400, detail="åªæ”¯æŒ.xlsxæ ¼å¼çš„Excelæ–‡ä»¶")
    
    # ä¿å­˜æ–‡ä»¶
    file_path = os.path.join(data_folder, file.filename)
    with open(file_path, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)
    
    return {"filename": file.filename, "status": "uploaded"}

import re
from pydantic import BaseModel

class SimilarDayRequest(BaseModel):
    target_date: str
    date_type: Optional[str] = None
    weights: Optional[dict] = None

@app.post("/api/similar-day")
async def find_similar_days(request: SimilarDayRequest):
    """
    æŸ¥æ‰¾ç›¸ä¼¼æ—¥
    åŒ¹é…ç»´åº¦ï¼šè´Ÿè·é¢„æµ‹ã€å¤©æ°”ã€æ¸©åº¦ã€Bç±»å æ¯”ã€æ–°èƒ½æºDæ—¥é¢„æµ‹ã€æ—¥å‰ç”µä»·
    """
    try:
        target_date_str = request.target_date
        weights = request.weights or {}
        
        # é»˜è®¤æƒé‡
        w_load = float(weights.get('load', 0.4))
        w_weather = float(weights.get('weather', 0.1))
        w_temp = float(weights.get('temp', 0.1))
        w_b_ratio = float(weights.get('b_ratio', 0.15))
        w_ne = float(weights.get('ne', 0.1))
        w_price = float(weights.get('price', 0.1))
        w_date = float(weights.get('date', 0.05)) # æ—¥æœŸè¡°å‡ç³»æ•°
        
        # æ–°å¢æƒé‡
        w_month = float(weights.get('month', 0.15)) # é»˜è®¤è€ƒè™‘æœˆä»½ç›¸ä¼¼æ€§ï¼ˆäºŒè¿›åˆ¶ï¼šåŒæœˆ=0ï¼Œä¸åŒæœˆ=1ï¼‰
        w_weekday = float(weights.get('weekday', 0.15)) # é»˜è®¤è€ƒè™‘æ˜ŸæœŸå‡ ç›¸ä¼¼æ€§ï¼ˆäºŒè¿›åˆ¶ï¼šåŒæ˜ŸæœŸå‡ =0ï¼Œä¸åŒ=1ï¼‰

        # 1. è·å–æ‰€æœ‰ç¼“å­˜æ•°æ®
        table_name = "cache_daily_hourly"
        with db_manager.engine.connect() as conn:
            # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
            tables = db_manager.get_tables()
            if table_name not in tables:
                return {"error": "ç¼“å­˜è¡¨ä¸å­˜åœ¨ï¼Œè¯·å…ˆç”Ÿæˆç¼“å­˜"}

            # è·å–å…¨é‡æ•°æ®
            # æˆ‘ä»¬éœ€è¦ä»¥ä¸‹å­—æ®µ: 
            # record_date, hour, load_forecast, weather, temperature, 
            # class_b_forecast, spot_ne_d_forecast, price_da
            
            # æ„å»ºæŸ¥è¯¢å­—æ®µ
            fields = [
                "record_date", "hour", 
                "load_forecast", "weather", "temperature",
                "class_b_forecast", "spot_ne_d_forecast", "price_da"
            ]
            
            # æ£€æŸ¥å­—æ®µæ˜¯å¦å­˜åœ¨ (é˜²æ­¢æŠ¥é”™)
            # ç®€å•èµ·è§ï¼Œä½¿ç”¨ SELECT *ï¼Œç„¶ååœ¨ Pandas é‡Œå¤„ç†
            df = pd.read_sql(f"SELECT * FROM {table_name}", conn)

        if df.empty:
            return {"error": "ç¼“å­˜è¡¨ä¸­æ— æ•°æ®"}

        # è½¬æ¢æ—¥æœŸæ ¼å¼
        df['record_date'] = pd.to_datetime(df['record_date']).dt.strftime('%Y-%m-%d')
        
        # 2. æå–ç›®æ ‡æ—¥æ•°æ®
        target_df = df[df['record_date'] == target_date_str].sort_values('hour')
        
        if target_df.empty:
            return {"error": f"ç›®æ ‡æ—¥æœŸ {target_date_str} æ— æ•°æ®ï¼Œè¯·å…ˆå¯¼å…¥é¢„æµ‹æ•°æ®"}

        # è·å–ç›®æ ‡æ—¥æœŸç±»å‹
        target_day_type = target_df['day_type'].iloc[0] if 'day_type' in target_df.columns else ''

        # 3. æ•°æ®é¢„å¤„ç†
        # éœ€è¦å°†é•¿è¡¨(long)è½¬ä¸ºå®½è¡¨(wide)ï¼Œæˆ–è€…ç›´æ¥æŒ‰æ—¥æœŸåˆ†ç»„è®¡ç®—
        
        # è¾…åŠ©å‡½æ•°ï¼šè®¡ç®—ä¸¤ä¸ªå‘é‡çš„è·ç¦» (MAPE æˆ– å½’ä¸€åŒ–æ¬§æ°è·ç¦»)
        # è¿™é‡Œä½¿ç”¨ MAPE (å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®) çš„å˜ä½“ä½œä¸ºå·®å¼‚åº¦é‡
        
        # å‡†å¤‡å†å²æ•°æ® (å¿…é¡»æ˜¯ç›®æ ‡æ—¥ä¹‹å‰çš„æ—¥æœŸ)
        history_df = df[df['record_date'] < target_date_str].copy()
        
        # è°ƒè¯•ï¼šæ˜¾ç¤ºç›®æ ‡æ—¥æœŸç±»å‹ï¼ˆä¸å†å¼ºåˆ¶è¿‡æ»¤ï¼‰
        print(f"[DEBUG] ç›®æ ‡æ—¥ç±»å‹: {target_day_type or 'æ— ç±»å‹'}")
        # ä¸å†å¼ºåˆ¶è¿‡æ»¤æ—¥æœŸç±»å‹ï¼Œå…è®¸åŒ¹é…æ‰€æœ‰å†å²æ•°æ®
        # ç”¨æˆ·å¯é€šè¿‡è®¾ç½®æœˆä»½/æ˜ŸæœŸå‡ æƒé‡ä¸º0æ¥ç¦ç”¨ç›¸å…³è¿‡æ»¤
        
        # å¿…é¡»æœ‰24å°æ—¶æ•°æ®çš„æ—¥æœŸæ‰å‚ä¸è®¡ç®—
        print(f"[DEBUG] å†å²æ•°æ®å¤©æ•°ï¼ˆ24å°æ—¶è¿‡æ»¤å‰ï¼‰: {len(history_df['record_date'].unique())}")
        valid_dates = history_df.groupby('record_date').count()['hour']
        valid_dates = valid_dates[valid_dates == 24].index.tolist()
        history_df = history_df[history_df['record_date'].isin(valid_dates)]
        print(f"[DEBUG] å†å²æ•°æ®å¤©æ•°ï¼ˆ24å°æ—¶è¿‡æ»¤åï¼‰: {len(history_df['record_date'].unique())}")
        
        if history_df.empty:
            return {"error": "æ²¡æœ‰è¶³å¤Ÿçš„å†å²æ•°æ®è¿›è¡ŒåŒ¹é…"}

        # ---------------------------
        # è®¡ç®—å„é¡¹å·®å¼‚
        # ---------------------------
        
        results = []
        print(f"[DEBUG] æƒé‡é…ç½® - load:{w_load}, temp:{w_temp}, weather:{w_weather}, "
              f"b_ratio:{w_b_ratio}, ne:{w_ne}, price:{w_price}, "
              f"date:{w_date}, month:{w_month}, weekday:{w_weekday}")
        target_date_obj = datetime.datetime.strptime(target_date_str, "%Y-%m-%d").date()
        print(f"[DEBUG] ç›®æ ‡æ—¥æœŸ: {target_date_str}, æœˆä»½: {target_date_obj.month}, æ˜ŸæœŸå‡ : {target_date_obj.weekday()}(0=å‘¨ä¸€)")

        # é¢„è®¡ç®—ç›®æ ‡å‘é‡
        t_load = target_df['load_forecast'].fillna(0).values
        t_temp = target_df['temperature'].fillna(0).values
        t_price = target_df['price_da'].fillna(0).values
        
        # Bç±»å æ¯”
        t_b = target_df['class_b_forecast'].fillna(0).values
        # é¿å…é™¤ä»¥0
        t_load_safe = np.where(t_load == 0, 1, t_load)
        t_b_ratio = t_b / t_load_safe
        
        # æ–°èƒ½æºDæ—¥
        # ä¼˜å…ˆä½¿ç”¨ spot_ne_d_forecastï¼Œå¦‚æœæ²¡æœ‰åˆ™å°è¯•ç”¨ new_energy_forecast
        if 'spot_ne_d_forecast' in target_df.columns and target_df['spot_ne_d_forecast'].sum() > 0:
            t_ne = target_df['spot_ne_d_forecast'].fillna(0).values
        elif 'new_energy_forecast' in target_df.columns:
            t_ne = target_df['new_energy_forecast'].fillna(0).values
        else:
            t_ne = np.zeros(24)

        # å¤©æ°” (å­—ç¬¦ä¸²æ•°ç»„)
        t_weather = target_df['weather'].fillna("").values
        
        # è®¡ç®—ç›®æ ‡æ—¥æœŸçš„ç»Ÿè®¡ä¿¡æ¯
        target_weather_type = ""
        if len(t_weather) > 12:
            target_weather_type = t_weather[12]  # å–ä¸­åˆæ—¶æ®µçš„å¤©æ°”ä½œä¸ºä»£è¡¨
        elif len(t_weather) > 0:
            target_weather_type = t_weather[0]   # å¦‚æœæ²¡æœ‰12ç‚¹æ•°æ®ï¼Œå–ç¬¬ä¸€ä¸ª
        
        target_avg_temp = float(np.mean(t_temp)) if len(t_temp) > 0 else 0.0
        target_avg_load = float(np.mean(t_load)) if len(t_load) > 0 else 0.0
        target_avg_price = float(np.mean(t_price)) if len(t_price) > 0 else 0.0
        target_avg_b_ratio = float(np.mean(t_b_ratio)) if len(t_b_ratio) > 0 else 0.0
        target_avg_ne = float(np.mean(t_ne)) if len(t_ne) > 0 else 0.0

        # éå†å†å²æ—¥æœŸ
        # ä¸ºäº†åŠ é€Ÿï¼Œå¯ä»¥ä½¿ç”¨ groupby Applyï¼Œä½†å¾ªç¯ç®€å•ç›´è§‚
        for date_val, group in history_df.groupby('record_date'):
            group = group.sort_values('hour')
            
            # 1. è´Ÿè·å·®å¼‚ (MAPE)
            h_load = group['load_forecast'].fillna(0).values
            # å¦‚æœè´Ÿè·ä¸ºç©ºï¼Œè·³è¿‡
            if np.sum(h_load) == 0:
                diff_load = 1.0 # æœ€å¤§å·®å¼‚
            else:
                # MAPE: mean(abs(t - h) / t) -> ä½† t å¯èƒ½ä¸º0ï¼Œä¸”æˆ‘ä»¬è¦çš„æ˜¯ç›¸ä¼¼åº¦
                # ä½¿ç”¨ å½’ä¸€åŒ–æ¬§æ°è·ç¦»: dist / (norm(t) + norm(h)) æˆ– simple MAPE
                # ç®€å•å¤„ç†ï¼šmean(abs(diff)) / mean(target)
                mean_target = np.mean(t_load) if np.mean(t_load) > 0 else 1
                diff_load = np.mean(np.abs(t_load - h_load)) / mean_target
            
            # 2. æ¸©åº¦å·®å¼‚ (RMSE + æœ€é«˜æœ€ä½å¯¹æ¯”)
            h_temp = group['temperature'].fillna(0).values
            diff_temp = np.sqrt(np.mean((t_temp - h_temp)**2))
            # æœ€é«˜æ¸©åº¦å·®å¼‚
            max_diff = np.max(t_temp) - np.max(h_temp)
            diff_temp_max = abs(max_diff)
            # æœ€ä½æ¸©åº¦å·®å¼‚
            min_diff = np.min(t_temp) - np.min(h_temp)
            diff_temp_min = abs(min_diff)
            # ç»¼åˆæ¸©åº¦å·®å¼‚å½’ä¸€åŒ– (å‡è®¾æ¸©å·®10åº¦ç®—å¤§)
            diff_temp_norm = min((diff_temp / 10.0 + diff_temp_max / 10.0 + diff_temp_min / 10.0) / 3.0, 1.0)
            
            # 3. Bç±»å æ¯”å·®å¼‚
            h_b = group['class_b_forecast'].fillna(0).values
            h_load_safe = np.where(h_load == 0, 1, h_load)
            h_b_ratio = h_b / h_load_safe
            diff_b_ratio = np.mean(np.abs(t_b_ratio - h_b_ratio)) # æœ¬èº«å°±æ˜¯æ¯”ä¾‹ï¼Œç›´æ¥å·®å€¼
            
            # 4. æ–°èƒ½æºå·®å¼‚
            # åŒæ ·å¤„ç†åˆ—å
            if 'spot_ne_d_forecast' in group.columns and group['spot_ne_d_forecast'].sum() > 0:
                h_ne = group['spot_ne_d_forecast'].fillna(0).values
            elif 'new_energy_forecast' in group.columns:
                h_ne = group['new_energy_forecast'].fillna(0).values
            else:
                h_ne = np.zeros(24)
            
            mean_ne_target = np.mean(t_ne) if np.mean(t_ne) > 0 else 1
            diff_ne = np.mean(np.abs(t_ne - h_ne)) / mean_ne_target
            
            # 5. ä»·æ ¼å·®å¼‚
            h_price = group['price_da'].fillna(0).values
            mean_price_target = np.mean(t_price) if np.mean(t_price) > 0 else 1
            diff_price = np.mean(np.abs(t_price - h_price)) / mean_price_target
            
            # 6. å¤©æ°”å·®å¼‚ (ä¸åŒ¹é…çš„å°æ—¶æ•°æ¯”ä¾‹)
            h_weather = group['weather'].fillna("").values
            # ç®€å•æ¯”è¾ƒå­—ç¬¦ä¸²æ˜¯å¦ç›¸ç­‰
            diff_weather = np.mean(t_weather != h_weather)
            
            # 7. æ—¥æœŸæƒé‡ (è¶Šè¿‘è¶Šå¥½)
            # è®¡ç®—å¤©æ•°å·®
            hist_date_obj = datetime.datetime.strptime(date_val, "%Y-%m-%d").date()
            days_diff = abs((target_date_obj - hist_date_obj).days)
            # è¡°å‡å› å­: 1 - exp(-k * days) -> è·ç¦»
            # æˆ–è€… è·ç¦»å¢åŠ : days_diff / 365
            date_penalty = min(days_diff / 365.0, 1.0)
            
            # 8. æœˆä»½å·®å¼‚ (äºŒè¿›åˆ¶: åŒæœˆ=0, ä¸åŒæœˆ=1)
            target_month = target_date_obj.month
            hist_month = hist_date_obj.month
            diff_month = 0.0 if target_month == hist_month else 1.0
            
            # 9. æ˜ŸæœŸå‡ å·®å¼‚ (äºŒè¿›åˆ¶: åŒä¸ºæ˜ŸæœŸå‡ =0, ä¸åŒ=1)
            target_weekday = target_date_obj.weekday()  # Monday=0, Sunday=6
            hist_weekday = hist_date_obj.weekday()
            diff_weekday = 0.0 if target_weekday == hist_weekday else 1.0
            
            # æ€»å·®å¼‚å¾—åˆ† (è¶Šå°è¶Šå¥½)
            # å„é¡¹ diff éƒ½åœ¨ [0, 1] å·¦å³ (MAPEå¯èƒ½å¤§äº1ï¼Œä½†é€šå¸¸åœ¨0-0.5)
            total_score = (
                w_load * diff_load +
                w_temp * diff_temp_norm +
                w_b_ratio * diff_b_ratio +
                w_ne * diff_ne +
                w_price * diff_price +
                w_weather * diff_weather +
                w_date * date_penalty +
                w_month * diff_month +
                w_weekday * diff_weekday
            )
            
            results.append({
                "date": date_val,
                "score": total_score,
                "details": {
                    "diff_load": float(diff_load),
                    "diff_temp": float(diff_temp),
                    "diff_temp_max": float(diff_temp_max),
                    "diff_temp_min": float(diff_temp_min),
                    "diff_weather": float(diff_weather),
                    "diff_price": float(diff_price),
                    "diff_b_ratio": float(diff_b_ratio),
                    "diff_ne": float(diff_ne),
                    "diff_month": float(diff_month),
                    "diff_weekday": float(diff_weekday)
                },
                # è¿”å›ä¸€äº›ç”¨äºå±•ç¤ºçš„æ•°æ®
                "load_curve": h_load.tolist(),
                "price_curve": h_price.tolist(),
                "temp_avg": float(np.mean(h_temp)),
                "weather_type": h_weather[12] if len(h_weather) > 12 else "", # å–ä¸­åˆå¤©æ°”ä½œä¸ºä»£è¡¨
                "day_type": group['day_type'].iloc[0] if 'day_type' in group.columns else ""
            })
            
        # æ’åºå¹¶è¿”å›å‰5
        results.sort(key=lambda x: x['score'])
        top_matches = results[:5]
        
        # è½¬æ¢å¾—åˆ†ä¸ºç›¸ä¼¼åº¦ (1 / (1 + score)) æˆ–è€… (1 - score)
        for r in top_matches:
            r['similarity_score'] = max(0, 1 - r['score']) # ç®€å•çº¿æ€§æ˜ å°„
            
        return {
            "target_date": target_date_str,
            "target_day_type": target_day_type,
            "target_weather_type": target_weather_type,
            "target_stats": {
                "avg_temp": target_avg_temp,
                "avg_load": target_avg_load,
                "avg_price": target_avg_price,
                "avg_b_ratio": target_avg_b_ratio,
                "avg_ne": target_avg_ne
            },
            "target_load_curve": t_load.tolist(),
            "target_price_curve": t_price.tolist(),
            "matches": top_matches
        }

    except Exception as e:
        import traceback
        traceback.print_exc()
        return JSONResponse(status_code=500, content={"error": str(e)})

@app.post("/api/update-weather")
async def update_weather(background_tasks: BackgroundTasks):
    """æ‰‹åŠ¨è§¦å‘å¤©æ°”æ•°æ®æ›´æ–°"""
    try:
        import calendar_weather
        today = datetime.date.today()
        # æ›´æ–°æœ€è¿‘30å¤©å’Œæœªæ¥15å¤©çš„æ•°æ®
        start_date = today - datetime.timedelta(days=30)
        end_date = today + datetime.timedelta(days=15)
        
        # ä½¿ç”¨åå°ä»»åŠ¡æ‰§è¡Œï¼Œé¿å…é˜»å¡
        def run_update():
            print(f"ğŸŒ¦ï¸ å¼€å§‹æ›´æ–°å¤©æ°”æ•°æ®: {start_date} -> {end_date}")
            # update_calendar å†…éƒ¨ç°åœ¨ä¼šè‡ªåŠ¨è°ƒç”¨ update_price_cache_for_date(..., only_weather=True)
            # ä»è€Œå®ç°â€œåªæ›´æ–°å¤©æ°”è¡¨ï¼Œå¹¶å­˜å…¥ç¼“å­˜è¡¨ï¼Œä¸æ›´æ–°ä»·å·®æ•°æ®â€
            calendar_weather.update_calendar(start_date, end_date)
            print("âœ… å¤©æ°”æ•°æ®åŠç¼“å­˜æ›´æ–°å®Œæˆ")
            
        background_tasks.add_task(run_update)
        
        return {"status": "success", "message": f"å¤©æ°”æ›´æ–°ä»»åŠ¡å·²å¯åŠ¨ ({start_date} è‡³ {end_date})"}
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"å¯åŠ¨å¤©æ°”æ›´æ–°ä»»åŠ¡å¤±è´¥: {str(e)}")

@app.post("/import")
async def import_file(filename: str = Form(...), background_tasks: BackgroundTasks = BackgroundTasks()):
    """å¯¼å…¥æŒ‡å®šçš„Excelæ–‡ä»¶åˆ°æ•°æ®åº“"""
    data_folder = "data"
    file_path = os.path.join(data_folder, filename)
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(file_path):
        raise HTTPException(status_code=404, detail=f"æ–‡ä»¶ {filename} ä¸å­˜åœ¨")
    
    # å®šä¹‰æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
    dated_realtime_pattern = r'\d{4}-\d{2}-\d{2}å®æ—¶èŠ‚ç‚¹ç”µä»·æŸ¥è¯¢'
    dated_dayahead_pattern = r'\d{4}-\d{2}-\d{2}æ—¥å‰èŠ‚ç‚¹ç”µä»·æŸ¥è¯¢'

    if "è´Ÿè·å®é™…ä¿¡æ¯" in filename or "è´Ÿè·é¢„æµ‹ä¿¡æ¯" in filename:
        method = importer.import_power_data
    # elif "ä¿¡æ¯æŠ«éœ²(åŒºåŸŸ)æŸ¥è¯¢å®é™…ä¿¡æ¯" in filename:
    #     method = importer.import_custom_excel
    # elif "ä¿¡æ¯æŠ«éœ²(åŒºåŸŸ)æŸ¥è¯¢é¢„æµ‹ä¿¡æ¯" in filename:
    #     method = importer.import_custom_excel_pred
    elif "ä¿¡æ¯æŠ«éœ²æŸ¥è¯¢é¢„æµ‹ä¿¡æ¯" in filename:
        method = importer.import_imformation_pred
    elif "ä¿¡æ¯æŠ«éœ²æŸ¥è¯¢å®é™…ä¿¡æ¯" in filename:
        method = importer.import_imformation_true    
    # å…ˆå¤„ç†å¸¦æ—¥æœŸçš„ç‰¹æ®Šç‰ˆæœ¬
    elif re.search(dated_realtime_pattern, filename) or re.search(dated_dayahead_pattern, filename):
        method = importer.import_point_data_new
    # ç„¶åå¤„ç†ä¸å¸¦æ—¥æœŸçš„é€šç”¨ç‰ˆæœ¬
    elif "å®æ—¶èŠ‚ç‚¹ç”µä»·æŸ¥è¯¢" in filename or "æ—¥å‰èŠ‚ç‚¹ç”µä»·æŸ¥è¯¢" in filename:
        method = importer.import_point_data
    else:
        raise HTTPException(status_code=400, detail=f"æ— åŒ¹é…çš„å¯¼å…¥è§„åˆ™: {filename}")

    # æ‰§è¡ŒåŒæ­¥å¯¼å…¥
    result = method(file_path)
    
    # æ£€æŸ¥ç»“æœæ˜¯å¦ä¸º False (è¡¨ç¤ºå¯¼å…¥å¤±è´¥)
    if result is False:
        raise HTTPException(status_code=500, detail=f"å¯¼å…¥å¤±è´¥: {filename}ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼æˆ–æ—¥å¿—")

    # [æ–°å¢é€»è¾‘] è‡ªåŠ¨è§¦å‘ç¼“å­˜æ›´æ–°
    try:
        # å°è¯•ä»æ–‡ä»¶åæå–æ—¥æœŸ
        date_match = re.search(r"(\d{4}-\d{2}-\d{2})", filename)
        if not date_match:
             date_match = re.search(r"(\d{8})", filename)
        
        target_date = None
        if date_match:
            d_str = date_match.group(1)
            if len(d_str) == 8:
                target_date = f"{d_str[:4]}-{d_str[4:6]}-{d_str[6:]}"
            else:
                target_date = d_str
        
        # åªæœ‰åœ¨å¯¼å…¥äº†èŠ‚ç‚¹ç”µä»·ç›¸å…³æ–‡ä»¶æˆ–ä¿¡æ¯æŠ«éœ²æ–‡ä»¶ï¼Œä¸”èƒ½æå–åˆ°æ—¥æœŸæ—¶ï¼Œæ‰è§¦å‘æ›´æ–°
        if target_date and ("èŠ‚ç‚¹ç”µä»·" in filename or "ä¿¡æ¯æŠ«éœ²" in filename):
            print(f"ğŸš€ è‡ªåŠ¨è§¦å‘ç¼“å­˜æ›´æ–°ä»»åŠ¡: {target_date}")
            background_tasks.add_task(update_price_cache_for_date, target_date)
            
    except Exception as e:
        print(f"âš ï¸ è‡ªåŠ¨è§¦å‘ç¼“å­˜æ›´æ–°å¤±è´¥: {e}")

    if method == importer.import_imformation_pred:
        # ç»“æœå¯èƒ½æ˜¯å•ä¸ªå››å…ƒç»„ (success, table, count, preview)
        # ä¹Ÿå¯èƒ½æ˜¯å¤šä¸ªå››å…ƒç»„çš„å…ƒç»„ ((s1,t1,c1,p1), (s2,t2,c2,p2))
        
        # æƒ…å†µ1: å•ä¸ªç»“æœ (4ä¸ªå…ƒç´ )
        if isinstance(result, tuple) and len(result) == 4 and not isinstance(result[0], tuple):
             success, table_name, record_count, preview_data = result
             
        # æƒ…å†µ2: å¤šä¸ªç»“æœ (å…ƒç»„çš„å…ƒç»„)
        elif isinstance(result, tuple) and len(result) > 0 and isinstance(result[0], tuple):
             # åˆå¹¶æ‰€æœ‰ç»“æœ
             success = all(r[0] for r in result)
             table_name = ", ".join([str(r[1]) for r in result])
             record_count = sum(r[2] for r in result)
             # åˆå¹¶é¢„è§ˆæ•°æ® (å–å‰å‡ ä¸ª)
             preview_data = []
             for r in result:
                 if r[3]:
                     preview_data.extend(r[3])
             preview_data = preview_data[:5] # åªä¿ç•™å‰5æ¡ä½œä¸ºæ€»é¢„è§ˆ
             
        else:
             raise HTTPException(status_code=500, detail=f"å¯¼å…¥è¿”å›æ ¼å¼é”™è¯¯: {result}")
    
    elif method == importer.import_imformation_true:
         if isinstance(result, tuple) and len(result) == 4:
             success, table_name, record_count, preview_data = result
         # å¤„ç†å¯èƒ½è¿”å›Noneçš„æƒ…å†µï¼ˆä¾‹å¦‚å¯¼å…¥è¿‡ç¨‹æŠ¥é”™äº†ï¼‰
         elif result is None:
             raise HTTPException(status_code=500, detail="å¯¼å…¥å¤±è´¥: å†…éƒ¨é”™è¯¯")
         # å¤„ç†è¿”å›å¤šè¡¨ç»“æœçš„æƒ…å†µ (tuple of tuples)
         elif isinstance(result, tuple) and len(result) > 0 and isinstance(result[0], tuple):
             # åˆå¹¶æ‰€æœ‰ç»“æœ
             success = all(r[0] for r in result)
             table_name = ", ".join([str(r[1]) for r in result])
             record_count = sum(r[2] for r in result)
             # åˆå¹¶é¢„è§ˆæ•°æ® (å–å‰å‡ ä¸ª)
             preview_data = []
             for r in result:
                 if r[3]:
                     preview_data.extend(r[3])
             preview_data = preview_data[:5]
         else:
             # å¦‚æœæ˜¯å…¶ä»–æ ¼å¼ï¼Œå°è¯•æ‰“å°ä¸€ä¸‹çœ‹çœ‹
             print(f"DEBUG: import_imformation_true returned: {type(result)} - {result}")
             raise HTTPException(status_code=500, detail=f"å¯¼å…¥è¿”å›æ ¼å¼é”™è¯¯: {result}")

    elif method == importer.import_custom_excel:
        if isinstance(result, tuple) and len(result) == 3:
            # è§£åŒ…ä¸‰ä¸ªç»“æœå…ƒç»„
            (success1, table_name1, record_count1, preview_data1), (success2, table_name2, record_count2, preview_data2),(success3,table_name3,record_count3,preview_data3) = result
            # åˆå¹¶ç»“æœï¼Œè¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ä¸‰ä¸ªç»“æœçš„ç»„åˆ
            success = success1 and success2 and success3
            table_name = f"{table_name1}, {table_name2}, {table_name3}"
            record_count = record_count1 + record_count2 + record_count3
            preview_data = preview_data1 + preview_data2 + preview_data3
        else:
             raise HTTPException(status_code=500, detail=f"å¯¼å…¥è¿”å›æ ¼å¼é”™è¯¯: {result}")

    elif method == importer.import_custom_excel_pred:
        if isinstance(result, tuple) and len(result) == 4:
            (success1, table_name1, record_count1, preview_data1), (success2, table_name2, record_count2, preview_data2), (success4, table_name4, record_count4, preview_data4), (success5, table_name5, record_count5, preview_data5) = result
            # åˆå¹¶ç»“æœï¼Œè¿™é‡Œæˆ‘ä»¬ä½¿ç”¨å››ä¸ªç»“æœçš„ç»„åˆ
            success = success1 and success2 and success4 and success5
            table_name = f"{table_name1}, {table_name2}, {table_name4}, {table_name5}"
            record_count = record_count1 + record_count2 + record_count4 + record_count5 
            preview_data = preview_data1 + preview_data2 + preview_data4 + preview_data5 
        else:
             raise HTTPException(status_code=500, detail=f"å¯¼å…¥è¿”å›æ ¼å¼é”™è¯¯: {result}")
    else:
        # å…¶ä»–å¯¼å…¥æ–¹æ³•çš„å¸¸è§„å¤„ç†
        if isinstance(result, tuple) and len(result) == 4:
            success, table_name, record_count, preview_data = result
        else:
            raise HTTPException(status_code=500, detail=f"å¯¼å…¥è¿”å›æ ¼å¼é”™è¯¯: {result}")
        
    if success:
        return {
            "filename": filename, 
            "status": "imported", 
            "table_name": table_name, 
            "record_count": record_count,
            "preview_data": preview_data
        }
    else:
        raise HTTPException(status_code=500, detail=f"å¯¼å…¥å¤±è´¥: {filename}")

@app.get("/tables")
async def get_tables():
    """è·å–æ‰€æœ‰æ•°æ®è¡¨"""
    tables = db_manager.get_tables()
    return {"tables": tables}

@app.get("/tables/{table_name}")
async def get_table_data(table_name: str, limit: int = 5):
    """è·å–æŒ‡å®šè¡¨çš„æ•°æ®"""
    result = db_manager.get_table_data(table_name, limit)
    return result

# æ–°å¢ï¼šè·å–è¡¨ç»“æ„ä¿¡æ¯
@app.get("/tables/{table_name}/schema")
async def get_table_schema(table_name: str):
    """è·å–æŒ‡å®šè¡¨çš„ç»“æ„ä¿¡æ¯"""
    try:
        with db_manager.engine.connect() as conn:
            result = conn.execute(text(f"DESCRIBE {table_name}"))
            schema = []
            for row in result:
                schema.append({
                    "field": row[0],
                    "type": row[1],
                    "null": row[2],
                    "key": row[3],
                    "default": row[4],
                    "extra": row[5]
                })
            return {"schema": schema}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–è¡¨ç»“æ„å¤±è´¥: {str(e)}")

# æ–°å¢ï¼šæŸ¥è¯¢è¡¨æ•°æ®æ¥å£
@app.get("/tables/{table_name}/query")
async def query_table_data(table_name: str, 
                          offset: int = 0, 
                          limit: int = 20,
                          conditions: str = None):
    """æŸ¥è¯¢æŒ‡å®šè¡¨çš„æ•°æ®ï¼Œæ”¯æŒå¤šæ¡ä»¶æŸ¥è¯¢
    conditions: JSONå­—ç¬¦ä¸²ï¼Œæ ¼å¼å¦‚ [{"column": "col1", "operator": "=", "value": "val1"}, 
                                   {"column": "col2", "operator": ">", "value": "val2"}]
    """
    try:
        with db_manager.engine.connect() as conn:
            # æ„å»ºæŸ¥è¯¢æ¡ä»¶
            where_clauses = []
            params = {}
            
            if conditions:
                import json
                try:
                    condition_list = json.loads(conditions)
                    if isinstance(condition_list, list):
                        for i, cond in enumerate(condition_list):
                            column = cond.get("column")
                            operator = cond.get("operator")
                            value = cond.get("value")
                            
                            if column and operator and value is not None:
                                # ç®€å•çš„SQLæ³¨å…¥é˜²æŠ¤
                                allowed_operators = ['=', '!=', '>', '<', '>=', '<=', 'LIKE']
                                if operator not in allowed_operators:
                                    raise HTTPException(status_code=400, detail=f"ä¸æ”¯æŒçš„æ“ä½œç¬¦: {operator}")
                                
                                param_name = f"value_{i}"
                                if operator == 'LIKE':
                                    where_clauses.append(f"{column} LIKE :{param_name}")
                                    params[param_name] = f"%{value}%"
                                else:
                                    where_clauses.append(f"{column} {operator} :{param_name}")
                                    # å°è¯•è½¬æ¢æ•°å€¼ç±»å‹
                                    try:
                                        params[param_name] = int(value)
                                    except ValueError:
                                        try:
                                            params[param_name] = float(value)
                                        except ValueError:
                                            params[param_name] = value
                except json.JSONDecodeError:
                    raise HTTPException(status_code=400, detail="æ¡ä»¶æ ¼å¼é”™è¯¯")
            
            # æ„å»ºWHEREå­å¥
            where_clause = ""
            if where_clauses:
                where_clause = "WHERE " + " AND ".join(where_clauses)
            
            # è·å–æ€»è®°å½•æ•°
            count_query = f"SELECT COUNT(*) FROM {table_name} {where_clause}"
            count_result = conn.execute(text(count_query), params)
            total_count = count_result.scalar()
            
            # è·å–åˆ†é¡µæ•°æ®
            # é»˜è®¤æ·»åŠ æ’åºï¼šä¼˜å…ˆæŒ‰record_dateå€’åºï¼Œå…¶æ¬¡æŒ‰idå€’åº
            order_clause = ""
            # ç®€å•æ£€æŸ¥è¡¨ç»“æ„ä¸­æ˜¯å¦æœ‰record_dateåˆ—ï¼ˆå¯ä»¥é€šè¿‡æŸ¥è¯¢ä¸€è¡Œæ•°æ®æˆ–describeï¼Œè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå‡è®¾å¤§éƒ¨åˆ†è¡¨éƒ½æœ‰idï¼‰
            # æ›´ç¨³å¦¥çš„æ–¹å¼æ˜¯ç›´æ¥å°è¯•ORDER BY id DESCï¼Œå¦‚æœæŠ¥é”™åˆ™å¿½ç•¥
            # ä½†ç”±äºæˆ‘ä»¬è¦æ‰§è¡ŒSQLï¼Œè¿™é‡Œæœ€å¥½ç›´æ¥æ‹¼æ¥åˆ°SQLä¸­ã€‚
            # ä¸ºäº†å…¼å®¹æ€§ï¼Œæˆ‘ä»¬å…ˆä¸å¼ºåˆ¶åŠ ORDER BYï¼Œé™¤éç”¨æˆ·æ²¡æœ‰æŒ‡å®šæ’åºï¼ˆå½“å‰æ¥å£ä¸æ”¯æŒæŒ‡å®šæ’åºï¼‰
            # æˆ‘ä»¬å¯ä»¥é»˜è®¤åŠ  ORDER BY id DESCï¼Œå› ä¸ºå¤§éƒ¨åˆ†è¡¨éƒ½æœ‰idä¸»é”®
            
            # æ£€æŸ¥æ˜¯å¦æœ‰idåˆ—æˆ–record_dateåˆ—æ¯”è¾ƒè€—æ—¶ï¼Œè¿™é‡Œç›´æ¥å°è¯•æŒ‰idå€’åºï¼Œå› ä¸ºæˆ‘ä»¬çš„å»ºè¡¨è¯­å¥éƒ½åŒ…å«id
            data_query = f"SELECT * FROM {table_name} {where_clause} ORDER BY id DESC LIMIT :limit OFFSET :offset"
            
            params.update({"limit": limit, "offset": offset})
            try:
                data_result = conn.execute(text(data_query), params)
            except Exception:
                # å¦‚æœå¤±è´¥ï¼ˆä¾‹å¦‚æ²¡æœ‰idåˆ—ï¼‰ï¼Œå›é€€åˆ°æ— æ’åº
                data_query = f"SELECT * FROM {table_name} {where_clause} LIMIT :limit OFFSET :offset"
                data_result = conn.execute(text(data_query), params)
            
            data = []
            for row in data_result:
                row_dict = dict(row._mapping)
                data.append(row_dict)
            
            return {
                "data": data,
                "total": total_count,
                "offset": offset,
                "limit": limit
            }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æŸ¥è¯¢æ•°æ®å¤±è´¥: {str(e)}")

@app.get("/tables/{table_name}/export")
async def export_table_data(table_name: str,
                           conditions: str = None):
    """å¯¼å‡ºæŒ‡å®šè¡¨çš„æ•°æ®ä¸ºExcelæ ¼å¼ï¼Œæ”¯æŒå¤šæ¡ä»¶æŸ¥è¯¢
    conditions: JSONå­—ç¬¦ä¸²ï¼Œæ ¼å¼å¦‚ [{"column": "col1", "operator": "=", "value": "val1"}, 
                                   {"column": "col2", "operator": ">", "value": "val2"}]
    """
    try:
        print(f"å¯¼å‡ºè¯·æ±‚å¼€å§‹: table_name={table_name}, conditions={conditions}")
        
        with db_manager.engine.connect() as conn:
            # æ„å»ºæŸ¥è¯¢æ¡ä»¶
            where_clauses = []
            params = {}
            
            if conditions:
                import json
                try:
                    condition_list = json.loads(conditions)
                    if isinstance(condition_list, list):
                        for i, cond in enumerate(condition_list):
                            column = cond.get("column")
                            operator = cond.get("operator")
                            value = cond.get("value")
                            
                            if column and operator and value is not None:
                                # ç®€å•çš„SQLæ³¨å…¥é˜²æŠ¤
                                allowed_operators = ['=', '!=', '>', '<', '>=', '<=', 'LIKE']
                                if operator not in allowed_operators:
                                    raise HTTPException(status_code=400, detail=f"ä¸æ”¯æŒçš„æ“ä½œç¬¦: {operator}")
                                
                                param_name = f"value_{i}"
                                if operator == 'LIKE':
                                    where_clauses.append(f"{column} LIKE :{param_name}")
                                    params[param_name] = f"%{value}%"
                                else:
                                    where_clauses.append(f"{column} {operator} :{param_name}")
                                    # å°è¯•è½¬æ¢æ•°å€¼ç±»å‹
                                    try:
                                        params[param_name] = int(value)
                                    except ValueError:
                                        try:
                                            params[param_name] = float(value)
                                        except ValueError:
                                            params[param_name] = value
                except json.JSONDecodeError:
                    raise HTTPException(status_code=400, detail="æ¡ä»¶æ ¼å¼é”™è¯¯")
            
            # æ„å»ºWHEREå­å¥
            where_clause = ""
            if where_clauses:
                where_clause = "WHERE " + " AND ".join(where_clauses)
            
            # è·å–æ‰€æœ‰æ•°æ®
            data_query = f"SELECT * FROM {table_name} {where_clause}"
            print(f"æ‰§è¡ŒæŸ¥è¯¢: {data_query}, å‚æ•°: {params}")
            data_result = conn.execute(text(data_query), params)
            
            data = []
            for row in data_result:
                row_dict = dict(row._mapping)
                data.append(row_dict)
            
            print(f"æŸ¥è¯¢ç»“æœæ•°é‡: {len(data)}")
            if len(data) > 0:
                print(f"å‰å‡ æ¡æ•°æ®ç¤ºä¾‹: {data[:2]}")
            
            # å¦‚æœæ²¡æœ‰æ•°æ®ï¼Œè¿”å›ç©ºExcel
            if not data:
                import pandas as pd
                import numpy as np
                from io import BytesIO
                df = pd.DataFrame()
                output = BytesIO()
                with pd.ExcelWriter(output, engine='openpyxl') as writer:
                    df.to_excel(writer, index=False)
                output.seek(0)
                
                from fastapi.responses import StreamingResponse
                return StreamingResponse(
                    iter([output.getvalue()]),
                    media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    headers={"Content-Disposition": f"attachment; filename={table_name}.xlsx"}
                )
            
            # è½¬æ¢ä¸ºDataFrameè¿›è¡Œå¤„ç†
            import pandas as pd
            import numpy as np
            from io import BytesIO
            import os
            from datetime import datetime
            
            df = pd.DataFrame(data)
            print(f"DataFrameåˆ—: {df.columns.tolist()}")
            print(f"DataFrameå½¢çŠ¶: {df.shape}")
            if len(df) > 0:
                print(f"DataFrameå‰å‡ è¡Œ:\n{df.head(2)}")
            
            # åˆ é™¤idåˆ—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if 'id' in df.columns:
                df = df.drop(columns=['id'])
                print("å·²åˆ é™¤idåˆ—")
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«å¿…è¦çš„åˆ—
            required_columns = ['channel_name', 'record_date', 'record_time', 'value', 'sheet_name']
            if not all(col in df.columns for col in required_columns):
                print(f"ç¼ºå°‘å¿…è¦åˆ—ï¼Œå½“å‰åˆ—: {df.columns.tolist()}")
                print("ä½¿ç”¨åŸå§‹å¯¼å‡ºæ–¹å¼")
                # å¦‚æœä¸åŒ…å«å¿…è¦åˆ—ï¼Œä½¿ç”¨åŸå§‹å¯¼å‡ºæ–¹å¼
                output = BytesIO()
                with pd.ExcelWriter(output, engine='openpyxl') as writer:
                    df.to_excel(writer, index=False)
                output.seek(0)
                
                # ç”Ÿæˆæ–‡ä»¶å
                record_date = df['record_date'].iloc[0] if 'record_date' in df.columns and len(df) > 0 else 'unknown'
                data_type = df['type'].iloc[0] if 'type' in df.columns and len(df) > 0 else 'unknown'
                
                # æ ¼å¼åŒ–record_dateä¸ºå­—ç¬¦ä¸²
                if hasattr(record_date, 'strftime'):
                    record_date_str = record_date.strftime('%Y-%m-%d')
                else:
                    record_date_str = str(record_date)
                
                filename = f"{record_date_str}_{data_type}.xlsx"
                
                import urllib.parse
                encoded_filename = urllib.parse.quote(filename)
                from fastapi.responses import StreamingResponse
                return StreamingResponse(
                    iter([output.getvalue()]),
                    media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    headers={
                        "Content-Disposition": f"attachment; filename*=UTF-8''{encoded_filename}"
                    }
                )
            
            # ç±»ä¼¼preHandle.pyçš„å¤„ç†æ–¹å¼
            # æå–å”¯ä¸€çš„sheet_nameï¼ˆå‡è®¾æ•°æ®ä¸­sheet_nameå”¯ä¸€ï¼‰
            sheet_name = df['sheet_name'].unique()[0] if len(df['sheet_name'].unique()) > 0 else 'Sheet1'
            # æå–å”¯ä¸€çš„æ—¥æœŸï¼ˆå‡è®¾æ•°æ®ä¸­æ—¥æœŸå”¯ä¸€ï¼‰
            record_date = df['record_date'].unique()[0] if len(df['record_date'].unique()) > 0 else pd.Timestamp.now().date()
            
            # æ ¼å¼åŒ–æ—¥æœŸä¸ºYYYY-MM-DD
            if hasattr(record_date, 'strftime'):
                record_date_str = record_date.strftime('%Y-%m-%d')
            else:
                record_date_str = str(record_date)
            
            # å¤„ç†æ–‡ä»¶åç‰¹æ®Šå­—ç¬¦ï¼ˆé¿å…æ–œæ ã€ç©ºæ ¼ç­‰å¯¼è‡´ä¿å­˜å¤±è´¥ï¼‰
            sheet_name_clean = str(sheet_name).replace('/', '_').replace('\\', '_').replace(' ', '')
            # æ„é€ æ–‡ä»¶åï¼š{sheet_name}({æ—¥æœŸ})_å°æ—¶.xlsx
            filename = f"{sheet_name_clean}({record_date_str})_å°æ—¶.xlsx"
            print(f"ç”Ÿæˆæ–‡ä»¶å: {filename}")
            
            # æ£€æŸ¥record_timeæ ¼å¼å¹¶å¤„ç†
            print(f"record_timeç¤ºä¾‹å€¼: {df['record_time'].head()}")
            
            # è½¬æ¢record_timeä¸ºå°æ—¶ï¼ˆå¤„ç†å„ç§å¯èƒ½çš„æ ¼å¼ï¼‰
            def extract_hour(time_value):
                if pd.isna(time_value):
                    return None
                if isinstance(time_value, str):
                    if ':' in time_value:
                        # æ ¼å¼å¦‚ "01:00", "1:00"
                        return int(time_value.split(':')[0])
                    else:
                        # å¯èƒ½æ˜¯æ•°å­—å­—ç¬¦ä¸²å¦‚ "100" è¡¨ç¤º 01:00
                        try:
                            time_int = int(time_value)
                            return time_int // 100
                        except:
                            return None
                elif isinstance(time_value, (int, float)):
                    # æ•°å­—æ ¼å¼å¦‚ 100 è¡¨ç¤º 01:00
                    return int(time_value) // 100
                else:
                    # timedeltaæˆ–å…¶ä»–æ ¼å¼
                    try:
                        # å¦‚æœæ˜¯timedeltaå¯¹è±¡
                        hours = time_value.seconds // 3600
                        return hours
                    except:
                        return None
            
            # åº”ç”¨å°æ—¶æå–å‡½æ•°
            df['hour'] = df['record_time'].apply(extract_hour)
            print(f"æå–çš„å°æ—¶åˆ—ç¤ºä¾‹: {df['hour'].head()}")
            
            # åˆ é™¤hourä¸ºNaNçš„è¡Œ
            df = df.dropna(subset=['hour'])
            print(f"åˆ é™¤æ— æ•ˆå°æ—¶åDataFrameå½¢çŠ¶: {df.shape}")
            
            # ç”Ÿæˆç”µç«™çº§é€è§†è¡¨
            if len(df) > 0:
                print("å¼€å§‹åˆ›å»ºé€è§†è¡¨")
                pivot_df = pd.pivot_table(
                    df,
                    index=['channel_name', 'record_date'],
                    columns='hour',
                    values='value',
                    aggfunc='mean'
                )
                print(f"é€è§†è¡¨åˆ›å»ºå®Œæˆï¼Œå½¢çŠ¶: {pivot_df.shape}")
                print(f"é€è§†è¡¨åˆ—: {pivot_df.columns.tolist()}")
                
                # é‡æ–°ç´¢å¼•ç¡®ä¿æœ‰24å°æ—¶åˆ—
                pivot_df = pivot_df.reindex(columns=range(24), fill_value=np.nan)
                pivot_df.columns = [f'{int(h)}:00' for h in pivot_df.columns]
                pivot_df = pivot_df.reset_index()
                
                # ä¿®æ”¹å‰ä¸¤åˆ—åç§°
                pivot_df = pivot_df.rename(columns={
                    'channel_name': 'èŠ‚ç‚¹åç§°',
                    'record_date': 'æ—¥æœŸ'
                })
                
                # æ’å…¥å•ä½åˆ—
                pivot_df.insert(
                    loc=2,
                    column='å•ä½',
                    value='ç”µä»·(å…ƒ/MWh)'
                )
                
                # æ·»åŠ å‘ç”µä¾§å…¨çœç»Ÿä¸€å‡ä»·è¡Œ
                hour_columns = [f'{h}:00' for h in range(24)]
                # ç¡®ä¿æ‰€æœ‰å°æ—¶åˆ—éƒ½å­˜åœ¨
                for col in hour_columns:
                    if col not in pivot_df.columns:
                        pivot_df[col] = np.nan
                
                # åœ¨è®¡ç®—å¹³å‡å€¼å‰ï¼Œç¡®ä¿æ‰€æœ‰åˆ—ä¸ºæ•°å€¼ç±»å‹
                for col in hour_columns:
                    pivot_df[col] = pd.to_numeric(pivot_df[col], errors='coerce')
                
                final_df = pivot_df
                print(f"æœ€ç»ˆDataFrameå½¢çŠ¶: {final_df.shape}")
                print(f"æœ€ç»ˆDataFrameåˆ—: {final_df.columns.tolist()}")
                if len(final_df) > 0:
                    print(f"æœ€ç»ˆDataFrameå‰å‡ è¡Œ:\n{final_df.head()}")
            else:
                # å¦‚æœå¤„ç†åæ²¡æœ‰æ•°æ®ï¼Œåˆ›å»ºç©ºçš„DataFrame
                print("å¤„ç†åæ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼Œåˆ›å»ºç©ºDataFrame")
                columns = ['èŠ‚ç‚¹åç§°', 'æ—¥æœŸ', 'å•ä½'] + [f'{h}:00' for h in range(24)]
                final_df = pd.DataFrame(columns=columns)
            
            # ç¡®ä¿createdæ–‡ä»¶å¤¹å­˜åœ¨
            created_folder = "created"
            if not os.path.exists(created_folder):
                os.makedirs(created_folder)
                print(f"åˆ›å»ºæ–‡ä»¶å¤¹: {created_folder}")
            
            # ç”Ÿæˆæ–‡ä»¶åï¼ˆå¸¦æ—¶é—´æˆ³é¿å…é‡å¤ï¼‰
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            file_name_with_timestamp = f"{sheet_name_clean}_{timestamp}.xlsx"
            file_path = os.path.join(created_folder, file_name_with_timestamp)
            print(f"ç”Ÿæˆæ–‡ä»¶è·¯å¾„: {file_path}")
            
            # å°†å¤„ç†åçš„final_dfä¿å­˜åˆ°æœåŠ¡å™¨æ–‡ä»¶å¤¹
            print("å¼€å§‹ç”ŸæˆExcelæ–‡ä»¶åˆ°æœåŠ¡å™¨")
            try:
                # ä½¿ç”¨openpyxlå¼•æ“ç›´æ¥å¯¼å‡º
                with pd.ExcelWriter(file_path, engine='openpyxl') as writer:
                    final_df.to_excel(writer, index=False, sheet_name=sheet_name_clean[:31])
                print(f"Excelæ–‡ä»¶ç”Ÿæˆå®Œæˆ: {file_path}")
                
            except Exception as e:
                print(f"Excelæ–‡ä»¶ç”Ÿæˆå¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                
                # å›é€€åˆ°CSVæ ¼å¼
                file_name_with_timestamp = file_name_with_timestamp.replace('.xlsx', '.csv')
                file_path = os.path.join(created_folder, file_name_with_timestamp)
                final_df.to_csv(file_path, index=False)
                print(f"CSVæ–‡ä»¶ç”Ÿæˆå®Œæˆ: {file_path}")
            
            # è¿”å›æ–‡ä»¶ä¸‹è½½é“¾æ¥
            from fastapi.responses import JSONResponse
            download_url = f"/download/{file_name_with_timestamp}"
            return JSONResponse({
                "status": "success",
                "message": "æ–‡ä»¶ç”ŸæˆæˆåŠŸ",
                "download_url": download_url,
                "filename": file_name_with_timestamp
            })
            
    except HTTPException:
        raise
    except Exception as e:
        print(f"å¯¼å‡ºæ•°æ®å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"å¯¼å‡ºæ•°æ®å¤±è´¥: {str(e)}")

@app.get("/download/{filename}")
async def download_file(filename: str):
    """ä¸‹è½½ç”Ÿæˆçš„æ–‡ä»¶"""
    import os
    from fastapi.responses import FileResponse
    from fastapi import HTTPException
    
    file_path = os.path.join("created", filename)
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(file_path):
        raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
    
    # æ ¹æ®æ–‡ä»¶æ‰©å±•åè®¾ç½®æ­£ç¡®çš„åª’ä½“ç±»å‹
    if filename.endswith('.xlsx'):
        media_type = "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
    elif filename.endswith('.csv'):
        media_type = "text/csv"
    else:
        media_type = "application/octet-stream"
    
    return FileResponse(
        path=file_path,
        media_type=media_type,
        filename=filename
    )

@app.delete("/tables/{table_name}")
async def delete_table(table_name: str):
    """åˆ é™¤æŒ‡å®šè¡¨"""
    success = db_manager.delete_table(table_name)
    if success:
        return {"status": "success", "message": f"è¡¨ {table_name} å·²åˆ é™¤"}
    else:
        raise HTTPException(status_code=500, detail=f"åˆ é™¤è¡¨ {table_name} å¤±è´¥")

@app.post("/import-all")
async def import_all_files(background_tasks: BackgroundTasks):
    """å¯¼å…¥dataç›®å½•ä¸­çš„æ‰€æœ‰Excelæ–‡ä»¶"""
    data_folder = "data"
    excel_files = glob.glob(os.path.join(data_folder, "*.xlsx"))
    
    if not excel_files:
        raise HTTPException(status_code=404, detail=f"åœ¨ {data_folder} æ–‡ä»¶å¤¹ä¸­æœªæ‰¾åˆ°ä»»ä½•Excelæ–‡ä»¶")
    
    # æ·»åŠ æ‰€æœ‰æ–‡ä»¶åˆ°åå°ä»»åŠ¡
    for excel_file in excel_files:
        filename = os.path.basename(excel_file)
        # ä¿®å¤ï¼šæ­£ç¡®ä¼ é€’å‚æ•°
        background_tasks.add_task(import_file, filename=filename)
    
    return {
        "total": len(excel_files),
        "files": [os.path.basename(file) for file in excel_files],
        "status": "importing"
    }

@app.delete("/files/{filename}")
async def delete_file(filename: str):
    """åˆ é™¤æŒ‡å®šçš„Excelæ–‡ä»¶"""
    data_folder = "data"
    file_path = os.path.join(data_folder, filename)
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(file_path):
        raise HTTPException(status_code=404, detail=f"æ–‡ä»¶ {filename} ä¸å­˜åœ¨")
    
    # åˆ é™¤æ–‡ä»¶
    try:
        os.remove(file_path)
        return {"filename": filename, "status": "deleted"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ é™¤æ–‡ä»¶å¤±è´¥: {str(e)}")

@app.delete("/files")
async def delete_all_files():
    """åˆ é™¤æ‰€æœ‰Excelæ–‡ä»¶"""
    data_folder = "data"
    if not os.path.exists(data_folder):
        raise HTTPException(status_code=404, detail="æ•°æ®ç›®å½•ä¸å­˜åœ¨")
    
    deleted_files = []
    for filename in os.listdir(data_folder):
        if filename.endswith(".xlsx"):
            file_path = os.path.join(data_folder, filename)
            try:
                os.remove(file_path)
                deleted_files.append(filename)
            except Exception as e:
                logger.error(f"åˆ é™¤æ–‡ä»¶ {filename} å¤±è´¥: {e}")
    
    return {
        "message": f"æˆåŠŸåˆ é™¤ {len(deleted_files)} ä¸ªæ–‡ä»¶",
        "deleted_files": deleted_files
    }

@app.delete("/tables")
async def delete_all_tables():
    """åˆ é™¤æ‰€æœ‰æ•°æ®åº“è¡¨"""
    try:
        # è·å–æ‰€æœ‰è¡¨å
        tables = db_manager.get_tables()
        
        deleted_tables = []
        for table in tables:
            try:
                # åˆ é™¤è¡¨
                db_manager.delete_table(table)
                deleted_tables.append(table)
            except Exception as e:
                print(f"åˆ é™¤è¡¨ {table} å¤±è´¥: {e}")
        
        return {
            "message": f"æˆåŠŸåˆ é™¤ {len(deleted_tables)} ä¸ªè¡¨",
            "deleted_tables": deleted_tables
        }
    except Exception as e:
        print(f"åˆ é™¤æ‰€æœ‰è¡¨æ—¶å‡ºé”™: {e}")
        raise HTTPException(status_code=500, detail="åˆ é™¤æ‰€æœ‰è¡¨å¤±è´¥")

        return {
            "message": f"æˆåŠŸåˆ é™¤ {len(deleted_tables)} ä¸ªè¡¨",
            "deleted_tables": deleted_tables
        }
    except Exception as e:
        print(f"åˆ é™¤æ‰€æœ‰è¡¨æ—¶å‡ºé”™: {e}")
        raise HTTPException(status_code=500, detail="åˆ é™¤æ‰€æœ‰è¡¨å¤±è´¥")

@app.post("/api/generate-daily-hourly-cache")
async def generate_daily_hourly_cache():
    """
    ç”Ÿæˆæ‰€æœ‰æ—¥æœŸçš„åˆ†æ—¶æ•°æ®ç¼“å­˜
    (ä¿®æ”¹ä¸ºï¼šä»…æ‰§è¡Œ init_weather é€»è¾‘ï¼Œå³å…¨é‡æ›´æ–°æ—¥å†å’Œå¤©æ°”ï¼Œå¹¶åŒæ­¥ç¼“å­˜ä¸­çš„å¤©æ°”æ•°æ®)
    """
    from sql_config import SQL_RULES
    from fastapi.concurrency import run_in_threadpool
    import calendar_weather
    
    try:
        # 1. ç¡®å®šè¡¨ç»“æ„ (ä¿ç•™å»ºè¡¨é€»è¾‘ï¼Œé˜²æ­¢è¡¨ä¸å­˜åœ¨å¯¼è‡´åç»­æ›´æ–°ç¼“å­˜å¤±è´¥)
        table_name = "cache_daily_hourly"
        
        # æ„å»ºå­—æ®µåˆ—è¡¨
        # åŸºç¡€å­—æ®µ
        columns_def = [
            "`record_date` DATE NOT NULL",
            "`hour` TINYINT NOT NULL",
            "`updated_at` TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP"
        ]
        
        # ä» SQL_RULES åŠ¨æ€ç”Ÿæˆå­—æ®µ
        # åŠ ä¸Šè®¡ç®—å­—æ®µ
        calc_fields = {
            "price_diff": "FLOAT COMMENT 'ä»·å·®'",
            "load_deviation": "FLOAT COMMENT 'è´Ÿè·åå·®'",
            "new_energy_forecast": "FLOAT COMMENT 'æ–°èƒ½æºé¢„æµ‹æ€»å’Œ'"
        }
        
        # åˆå¹¶æ‰€æœ‰å­—æ®µ
        all_fields = {}
        
        # æ·»åŠ è§„åˆ™ä¸­çš„å­—æ®µ
        for key, rule in SQL_RULES.items():
            field_name = key
            # é»˜è®¤éƒ½æ˜¯ FLOATï¼Œé™¤äº†æ—¥æœŸ/å­—ç¬¦ä¸²ç±»å‹
            if key in ['date', 'day_type', 'week_day', 'weather', 'wind_direction']:
                col_type = "VARCHAR(50)"
            else:
                col_type = "FLOAT"
            
            all_fields[field_name] = f"`{field_name}` {col_type} COMMENT '{rule.get('name', '')}'"
            
        # æ·»åŠ è®¡ç®—å­—æ®µ
        for k, v in calc_fields.items():
            all_fields[k] = f"`{k}` {v}"
            
        # ç»„è£… CREATE TABLE è¯­å¥
        cols_sql = ",\n".join(list(all_fields.values()) + columns_def)
        
        with db_manager.engine.begin() as conn:
            create_sql = f"""
            CREATE TABLE IF NOT EXISTS {table_name} (
                {cols_sql},
                PRIMARY KEY (`record_date`, `hour`)
            ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
            """
            conn.execute(text(create_sql))
            print(f"âœ… ç¼“å­˜è¡¨ {table_name} å·²å°±ç»ª")

        # 2. æ‰§è¡Œ init_weather é€»è¾‘ (å…¨é‡æ›´æ–°æ—¥å†å’Œå¤©æ°”)
        # å‚è€ƒ init_calendar.py çš„èŒƒå›´ï¼Œæˆ–è€…è¦†ç›–è¾ƒé•¿çš„æ—¶é—´æ®µ
        start_date = datetime.date(2023, 1, 1)
        end_date = datetime.date(2027, 12, 31)
        
        print(f"ğŸš€ å¼€å§‹æ‰§è¡Œå…¨é‡å¤©æ°”åˆå§‹åŒ–: {start_date} -> {end_date}")
        
        # åœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œï¼Œé¿å…é˜»å¡ä¸»çº¿ç¨‹
        await run_in_threadpool(calendar_weather.update_calendar, start_date, end_date)
        
        return {"status": "success", "message": f"å…¨é‡å¤©æ°”åŠç¼“å­˜æ›´æ–°å®Œæˆ ({start_date} è‡³ {end_date})"}

    except Exception as e:
        import traceback
        traceback.print_exc()
        return JSONResponse(status_code=500, content={"error": str(e)})

async def calculate_daily_hourly_data(date: str):
    """
    è®¡ç®—æŒ‡å®šæ—¥æœŸçš„åˆ†æ—¶æ•°æ®ï¼ˆæ ¸å¿ƒé€»è¾‘æå–ï¼‰
    è¿”å›: List[Dict] (24å°æ—¶æ•°æ®)
    """
    from sql_config import SQL_RULES, TABLE_SOURCE_POWER, TABLE_SOURCE_WEATHER
    try:
        target_date = pd.to_datetime(date).date()
        date_str = target_date.strftime("%Y%m%d")
        target_date_str = target_date.strftime("%Y-%m-%d")
        table_name_power = f"power_data_{date_str}"
        table_name_weather = "calendar_weather"
        
        tables = db_manager.get_tables()
        if table_name_power not in tables:
            return None
            
        hourly_data_lists = {h: {} for h in range(24)}
        daily_weather_data = {}
        
        with db_manager.engine.connect() as conn:
            # 1. æŸ¥ç”µåŠ›æ•°æ®
            for key, rule in SQL_RULES.items():
                if rule.get("source") == TABLE_SOURCE_POWER:
                    where_clause = rule["where"]
                    sql = text(f"SELECT record_time, value FROM {table_name_power} WHERE {where_clause}")
                    result = conn.execute(sql).fetchall()
                    
                    for row in result:
                        r_time = row[0]
                        val = float(row[1]) if row[1] is not None else 0
                        
                        if hasattr(r_time, 'total_seconds'):
                            hour = int(r_time.total_seconds() // 3600)
                        else:
                            continue
                            
                        if 0 <= hour <= 23:
                            hourly_data_lists[hour].setdefault(key, []).append(val)

        # 2. æŸ¥å¤©æ°”æ•°æ®
        if table_name_weather in tables:
            with db_manager.engine.connect() as conn:
                sql = text(f"SELECT * FROM {table_name_weather} WHERE date = :d")
                row = conn.execute(sql, {"d": target_date_str}).fetchone()
                
                if row:
                    row_dict = dict(row._mapping)
                    weather_json = row_dict.get("weather_json")
                    if isinstance(weather_json, str):
                        try:
                            import json
                            weather_json = json.loads(weather_json)
                        except:
                            weather_json = {}
                    elif weather_json is None:
                        weather_json = {}
                    
                    for key, rule in SQL_RULES.items():
                        if rule.get("source") == TABLE_SOURCE_WEATHER:
                            col = rule.get("column")
                            json_key = rule.get("json_key")
                            
                            val = None
                            if col == "weather_json" and json_key:
                                val = weather_json.get(json_key)
                                if isinstance(val, list) and len(val) == 24:
                                    for h in range(24):
                                        hourly_data_lists[h][key] = val[h]
                                    continue
                            elif col in row_dict:
                                val = row_dict[col]
                            
                            daily_weather_data[key] = val

        # 3. èšåˆä¸è®¡ç®—
        result_list = []
        for h in range(24):
            lists = hourly_data_lists[h]
            row = {"hour": h}
            
            # å‡å€¼èšåˆ
            for key, rule in SQL_RULES.items():
                if rule.get("source") == TABLE_SOURCE_POWER:
                    vals = lists.get(key, [])
                    if vals:
                        row[key] = sum(vals) / len(vals)
                elif key in lists:
                    row[key] = lists[key]
            
            # å¡«å……å•æ—¥å¤©æ°”
            for k, v in daily_weather_data.items():
                row[k] = v
            
            # è®¡ç®—è¡ç”Ÿå­—æ®µ
            if "price_da" in row and "price_rt" in row:
                row["price_diff"] = row["price_da"] - row["price_rt"]
            
            if "load_forecast" in row and "load_actual" in row:
                row["load_deviation"] = row["load_forecast"] - row["load_actual"]
            
            if "new_energy_forecast" not in row:
                pv = row.get("ne_pv_forecast", 0) or 0
                wind = row.get("ne_wind_forecast", 0) or 0
                if pv > 0 or wind > 0:
                    row["new_energy_forecast"] = pv + wind
            
            result_list.append(row)
            
        return result_list
    except Exception as e:
        print(f"Calculation error for {date}: {e}")
        return None

@app.post("/daily-averages")
async def query_daily_averages(
    dates: str = Form(..., description="æ—¥æœŸåˆ—è¡¨ï¼ŒJSONæ ¼å¼ï¼Œä¾‹å¦‚: [\"2023-09-18\", \"2023-09-19\"]"),
    data_type_keyword: str = Form("æ—¥å‰èŠ‚ç‚¹ç”µä»·", description="æ•°æ®ç±»å‹å…³é”®å­—"),
    station_name: str = Form(None, description="ç«™ç‚¹åç§°ï¼ˆå¯é€‰ï¼‰")
):
    """
    æŸ¥è¯¢å¤šå¤©çš„å‡å€¼æ•°æ®
    
    å‚æ•°:
    - dates: æ—¥æœŸåˆ—è¡¨ï¼ŒJSONæ ¼å¼
    - data_type_keyword: æ•°æ®ç±»å‹å…³é”®å­—
    - station_name: ç«™ç‚¹åç§°ï¼ˆå¯é€‰ï¼‰
    
    è¿”å›:
    - æŸ¥è¯¢ç»“æœ
    """
    try:
        import json
        date_list = json.loads(dates)
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"æ—¥æœŸæ ¼å¼é”™è¯¯: {str(e)}")
    
    result = importer.query_daily_averages(date_list, data_type_keyword, station_name)
    
    if result["total"] == 0:
        return {"total": 0, "data": []}
    
    return result

@app.get("/daily-averages/export")
async def export_daily_averages(
    dates: str = Query(..., description="æ—¥æœŸåˆ—è¡¨ï¼ŒJSONæ ¼å¼"),
    data_type_keyword: str = Query("æ—¥å‰èŠ‚ç‚¹ç”µä»·", description="æ•°æ®ç±»å‹å…³é”®å­—")
):
    """
    å¯¼å‡ºå¤šå¤©çš„å‡å€¼æ•°æ®ä¸ºExcelæ–‡ä»¶
    
    å‚æ•°:
    - dates: æ—¥æœŸåˆ—è¡¨ï¼ŒJSONæ ¼å¼
    - data_type_keyword: æ•°æ®ç±»å‹å…³é”®å­—
    
    è¿”å›:
    - Excelæ–‡ä»¶ä¸‹è½½
    """
    try:
        date_list = json.loads(dates)
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"æ—¥æœŸæ ¼å¼é”™è¯¯: {str(e)}")
    
    # æŸ¥è¯¢æ•°æ®
    result = importer.query_daily_averages(date_list, data_type_keyword)
    
    # ç”Ÿæˆæ–‡ä»¶åï¼šå¤šå¤©å‡å€¼æŸ¥è¯¢_æ—¶é—´æˆ³.xlsx
    from datetime import datetime
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"å¤šå¤©å‡å€¼æŸ¥è¯¢_{timestamp}.xlsx"
    
    if not result["data"]:
        # å¦‚æœæ²¡æœ‰æ•°æ®ï¼Œè¿”å›ç©ºExcel
        df = pd.DataFrame()
        output = BytesIO()
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            df.to_excel(writer, index=False)
        output.seek(0)
        
        from fastapi.responses import StreamingResponse
        import urllib.parse
        encoded_filename = urllib.parse.quote(filename)
        return StreamingResponse(
            iter([output.getvalue()]),
            media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            headers={"Content-Disposition": f"attachment; filename*=UTF-8''{encoded_filename}"}
        )
    
    # è½¬æ¢ä¸ºDataFrame
    df = pd.DataFrame(result["data"])
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«å¿…è¦çš„åˆ—
    required_columns = ['channel_name', 'record_date', 'record_time', 'value', 'sheet_name']
    if all(col in df.columns for col in required_columns):
        # ç±»ä¼¼preHandle.pyçš„å¤„ç†æ–¹å¼ï¼Œç”Ÿæˆé€è§†è¡¨æ ¼å¼
        try:
            # æå–å”¯ä¸€çš„sheet_nameï¼ˆå‡è®¾æ•°æ®ä¸­sheet_nameå”¯ä¸€ï¼‰
            sheet_name = df['sheet_name'].unique()[0] if len(df['sheet_name'].unique()) > 0 else 'Sheet1'
            # æå–å”¯ä¸€çš„æ—¥æœŸï¼ˆå‡è®¾æ•°æ®ä¸­æ—¥æœŸå”¯ä¸€ï¼‰
            record_date = df['record_date'].unique()[0] if len(df['record_date'].unique()) > 0 else pd.Timestamp.now().date()
            
            # æ ¼å¼åŒ–æ—¥æœŸä¸ºYYYY-MM-DD
            if hasattr(record_date, 'strftime'):
                record_date_str = record_date.strftime('%Y-%m-%d')
            else:
                record_date_str = str(record_date)
            
            # å¤„ç†æ–‡ä»¶åç‰¹æ®Šå­—ç¬¦ï¼ˆé¿å…æ–œæ ã€ç©ºæ ¼ç­‰å¯¼è‡´ä¿å­˜å¤±è´¥ï¼‰
            sheet_name_clean = str(sheet_name).replace('/', '_').replace('\\', '_').replace(' ', '')
            
            # è½¬æ¢record_timeä¸ºå°æ—¶ï¼ˆå¤„ç†å„ç§å¯èƒ½çš„æ ¼å¼ï¼‰
            def extract_hour(time_value):
                if pd.isna(time_value):
                    return None
                if isinstance(time_value, str):
                    if ':' in time_value:
                        # æ ¼å¼å¦‚ "01:00", "1:00"
                        return int(time_value.split(':')[0])
                    else:
                        # å¯èƒ½æ˜¯æ•°å­—å­—ç¬¦ä¸²å¦‚ "100" è¡¨ç¤º 01:00
                        try:
                            time_int = int(time_value)
                            return time_int // 100
                        except:
                            return None
                elif isinstance(time_value, (int, float)):
                    # æ•°å­—æ ¼å¼å¦‚ 100 è¡¨ç¤º 01:00
                    return int(time_value) // 100
                else:
                    # timedeltaæˆ–å…¶ä»–æ ¼å¼
                    try:
                        # å¦‚æœæ˜¯timedeltaå¯¹è±¡
                        hours = time_value.seconds // 3600
                        return hours
                    except:
                        return None
            
            # åº”ç”¨å°æ—¶æå–å‡½æ•°
            df['hour'] = df['record_time'].apply(extract_hour)
            
            # åˆ é™¤hourä¸ºNaNçš„è¡Œ
            df = df.dropna(subset=['hour'])
            
            # ç”Ÿæˆç”µç«™çº§é€è§†è¡¨
            if len(df) > 0:
                pivot_df = pd.pivot_table(
                    df,
                    index=['channel_name', 'record_date'],
                    columns='hour',
                    values='value',
                    aggfunc='mean'
                )
                
                # é‡æ–°ç´¢å¼•ç¡®ä¿æœ‰24å°æ—¶åˆ—ï¼Œå¹¶æ­£ç¡®æ ¼å¼åŒ–åˆ—å
                pivot_df = pivot_df.reindex(columns=range(24), fill_value=np.nan)
                # ç¡®ä¿åˆ—åæ ¼å¼ä¸º HH:00
                pivot_df.columns = [f'{int(h):02d}:00' for h in pivot_df.columns]
                pivot_df = pivot_df.reset_index()
                
                # ä¿®æ”¹å‰ä¸¤åˆ—åç§°
                pivot_df = pivot_df.rename(columns={
                    'channel_name': 'èŠ‚ç‚¹åç§°',
                    'record_date': 'æ—¥æœŸ'
                })
                
                # æ’å…¥å•ä½åˆ—
                pivot_df.insert(
                    loc=2,
                    column='å•ä½',
                    value='ç”µä»·(å…ƒ/MWh)'
                )
                
                # æ·»åŠ å‘ç”µä¾§å…¨çœç»Ÿä¸€å‡ä»·è¡Œ
                hour_columns = [f'{h:02d}:00' for h in range(24)]
                # ç¡®ä¿æ‰€æœ‰å°æ—¶åˆ—éƒ½å­˜åœ¨
                for col in hour_columns:
                    if col not in pivot_df.columns:
                        pivot_df[col] = np.nan
                
                # åœ¨è®¡ç®—å¹³å‡å€¼å‰ï¼Œç¡®ä¿æ‰€æœ‰åˆ—ä¸ºæ•°å€¼ç±»å‹
                for col in hour_columns:
                    pivot_df[col] = pd.to_numeric(pivot_df[col], errors='coerce')
                
                # è®¡ç®—å…¨çœç»Ÿä¸€å‡ä»·è¡Œ
                province_avg = {}
                for col in hour_columns:
                    if col in pivot_df.columns:
                        province_avg[col] = pivot_df[col].mean(skipna=True)
                              
                final_df = pivot_df
            else:
                # å¦‚æœå¤„ç†åæ²¡æœ‰æ•°æ®ï¼Œåˆ›å»ºç©ºçš„DataFrame
                columns = ['èŠ‚ç‚¹åç§°', 'æ—¥æœŸ', 'å•ä½'] + [f'{h:02d}:00' for h in range(24)]
                final_df = pd.DataFrame(columns=columns)
            
            # ç›´æ¥è¿”å›Excelæ–‡ä»¶æµ
            output = BytesIO()
            with pd.ExcelWriter(output, engine='openpyxl') as writer:
                final_df.to_excel(writer, index=False, sheet_name=sheet_name_clean[:31])
            output.seek(0)
            
            import urllib.parse
            encoded_filename = urllib.parse.quote(filename)
            from fastapi.responses import StreamingResponse
            return StreamingResponse(
                iter([output.getvalue()]),
                media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                headers={
                    "Content-Disposition": f"attachment; filename*=UTF-8''{encoded_filename}"
                }
            )
        except Exception as e:
            print(f"å¤„ç†é€è§†è¡¨æ ¼å¼æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
    
    # å¦‚æœä¸åŒ…å«å¿…è¦åˆ—æˆ–å¤„ç†é€è§†è¡¨å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å¯¼å‡ºæ–¹å¼
    # ç›´æ¥è¿”å›Excelæ–‡ä»¶æµ
    output = BytesIO()
    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        df.to_excel(writer, index=False, sheet_name='å¤šå¤©å‡å€¼æ•°æ®')
    output.seek(0)
    
    import urllib.parse
    encoded_filename = urllib.parse.quote(filename)
    from fastapi.responses import StreamingResponse
    return StreamingResponse(
        iter([output.getvalue()]),
        media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        headers={
            "Content-Disposition": f"attachment; filename*=UTF-8''{encoded_filename}"
        }
    )

@app.post("/daily-averages/export-from-result")
async def export_daily_averages_from_result(
    query_result: str = Form(..., description="æŸ¥è¯¢ç»“æœæ•°æ®"),
    data_type_keyword: str = Form("æ—¥å‰èŠ‚ç‚¹ç”µä»·", description="æ•°æ®ç±»å‹å…³é”®å­—")
):
    """
    æ ¹æ®å½“å‰æŸ¥è¯¢ç»“æœå¯¼å‡ºå¤šå¤©çš„å‡å€¼æ•°æ®ä¸ºExcelæ–‡ä»¶
    
    å‚æ•°:
    - query_result: å½“å‰æŸ¥è¯¢ç»“æœï¼ŒJSONæ ¼å¼
    - data_type_keyword: æ•°æ®ç±»å‹å…³é”®å­—
    
    è¿”å›:
    - Excelæ–‡ä»¶ä¸‹è½½
    """
    try:
        import json
        query_result_data = json.loads(query_result)
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"æŸ¥è¯¢ç»“æœæ ¼å¼é”™è¯¯: {str(e)}")
    
    # ç”Ÿæˆæ–‡ä»¶åï¼šå¤šå¤©å‡å€¼æŸ¥è¯¢_æ—¶é—´æˆ³.xlsx
    from datetime import datetime
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"å¤šå¤©å‡å€¼æŸ¥è¯¢_{timestamp}.xlsx"
    
    if not query_result_data:
        # å¦‚æœæ²¡æœ‰æ•°æ®ï¼Œè¿”å›ç©ºExcel
        df = pd.DataFrame()
        output = BytesIO()
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            df.to_excel(writer, index=False)
        output.seek(0)
        
        from fastapi.responses import StreamingResponse
        import urllib.parse
        encoded_filename = urllib.parse.quote(filename)
        return StreamingResponse(
            iter([output.getvalue()]),
            media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            headers={"Content-Disposition": f"attachment; filename*=UTF-8''{encoded_filename}"}
        )
    
    # è½¬æ¢ä¸ºDataFrame
    df = pd.DataFrame(query_result_data)
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«å¿…è¦çš„åˆ—
    required_columns = ['channel_name', 'record_date', 'record_time', 'value', 'sheet_name']
    if all(col in df.columns for col in required_columns):
        # ç±»ä¼¼preHandle.pyçš„å¤„ç†æ–¹å¼ï¼Œç”Ÿæˆé€è§†è¡¨æ ¼å¼
        try:
            # æå–å”¯ä¸€çš„sheet_nameï¼ˆå‡è®¾æ•°æ®ä¸­sheet_nameå”¯ä¸€ï¼‰
            sheet_name = df['sheet_name'].unique()[0] if len(df['sheet_name'].unique()) > 0 else 'Sheet1'
            # æå–å”¯ä¸€çš„æ—¥æœŸï¼ˆå‡è®¾æ•°æ®ä¸­æ—¥æœŸå”¯ä¸€ï¼‰
            record_date = df['record_date'].unique()[0] if len(df['record_date'].unique()) > 0 else pd.Timestamp.now().date()
            
            # æ ¼å¼åŒ–æ—¥æœŸä¸ºYYYY-MM-DD
            if hasattr(record_date, 'strftime'):
                record_date_str = record_date.strftime('%Y-%m-%d')
            else:
                record_date_str = str(record_date)
            
            # å¤„ç†æ–‡ä»¶åç‰¹æ®Šå­—ç¬¦ï¼ˆé¿å…æ–œæ ã€ç©ºæ ¼ç­‰å¯¼è‡´ä¿å­˜å¤±è´¥ï¼‰
            sheet_name_clean = str(sheet_name).replace('/', '_').replace('\\', '_').replace(' ', '')
            
            # è½¬æ¢record_timeä¸ºå°æ—¶ï¼ˆå¤„ç†å„ç§å¯èƒ½çš„æ ¼å¼ï¼‰
            # è½¬æ¢record_timeä¸ºå°æ—¶
            def extract_hour(time_value):
                if pd.isna(time_value):
                    return None
                
                try:
                    # 1. ä¼˜å…ˆå¤„ç†æ•´æ•°/æµ®ç‚¹æ•°
                    if isinstance(time_value, (int, float, np.number)):
                        val = int(time_value)
                        
                        # ã€æ ¸å¿ƒä¿®å¤é€»è¾‘ã€‘
                        # å¦‚æœæ•°å€¼å¾ˆå¤§ï¼ˆè¶…è¿‡2400ï¼‰ï¼Œè¯´æ˜è‚¯å®šæ˜¯ç§’æ•°ï¼Œä¸æ˜¯HHMM
                        # ä¾‹å¦‚ 3600(ç§’) / 3600 = 1ç‚¹
                        if val >= 3600: 
                             return val // 3600
                        
                        # å¦‚æœæ•°å€¼åœ¨ 0-23 ä¹‹é—´ï¼Œç›´æ¥æ˜¯å°æ—¶
                        if 0 <= val < 24:
                            return val
                            
                        # å¦‚æœæ˜¯ HHMM æ ¼å¼ (ä¾‹å¦‚ 100 ä»£è¡¨ 01:00, 2300 ä»£è¡¨ 23:00)
                        if 100 <= val <= 2400:
                            return val // 100
                            
                        # å…œåº•ï¼šå¦‚æœæ˜¯ 0ï¼Œæ—¢å¯èƒ½æ˜¯0ç‚¹ä¹Ÿå¯èƒ½æ˜¯0ç§’ï¼Œè¿”å›0
                        if val == 0:
                            return 0

                    # 2. å¤„ç†å­—ç¬¦ä¸²
                    time_str = str(time_value).strip()
                    if ':' in time_str:
                        return int(time_str.split(':')[0])
                    
                    # 3. å¤„ç† Timedelta å¯¹è±¡
                    if hasattr(time_value, 'total_seconds'):
                        return int(time_value.total_seconds() // 3600)
                    if hasattr(time_value, 'seconds'):
                        return int(time_value.seconds // 3600)

                    # å†æ¬¡å°è¯•è½¬æ•°å­—å¤„ç†ï¼ˆé˜²æ­¢å­—ç¬¦ä¸²ç±»å‹çš„æ•°å­— "3600"ï¼‰
                    try:
                        val = int(float(time_str))
                        if val >= 3600: return val // 3600
                        if val < 24: return val
                        return val // 100
                    except:
                        pass

                    return None
                except Exception as e:
                    return None
            
            # åº”ç”¨å°æ—¶æå–å‡½æ•°
            df['hour'] = df['record_time'].apply(extract_hour)
            print("è½¬æ¢åçš„å‰10è¡Œæ•°æ®:")
            print(df[['record_time', 'hour']].head(10))
            print("Houråˆ—çš„å”¯ä¸€å€¼:", df['hour'].unique())
            # åˆ é™¤hourä¸ºNaNçš„è¡Œ
            df = df.dropna(subset=['hour'])
            
            # ç”Ÿæˆç”µç«™çº§é€è§†è¡¨
            if len(df) > 0:
                pivot_df = pd.pivot_table(
                    df,
                    index=['channel_name', 'record_date'],
                    columns='hour',
                    values='value',
                    aggfunc='mean'
                )
                
                # é‡æ–°ç´¢å¼•ç¡®ä¿æœ‰24å°æ—¶åˆ—ï¼Œå¹¶æ­£ç¡®æ ¼å¼åŒ–åˆ—å
                pivot_df = pivot_df.reindex(columns=range(24), fill_value=np.nan)
                # ç¡®ä¿åˆ—åæ ¼å¼ä¸º HH:00
                pivot_df.columns = [f'{int(h):02d}:00' for h in pivot_df.columns]
                pivot_df = pivot_df.reset_index()
                
                # ä¿®æ”¹å‰ä¸¤åˆ—åç§°
                pivot_df = pivot_df.rename(columns={
                    'channel_name': 'èŠ‚ç‚¹åç§°',
                    'record_date': 'æ—¥æœŸ'
                })
                
                # æ’å…¥å•ä½åˆ—
                pivot_df.insert(
                    loc=2,
                    column='å•ä½',
                    value='ç”µä»·(å…ƒ/MWh)'
                )
                
                # æ·»åŠ å‘ç”µä¾§å…¨çœç»Ÿä¸€å‡ä»·è¡Œ
                hour_columns = [f'{h:02d}:00' for h in range(24)]
                # ç¡®ä¿æ‰€æœ‰å°æ—¶åˆ—éƒ½å­˜åœ¨
                for col in hour_columns:
                    if col not in pivot_df.columns:
                        pivot_df[col] = np.nan
                
                # åœ¨è®¡ç®—å¹³å‡å€¼å‰ï¼Œç¡®ä¿æ‰€æœ‰åˆ—ä¸ºæ•°å€¼ç±»å‹
                for col in hour_columns:
                    pivot_df[col] = pd.to_numeric(pivot_df[col], errors='coerce')
                
                # è®¡ç®—å…¨çœç»Ÿä¸€å‡ä»·è¡Œ
                province_avg = {}
                for col in hour_columns:
                    if col in pivot_df.columns:
                        province_avg[col] = pivot_df[col].mean(skipna=True)
                              
                final_df = pivot_df
            else:
                # å¦‚æœå¤„ç†åæ²¡æœ‰æ•°æ®ï¼Œåˆ›å»ºç©ºçš„DataFrame
                columns = ['èŠ‚ç‚¹åç§°', 'æ—¥æœŸ', 'å•ä½'] + [f'{h:02d}:00' for h in range(24)]
                final_df = pd.DataFrame(columns=columns)
            
            # ç›´æ¥è¿”å›Excelæ–‡ä»¶æµ
            output = BytesIO()
            with pd.ExcelWriter(output, engine='openpyxl') as writer:
                final_df.to_excel(writer, index=False, sheet_name=sheet_name_clean[:31])
            output.seek(0)
            
            import urllib.parse
            encoded_filename = urllib.parse.quote(filename)
            from fastapi.responses import StreamingResponse
            return StreamingResponse(
                iter([output.getvalue()]),
                media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                headers={
                    "Content-Disposition": f"attachment; filename*=UTF-8''{encoded_filename}"
                }
            )
        except Exception as e:
            print(f"å¤„ç†é€è§†è¡¨æ ¼å¼æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
    
    # å¦‚æœä¸åŒ…å«å¿…è¦åˆ—æˆ–å¤„ç†é€è§†è¡¨å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å¯¼å‡ºæ–¹å¼
    output = BytesIO()
    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        df.to_excel(writer, index=False, sheet_name='å¤šå¤©å‡å€¼æ•°æ®')
    output.seek(0)
    
    import urllib.parse
    encoded_filename = urllib.parse.quote(filename)
    from fastapi.responses import StreamingResponse
    return StreamingResponse(
        iter([output.getvalue()]),
        media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        headers={
            "Content-Disposition": f"attachment; filename*=UTF-8''{encoded_filename}"
        }
    )

@app.post("/price-difference")
async def query_price_difference(
    dates: str = Form(..., description="æ—¥æœŸåˆ—è¡¨ï¼ŒJSONæ ¼å¼ï¼Œä¾‹å¦‚: [\"2023-09-18\", \"2023-09-19\"]"),
    region: str = Form("", description="åœ°åŒºå‰ç¼€ï¼Œå¦‚'äº‘å—_'ï¼Œé»˜è®¤ä¸ºç©º"),
    station_name: str = Form(None, description="ç«™ç‚¹åç§°ï¼ˆå¯é€‰ï¼‰")
):
    """
    æŸ¥è¯¢ä»·å·®æ•°æ®ï¼ˆæ—¥å‰èŠ‚ç‚¹ç”µä»· - å®æ—¶èŠ‚ç‚¹ç”µä»·ï¼‰
    
    å‚æ•°:
    - dates: æ—¥æœŸåˆ—è¡¨ï¼ŒJSONæ ¼å¼
    - region: åœ°åŒºå‰ç¼€ï¼Œå¦‚"äº‘å—_"ï¼Œé»˜è®¤ä¸ºç©º
    - station_name: ç«™ç‚¹åç§°ï¼ˆå¯é€‰ï¼‰
    
    è¿”å›:
    - ä»·å·®æŸ¥è¯¢ç»“æœ
    """
    try:
        import json
        date_list = json.loads(dates)
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"æ—¥æœŸæ ¼å¼é”™è¯¯: {str(e)}")
    
    result = importer.query_price_difference(date_list, region, station_name)
    
    return result

@app.post("/price-difference/export-from-result")
async def export_price_difference_from_result(
    query_result: str = Form(..., description="æŸ¥è¯¢ç»“æœæ•°æ®"),
    region: str = Form("", description="åœ°åŒºå‰ç¼€")
):
    """
    æ ¹æ®å½“å‰æŸ¥è¯¢ç»“æœå¯¼å‡ºä»·å·®æ•°æ®ä¸ºExcelæ–‡ä»¶
    
    å‚æ•°:
    - query_result: å½“å‰æŸ¥è¯¢ç»“æœï¼ŒJSONæ ¼å¼
    - region: åœ°åŒºå‰ç¼€
    
    è¿”å›:
    - Excelæ–‡ä»¶ä¸‹è½½
    """
    try:
        import json
        query_result_data = json.loads(query_result)
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"æŸ¥è¯¢ç»“æœæ ¼å¼é”™è¯¯: {str(e)}")
    
    # ç”Ÿæˆæ–‡ä»¶åï¼šä»·å·®æŸ¥è¯¢_æ—¶é—´æˆ³.xlsx
    from datetime import datetime
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"ä»·å·®æŸ¥è¯¢_{timestamp}.xlsx"
    
    if not query_result_data:
        # å¦‚æœæ²¡æœ‰æ•°æ®ï¼Œè¿”å›ç©ºExcel
        df = pd.DataFrame()
        output = BytesIO()
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            df.to_excel(writer, index=False)
        output.seek(0)
        
        from fastapi.responses import StreamingResponse
        import urllib.parse
        encoded_filename = urllib.parse.quote(filename)
        return StreamingResponse(
            iter([output.getvalue()]),
            media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            headers={"Content-Disposition": f"attachment; filename*=UTF-8''{encoded_filename}"}
        )
    
    # è½¬æ¢ä¸ºDataFrame
    df = pd.DataFrame(query_result_data)
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«å¿…è¦çš„åˆ—
    required_columns = ['channel_name', 'record_date', 'record_time', 'value', 'sheet_name']
    if all(col in df.columns for col in required_columns):
        # ç±»ä¼¼preHandle.pyçš„å¤„ç†æ–¹å¼ï¼Œç”Ÿæˆé€è§†è¡¨æ ¼å¼
        try:
            # æå–å”¯ä¸€çš„sheet_name
            sheet_name = df['sheet_name'].unique()[0] if len(df['sheet_name'].unique()) > 0 else 'Sheet1'
            record_date = df['record_date'].unique()[0] if len(df['record_date'].unique()) > 0 else pd.Timestamp.now().date()
            
            # æ ¼å¼åŒ–æ—¥æœŸä¸ºYYYY-MM-DD
            if hasattr(record_date, 'strftime'):
                record_date_str = record_date.strftime('%Y-%m-%d')
            else:
                record_date_str = str(record_date)
            
            # å¤„ç†æ–‡ä»¶åç‰¹æ®Šå­—ç¬¦
            sheet_name_clean = str(sheet_name).replace('/', '_').replace('\\', '_').replace(' ', '')
            
            # è½¬æ¢record_timeä¸ºå°æ—¶
            def extract_hour(time_value):
                if pd.isna(time_value):
                    return None
                try:
                    if isinstance(time_value, (int, float, np.number)):
                        val = int(time_value)
                        if val >= 3600:
                            return val // 3600
                        if 0 <= val < 24:
                            return val
                        if 100 <= val <= 2400:
                            return val // 100
                        if val == 0:
                            return 0
                    time_str = str(time_value).strip()
                    if ':' in time_str:
                        return int(time_str.split(':')[0])
                    if hasattr(time_value, 'total_seconds'):
                        return int(time_value.total_seconds() // 3600)
                    if hasattr(time_value, 'seconds'):
                        return int(time_value.seconds // 3600)
                    try:
                        val = int(float(time_str))
                        if val >= 3600:
                            return val // 3600
                        if val < 24:
                            return val
                        return val // 100
                    except:
                        pass
                    return None
                except Exception as e:
                    return None
            
            # åº”ç”¨å°æ—¶æå–å‡½æ•°
            df['hour'] = df['record_time'].apply(extract_hour)
            df = df.dropna(subset=['hour'])
            
            # ç”Ÿæˆé€è§†è¡¨
            if len(df) > 0:
                pivot_df = pd.pivot_table(
                    df,
                    index=['channel_name', 'record_date'],
                    columns='hour',
                    values='value',
                    aggfunc='mean'
                )
                
                # é‡æ–°ç´¢å¼•ç¡®ä¿æœ‰24å°æ—¶åˆ—
                pivot_df = pivot_df.reindex(columns=range(24), fill_value=np.nan)
                pivot_df.columns = [f'{int(h):02d}:00' for h in pivot_df.columns]
                pivot_df = pivot_df.reset_index()
                
                # ä¿®æ”¹åˆ—åç§°
                pivot_df = pivot_df.rename(columns={
                    'channel_name': 'èŠ‚ç‚¹åç§°',
                    'record_date': 'æ—¥æœŸ'
                })
                
                # æ’å…¥å•ä½åˆ—
                pivot_df.insert(
                    loc=2,
                    column='å•ä½',
                    value='ä»·å·®(å…ƒ/MWh)'
                )
                
                # ç¡®ä¿æ‰€æœ‰å°æ—¶åˆ—éƒ½å­˜åœ¨
                hour_columns = [f'{h:02d}:00' for h in range(24)]
                for col in hour_columns:
                    if col not in pivot_df.columns:
                        pivot_df[col] = np.nan
                
                # ç¡®ä¿æ‰€æœ‰åˆ—ä¸ºæ•°å€¼ç±»å‹
                for col in hour_columns:
                    pivot_df[col] = pd.to_numeric(pivot_df[col], errors='coerce')
                
                final_df = pivot_df
            else:
                columns = ['èŠ‚ç‚¹åç§°', 'æ—¥æœŸ', 'å•ä½'] + [f'{h:02d}:00' for h in range(24)]
                final_df = pd.DataFrame(columns=columns)
            
            # è¿”å›Excelæ–‡ä»¶æµ
            output = BytesIO()
            from openpyxl.styles import PatternFill
            # from openpyxl.chart import BarChart, Reference, Series
            
            with pd.ExcelWriter(output, engine='openpyxl') as writer:
                final_df.to_excel(writer, index=False, sheet_name=sheet_name_clean[:31])
                
                # è·å–å·¥ä½œè¡¨
                worksheet = writer.sheets[sheet_name_clean[:31]]
                
                # åº”ç”¨æ¡ä»¶æ ¼å¼ï¼šå¤§äº0æ˜¾ç¤ºç»¿è‰²æ¸å˜ï¼Œå°äº0æ˜¾ç¤ºçº¢è‰²æ¸å˜
                hour_columns = [f'{h:02d}:00' for h in range(24)]
                
                # æ‰¾åˆ°æ‰€æœ‰æ•°å€¼ä¸­çš„æœ€å¤§ç»å¯¹å€¼ï¼Œç”¨äºç¡®å®šé¢œè‰²æ·±åº¦
                max_abs_value = 0
                for col in final_df.columns:
                    if col in hour_columns:
                        max_abs_value = max(max_abs_value, final_df[col].abs().max())
                
                # å¦‚æœæœ€å¤§ç»å¯¹å€¼ä¸º0ï¼Œåˆ™è®¾ä¸º1é¿å…é™¤é›¶é”™è¯¯
                if max_abs_value == 0:
                    max_abs_value = 1
                
                # å®šä¹‰é¢œè‰²å¡«å……å‡½æ•°
                def get_fill_color(value):
                    if pd.isna(value):
                        return None
                    
                    # è®¡ç®—é¢œè‰²å¼ºåº¦ï¼ŒåŸºäºç»å¯¹å€¼æ¯”ä¾‹
                    intensity = abs(value) / max_abs_value
                    
                    # ç¡®ä¿æœ€å°äº®åº¦ï¼Œé¿å…é¢œè‰²è¿‡æ·±
                    min_brightness = 150  # æœ€äº®ä¸º255
                    brightness_range = 255 - min_brightness
                    brightness = int(min_brightness + (1 - intensity) * brightness_range)
                    
                    if value > 0:
                        # æ­£æ•°ï¼šç»¿è‰²ç³»ï¼Œå¼ºåº¦è¶Šé«˜é¢œè‰²è¶Šæ·±
                        red = brightness
                        green = 255
                        blue = brightness
                    elif value < 0:
                        # è´Ÿæ•°ï¼šçº¢è‰²ç³»ï¼Œå¼ºåº¦è¶Šé«˜é¢œè‰²è¶Šæ·±
                        red = 255
                        green = brightness
                        blue = brightness
                    else:
                        # é›¶å€¼ï¼šç™½è‰²
                        return None
                    
                    # è½¬æ¢ä¸ºåå…­è¿›åˆ¶é¢œè‰²ä»£ç 
                    color_code = f"{red:02X}{green:02X}{blue:02X}"
                    return PatternFill(start_color=color_code, end_color=color_code, fill_type='solid')
                
                # æ‰¾åˆ°å°æ—¶åˆ—çš„åˆ—ç´¢å¼•å¹¶åº”ç”¨æ¡ä»¶æ ¼å¼
                for col_idx, col in enumerate(final_df.columns, start=1):
                    if col in hour_columns:
                        # å¯¹æ¯ä¸ªå°æ—¶åˆ—åº”ç”¨æ¡ä»¶æ ¼å¼
                        for row_idx in range(2, len(final_df) + 2):  # ä»ç¬¬2è¡Œå¼€å§‹ï¼ˆç¬¬1è¡Œæ˜¯è¡¨å¤´ï¼‰
                            cell = worksheet.cell(row=row_idx, column=col_idx)
                            if cell.value is not None:
                                try:
                                    value = float(cell.value)
                                    fill = get_fill_color(value)
                                    if fill:
                                        cell.fill = fill
                                except (ValueError, TypeError):
                                    pass

            output.seek(0)
            
            import urllib.parse
            encoded_filename = urllib.parse.quote(filename)
            from fastapi.responses import StreamingResponse
            return StreamingResponse(
                iter([output.getvalue()]),
                media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                headers={
                    "Content-Disposition": f"attachment; filename*=UTF-8''{encoded_filename}"
                }
            )
        except Exception as e:
            print(f"å¤„ç†é€è§†è¡¨æ ¼å¼æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
    
    # å¦‚æœä¸åŒ…å«å¿…è¦åˆ—æˆ–å¤„ç†é€è§†è¡¨å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å¯¼å‡ºæ–¹å¼
    output = BytesIO()
    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        df.to_excel(writer, index=False, sheet_name='ä»·å·®æ•°æ®')
    output.seek(0)
    
    import urllib.parse
    encoded_filename = urllib.parse.quote(filename)
    from fastapi.responses import StreamingResponse
    return StreamingResponse(
        iter([output.getvalue()]),
        media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        headers={
            "Content-Disposition": f"attachment; filename*=UTF-8''{encoded_filename}"
        }
    )

@app.get("/daily_hourly", response_class=HTMLResponse)
async def daily_hourly_page(request: Request):
    """è¿”å›24å°æ—¶æ•°æ®å±•ç¤ºé¡µé¢"""
    return templates.TemplateResponse("daily_hourly.html", {"request": request})

@app.get("/similar_day", response_class=HTMLResponse)
async def similar_day_page(request: Request):
    """è¿”å›ç±»æ¯”æ—¥åŒ¹é…é¡µé¢"""
    return templates.TemplateResponse("similar_day.html", {"request": request})

@app.get("/api/daily-hourly-data")
async def get_daily_hourly_data(date: str):
    """è·å–æŒ‡å®šæ—¥æœŸçš„24å°æ—¶æ•°æ® (ä¼˜å…ˆæŸ¥ç¼“å­˜)"""
    try:
        # 1. å°è¯•ä»ç¼“å­˜è¡¨æŸ¥è¯¢
        table_name = "cache_daily_hourly"
        target_date = pd.to_datetime(date).date()
        date_str = target_date.strftime("%Y-%m-%d")
        
        tables = db_manager.get_tables()
        if table_name in tables:
            with db_manager.engine.connect() as conn:
                # è·å–æ‰€æœ‰åˆ—
                sql = text(f"SELECT * FROM {table_name} WHERE record_date = :d ORDER BY hour ASC")
                result = conn.execute(sql, {"d": date_str}).fetchall()
                
                if result:
                    # è½¬æ¢å›å­—å…¸åˆ—è¡¨
                    data_list = []
                    for row in result:
                        d = dict(row._mapping)
                        # å¤„ç†æ—¥æœŸå¯¹è±¡è½¬å­—ç¬¦ä¸²
                        if 'record_date' in d:
                            d['record_date'] = str(d['record_date'])
                        if 'updated_at' in d:
                            d['updated_at'] = str(d['updated_at'])
                        data_list.append(d)
                    return {"status": "success", "data": data_list, "source": "cache"}

        # 2. å¦‚æœç¼“å­˜æ²¡å‘½ä¸­ï¼Œå®æ—¶è®¡ç®—
        print(f"Cache miss for {date_str}, calculating...")
        data = await calculate_daily_hourly_data(date_str)
        
        if data:
            # 3. å¼‚æ­¥å†™å…¥ç¼“å­˜ (ç®€å•èµ·è§ï¼Œè¿™é‡ŒåŒæ­¥å†™å…¥ï¼Œæˆ–ç•™ç»™ä¸‹æ¬¡æ‰¹é‡ç”Ÿæˆ)
            # ä¸ºäº†ä¿è¯ä¸‹æ¬¡æŸ¥è¯¢å¿«ï¼Œæœ€å¥½è¿™é‡Œå°±å†™å…¥ã€‚
            # ä½†è€ƒè™‘åˆ°è¡¨å¯èƒ½è¿˜æ²¡å»ºï¼Œæˆ–è€… calculate_daily_hourly_data æ˜¯ç‹¬ç«‹çš„
            # æˆ‘ä»¬å¯ä»¥åœ¨ calculate_daily_hourly_data å¤–éƒ¨å†è°ƒä¸€æ¬¡ç”Ÿæˆé€»è¾‘ï¼Œæˆ–è€…æš‚æ—¶åªè¿”å›å®æ—¶æ•°æ®
            # æ—¢ç„¶ç”¨æˆ·ä¸“é—¨è¦äº†ç¼“å­˜è¡¨ï¼Œæˆ‘ä»¬åº”è¯¥å°½åŠ›å»å­˜ã€‚
            
            # å°è¯•è‡ªåŠ¨å»ºè¡¨å¹¶å­˜å…¥? 
            # ç®€å•èµ·è§ï¼Œç›´æ¥è¿”å›å®æ—¶è®¡ç®—ç»“æœï¼Œå¹¶å»ºè®®ç”¨æˆ·ç‚¹å‡»"ç”Ÿæˆç¼“å­˜"
            # æˆ–è€…ï¼Œæˆ‘ä»¬å¯ä»¥è°ƒç”¨ generate_daily_hourly_cache çš„ä¸€éƒ¨åˆ†é€»è¾‘æ¥å­˜å•æ—¥
            # è¿™é‡Œæˆ‘ä»¬é€‰æ‹©ç›´æ¥è¿”å›å®æ—¶æ•°æ®ï¼Œä½†åœ¨å‰ç«¯æç¤ºã€‚
            return {"status": "success", "data": data, "source": "realtime"}
        else:
             return {"status": "error", "message": f"æœªæ‰¾åˆ° {date} çš„ç”µåŠ›æ•°æ®"}

    except Exception as e:
        import traceback
        traceback.print_exc()
        return JSONResponse(status_code=500, content={"error": str(e)})

@app.post("/api/generate-price-cache")
async def generate_price_cache(request: Request):
    """
    ç”ŸæˆèŠ‚ç‚¹ç”µä»·æ˜ å°„ç¼“å­˜è¡¨ -> åˆå¹¶å…¥ cache_daily_hourly
    """
    try:
        # 1. è·å–æ‰€æœ‰æœ‰æ•°æ®çš„æ—¥æœŸ
        all_tables = db_manager.get_tables()
        power_tables = [t for t in all_tables if t.startswith('power_data_')]
        
        dates_to_process = []
        for t in power_tables:
            try:
                d_str = t.replace('power_data_', '')
                dates_to_process.append(d_str) # YYYYMMDD
            except:
                pass
        
        dates_to_process.sort()
        total_days = len(dates_to_process)
        print(f"å¾…å¤„ç†æ—¥æœŸ: {total_days} å¤©")
        
        processed_count = 0
        inserted_count = 0
        
        for date_str in dates_to_process:
            # YYYYMMDD -> YYYY-MM-DD
            target_date_str = f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:]}"
            
            try:
                count = update_price_cache_for_date(target_date_str)
                inserted_count += count
            except Exception as e:
                print(f"Error processing {date_str}: {e}")
                import traceback
                traceback.print_exc()
                continue
            
            processed_count += 1
            if processed_count % 10 == 0:
                print(f"Price Cache: Processed {processed_count}/{total_days} days")

        return {
            "status": "success", 
            "processed_days": processed_count, 
            "inserted_records": inserted_count,
            "table": "cache_daily_hourly"
        }

    except Exception as e:
        import traceback
        traceback.print_exc()
        return JSONResponse(status_code=500, content={"error": str(e)})

def update_price_cache_for_date(target_date_str: str, only_weather: bool = False) -> int:
    """
    æ›´æ–°æŒ‡å®šæ—¥æœŸçš„ç”µä»·ç¼“å­˜ (ä¾› generate_price_cache å’Œ import_file è°ƒç”¨)
    è¿”å›æ’å…¥/æ›´æ–°çš„è®°å½•æ•° (æœ€å¤§24)
    
    Args:
        target_date_str: ç›®æ ‡æ—¥æœŸ YYYY-MM-DD
        only_weather: æ˜¯å¦åªæ›´æ–°å¤©æ°”æ•°æ® (ä¿ç•™åŸæœ‰ç”µåŠ›æ•°æ®)
    """
    from sql_config import SQL_RULES, TABLE_SOURCE_POWER, TABLE_SOURCE_WEATHER
    
    table_name = "cache_daily_hourly"

    # 1. ç¡®ä¿è¡¨å­˜åœ¨
    # (ä¸ºäº†æ€§èƒ½ï¼Œè¿™é‡Œå¯ä»¥å‡è®¾è¡¨å·²å­˜åœ¨ï¼Œæˆ–è€…æ¯æ¬¡éƒ½æ£€æŸ¥ï¼Œå¯¹äºå•æ¬¡å¯¼å…¥æ£€æŸ¥ä¸€ä¸‹æ— å¦¨)
    # æ„å»ºå­—æ®µåˆ—è¡¨
    columns_def = [
        "`record_date` DATE NOT NULL",
        "`hour` TINYINT NOT NULL",
        "`updated_at` TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP"
    ]
    
    calc_fields = {
        "price_diff": "FLOAT COMMENT 'ä»·å·®'",
        "load_deviation": "FLOAT COMMENT 'è´Ÿè·åå·®'",
        "new_energy_forecast": "FLOAT COMMENT 'æ–°èƒ½æºé¢„æµ‹æ€»å’Œ'"
    }
    
    all_fields = {}
    for key, rule in SQL_RULES.items():
        field_name = key
        if key in ['date', 'day_type', 'week_day', 'weather', 'wind_direction']:
            col_type = "VARCHAR(50)"
        else:
            col_type = "FLOAT"
        all_fields[field_name] = f"`{field_name}` {col_type} COMMENT '{rule.get('name', '')}'"
        
    for k, v in calc_fields.items():
        all_fields[k] = f"`{k}` {v}"
        
    cols_sql = ",\n".join(list(all_fields.values()) + columns_def)
    
    with db_manager.engine.begin() as conn:
        create_sql = f"""
        CREATE TABLE IF NOT EXISTS {table_name} (
            {cols_sql},
            PRIMARY KEY (`record_date`, `hour`)
        ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
        """
        conn.execute(text(create_sql))
    
    # 2. è·å–æ•°æ® (ä½¿ç”¨ sql_config ä¸­çš„è§„åˆ™åŠ¨æ€æŸ¥è¯¢)
    # from sql_config import SQL_RULES, TABLE_SOURCE_POWER, TABLE_SOURCE_WEATHER (Moved to top)
    
    # 2.1 æ„é€ å°æ—¶æ•°æ®æ˜ å°„ {hour: {field_name: [val1, val2]}}
    # ...
    
    hourly_map = {h: {} for h in range(24)}
    
    # åˆå§‹åŒ–å­—æ®µåˆ—è¡¨ (ç”¨äº hourly_map)
    # åŒ…æ‹¬ price_da, price_rt ä»¥åŠ SQL_RULES ä¸­å®šä¹‰çš„æ‰€æœ‰ POWER æ•°æ®
    field_keys = ['price_da', 'price_rt']
    for k, v in SQL_RULES.items():
        if v.get('source') == TABLE_SOURCE_POWER and k not in ['price_da', 'price_rt']:
            field_keys.append(k)
            
    # å¦‚æœ only_weather=Trueï¼Œåˆ™ä¸éœ€è¦åˆå§‹åŒ–è¿™äº›å­—æ®µçš„åˆ—è¡¨ï¼Œä¹Ÿä¸éœ€è¦æŸ¥è¯¢ç”µåŠ›æ•°æ®
    if not only_weather:
        for h in range(24):
            for k in field_keys:
                hourly_map[h][k] = []

        # 2.2 è·å–æ—¥å‰/å®æ—¶ç”µä»· (ä¿ç•™ä¹‹å‰çš„ç‰¹å®šé€»è¾‘ï¼šåŒºåŸŸè¿‡æ»¤)
        da_result = importer.query_daily_averages([target_date_str], "æ—¥å‰èŠ‚ç‚¹ç”µä»·")
        da_data = da_result.get("data", [])
        
        rt_result = importer.query_daily_averages([target_date_str], "å®æ—¶èŠ‚ç‚¹ç”µä»·")
        rt_data = rt_result.get("data", [])
        
        def filter_and_process_price(data_list, type_key):
            filtered = [item for item in data_list if "äº‘å—" not in str(item.get('type', ''))]
            has_guangdong = any("å¹¿ä¸œ" in str(item.get('type', '')) for item in filtered)
            if has_guangdong:
                filtered = [item for item in filtered if "å¹¿ä¸œ" in str(item.get('type', ''))]
            
            for item in filtered:
                rt_val = item['record_time']
                norm_time = normalize_record_time(rt_val, target_date_str)
                if norm_time is None:
                    continue
                
                hour = norm_time.hour
                if 0 <= hour <= 23:
                    val = float(item['value']) if item['value'] is not None else 0
                    hourly_map[hour][type_key].append(val)

        filter_and_process_price(da_data, 'price_da')
        filter_and_process_price(rt_data, 'price_rt')

        # 2.3 è·å– SQL_RULES ä¸­å®šä¹‰çš„å…¶ä»–ç”µåŠ›æ•°æ®
        # æ„é€ è¡¨å
        d_obj = datetime.datetime.strptime(target_date_str, "%Y-%m-%d")
        table_name_power = f"power_data_{d_obj.strftime('%Y%m%d')}"
        
        # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
        if table_name_power in db_manager.get_tables():
            with db_manager.engine.connect() as conn:
                for key, rule in SQL_RULES.items():
                    if rule.get('source') == TABLE_SOURCE_POWER and key not in ['price_da', 'price_rt']:
                        where_clause = rule.get('where')
                        if not where_clause:
                            continue
                            
                        try:
                            sql = text(f"SELECT record_time, value FROM {table_name_power} WHERE {where_clause}")
                            result = conn.execute(sql).fetchall()
                            
                            for row in result:
                                rt_val = row[0]
                                norm_time = normalize_record_time(rt_val, target_date_str)
                                if norm_time is None:
                                    continue
                                
                                hour = norm_time.hour
                                if 0 <= hour <= 23:
                                    val = float(row[1]) if row[1] is not None else 0
                                    hourly_map[hour][key].append(val)
                        except Exception as e:
                            print(f"æŸ¥è¯¢è§„åˆ™ {key} å¤±è´¥: {e}")

    # 2.4 è·å– SQL_RULES ä¸­å®šä¹‰çš„å¤©æ°”æ•°æ® (TABLE_SOURCE_WEATHER)
    # è¿™éƒ¨åˆ†æ•°æ®éœ€è¦ä» calendar_weather è¡¨ä¸­æŸ¥è¯¢ï¼Œç„¶åæ‹†è§£ json
    # æŸ¥è¯¢è¯¥æ—¥æœŸçš„å¤©æ°”æ•°æ®
    weather_row = None
    with db_manager.engine.connect() as conn:
        try:
            sql = text("SELECT * FROM calendar_weather WHERE date = :d")
            weather_row = conn.execute(sql, {"d": target_date_str}).mappings().fetchone()
        except Exception as e:
            print(f"æŸ¥è¯¢å¤©æ°”æ•°æ®å¤±è´¥: {e}")

    # æ— è®ºæ˜¯å¦æœ‰ weather_rowï¼Œå¦‚æœè¯¥æ—¥æœŸåªæœ‰å¤©æ°”æ•°æ®è€Œæ²¡æœ‰ç”µåŠ›æ•°æ®ï¼Œæˆ‘ä»¬ä¹Ÿå¸Œæœ›èƒ½å…¥åº“
    # æ‰€ä»¥å¿…é¡»ç¡®ä¿éå†åˆ°æ‰€æœ‰å¯èƒ½çš„æ¥æº
    
    if weather_row:
        # è§£æ JSON
        weather_json = None
        if weather_row.get('weather_json'):
            try:
                if isinstance(weather_row['weather_json'], str):
                    weather_json = json.loads(weather_row['weather_json'])
                else:
                    weather_json = weather_row['weather_json']
            except:
                pass
        
        # éå†è§„åˆ™å¡«å……æ•°æ®
        for key, rule in SQL_RULES.items():
            if rule.get('source') == TABLE_SOURCE_WEATHER:
                # 1. ç›´æ¥æ˜ å°„åˆ—
                col_name = rule.get('column')
                json_key = rule.get('json_key')
                
                # å¦‚æœæœ‰ json_keyï¼Œåˆ™ä» JSON ä¸­å–å€¼ (é€šå¸¸æ˜¯æ•°ç»„)
                if json_key and weather_json and json_key in weather_json:
                    values = weather_json[json_key]
                    if isinstance(values, list):
                        # å‡è®¾æ•°ç»„é•¿åº¦ä¸º 24ï¼Œå¯¹åº” 0-23 å°æ—¶
                        # å¦‚æœä¸è¶³ 24ï¼Œåˆ™å°½åŠ›å¡«å……
                        for h in range(min(len(values), 24)):
                            val = values[h]
                            if val is not None:
                                try:
                                    hourly_map[h].setdefault(key, []).append(float(val))
                                except (ValueError, TypeError):
                                    hourly_map[h].setdefault(key, []).append(val)
                
                # 2. å¦‚æœæ²¡æœ‰ json_keyï¼Œåˆ™æ˜¯å–åˆ—çš„æ ‡é‡å€¼ (å…¨å¤©ç›¸åŒ)
                elif col_name and col_name in weather_row and not json_key:
                    val = weather_row[col_name]
                    # ç‰¹æ®Šå¤„ç†æ—¥æœŸå­—æ®µï¼Œå°†å…¶è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                    if isinstance(val, (datetime.date, datetime.datetime)):
                        val = val.strftime("%Y-%m-%d")
                        
                    if val is not None:
                        # å…¨å¤© 24 å°æ—¶éƒ½ç”¨è¿™ä¸ªå€¼
                        for h in range(24):
                            # æ³¨æ„ï¼šå¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œappend åæ±‚å‡å€¼ä¼šæŠ¥é”™
                            # è¿™é‡Œéœ€è¦åˆ¤æ–­ç±»å‹
                            if isinstance(val, (int, float)):
                                hourly_map[h].setdefault(key, []).append(float(val))
                            else:
                                hourly_map[h].setdefault(key, []).append(val)
    
    # å³ä½¿æ²¡æœ‰ weather_rowï¼Œä¹Ÿå¯èƒ½å› ä¸ºæœ‰ç”µåŠ›æ•°æ®è€Œç»§ç»­æ‰§è¡Œ
    # å¦‚æœåªæœ‰å¤©æ°”æ•°æ®æ²¡æœ‰ç”µåŠ›æ•°æ®ï¼Œä¹Ÿä¼šå› ä¸º weather_row å­˜åœ¨è€Œæœ‰æ•°æ®
    # å¦‚æœä¸¤è€…éƒ½æ²¡æœ‰ï¼Œä¸‹é¢çš„ batch_data ä¸ºç©ºï¼Œè¿”å› 0

    # 4. æ„é€ å…¥åº“æ•°æ®
    batch_data = []
    
    # æ”¶é›†æ‰€æœ‰éœ€è¦æ›´æ–°çš„å­—æ®µ
    all_update_fields = set()
    
    if not only_weather:
        all_update_fields.add('price_da')
        all_update_fields.add('price_rt')
        all_update_fields.add('price_diff')
        all_update_fields.add('new_energy_forecast')
        all_update_fields.add('load_deviation')
        for k in field_keys:
            all_update_fields.add(k)
    
    # æ·»åŠ å¤©æ°”ç›¸å…³å­—æ®µåˆ°æ›´æ–°åˆ—è¡¨
    for key, rule in SQL_RULES.items():
        if rule.get('source') == TABLE_SOURCE_WEATHER:
            all_update_fields.add(key)

    for h in range(24):
        row_data = {
            "record_date": target_date_str,
            "hour": h
        }
        
        has_data = False
        
        # å¤„ç†å‡å€¼å­—æ®µ
        for k in list(all_update_fields): # éå†æ‰€æœ‰å¯èƒ½å­—æ®µ
            if k in ['record_date', 'hour', 'price_diff', 'new_energy_forecast', 'load_deviation']:
                continue
                
            vals = hourly_map[h].get(k, [])
            if vals:
                # æ£€æŸ¥æ˜¯å¦æ˜¯æ•°å­—
                first_val = vals[0]
                # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœ first_val æ˜¯ datetime.date å¯¹è±¡ï¼Œä¹Ÿè½¬ä¸ºå­—ç¬¦ä¸²
                if isinstance(first_val, (datetime.date, datetime.datetime)):
                    first_val = first_val.strftime("%Y-%m-%d")
                    row_data[k] = first_val
                elif isinstance(first_val, (int, float)):
                    avg = sum(vals) / len(vals)
                    row_data[k] = avg
                else:
                    # éæ•°å­—ï¼Œå–ç¬¬ä¸€ä¸ªéç©ºå€¼
                    row_data[k] = first_val
                has_data = True
            else:
                row_data[k] = None
                
            # [æ–°å¢] å¯¹æ‰€æœ‰ row_data çš„å€¼å†æ¬¡è¿›è¡Œç±»å‹æ¸…æ´—ï¼Œç¡®ä¿æ²¡æœ‰ date å¯¹è±¡
            val = row_data[k]
            if isinstance(val, (datetime.date, datetime.datetime)):
                row_data[k] = val.strftime("%Y-%m-%d")
        
        # å¦‚æœæ•´è¡Œæ²¡æœ‰ä»»ä½•æ•°æ®(è¿ç”µä»·éƒ½æ²¡æœ‰)ï¼Œæ˜¯å¦è·³è¿‡ï¼Ÿ
        # å¦‚æœæ˜¯å¢é‡æ›´æ–°ï¼Œå¯èƒ½åªæƒ³æ›´æ–°éƒ¨åˆ†å­—æ®µã€‚
        # ä½†å¦‚æœæ˜¯ Upsertï¼ŒNone ä¼šè¦†ç›–æ—§å€¼å—ï¼Ÿ
        # æˆ‘ä»¬åº”è¯¥åªåŒ…å«æœ‰å€¼çš„å­—æ®µï¼Œæˆ–è€…å…¨éƒ¨åŒ…å«ã€‚
        # è¿™é‡Œé€‰æ‹©ï¼šå¦‚æœæ²¡æœ‰ä»»ä½•æ•°æ®ï¼Œè·³è¿‡è¯¥å°æ—¶ï¼›å¦åˆ™æ’å…¥/æ›´æ–°æ‰€æœ‰å­—æ®µã€‚
        # ä¿®æ”¹é€»è¾‘ï¼šåªè¦æœ‰å¤©æ°”æ•°æ®ä¹Ÿç®—æœ‰æ•°æ®ï¼Œä¸èƒ½è·³è¿‡
        if not has_data:
            continue
            
        # è®¡ç®—è¡ç”Ÿå­—æ®µ
        # 1. ä»·å·®
        p_da = row_data.get('price_da')
        p_rt = row_data.get('price_rt')
        # ä¿®æ”¹é€»è¾‘ï¼šåªè¦å…¶ä¸­ä¸€ä¸ªæœ‰å€¼å°±å¯ä»¥æ›´æ–°ï¼Œè€Œä¸æ˜¯å¿…é¡»ä¸¤ä¸ªéƒ½æœ‰
        # å¦‚æœåªæœ‰ä¸€ä¸ªæœ‰å€¼ï¼Œdiff ä¸º None (å› ä¸ºæ— æ³•è®¡ç®—ä»·å·®)ï¼Œä½†åŸæœ‰çš„å€¼åº”è¯¥ä¿ç•™
        if p_da is not None and p_rt is not None:
            row_data['price_diff'] = p_da - p_rt
        else:
            row_data['price_diff'] = None
            
        # 2. æ–°èƒ½æºé¢„æµ‹æ€»å’Œ (å…‰ä¼+é£ç”µ)
        # å‡è®¾è§„åˆ™é‡Œæœ‰ ne_pv_forecast å’Œ ne_wind_forecast
        pv = row_data.get('ne_pv_forecast', 0) or 0
        wind = row_data.get('ne_wind_forecast', 0) or 0
        if pv or wind:
            row_data['new_energy_forecast'] = pv + wind
        else:
            row_data['new_energy_forecast'] = None

        # 3. è´Ÿè·åå·® (é¢„æµ‹ - å®é™…)
        l_fore = row_data.get('load_forecast')
        l_act = row_data.get('load_actual')
        if l_fore is not None and l_act is not None:
            row_data['load_deviation'] = l_fore - l_act
        else:
            row_data['load_deviation'] = None
            
        # [æ–°å¢] ç¡®ä¿ record_date å’Œ hour å§‹ç»ˆå­˜åœ¨ (è™½ç„¶å‰é¢å·²ç»å®šä¹‰äº†)
        row_data['record_date'] = target_date_str
        row_data['hour'] = h
            
        batch_data.append(row_data)
    
    # 5. å…¥åº“
    if batch_data:
        # åŠ¨æ€æ„å»º SQL
        # å­—æ®µåˆ—è¡¨: record_date, hour + å…¶ä»–æ‰€æœ‰å­—æ®µ
        # å› ä¸º batch_data é‡Œçš„ keys å¯èƒ½ä¸å®Œå…¨ä¸€è‡´(æœ‰äº›æ˜¯ None)ï¼Œæœ€å¥½ç»Ÿä¸€ä¸€ä¸‹
        # å…¶å® executemany è¦æ±‚æ‰€æœ‰å­—å…¸ keys ä¸€è‡´
        
        # ç¡®ä¿æ‰€æœ‰å­—å…¸éƒ½æœ‰æ‰€æœ‰å­—æ®µ
        final_keys = list(all_update_fields)
        # è¿‡æ»¤æ‰ä¸åœ¨ batch_data[0] é‡Œçš„ key (è™½ç„¶æˆ‘ä»¬åœ¨å¾ªç¯é‡Œéƒ½åŠ äº†)
        # ä¸ºäº†å®‰å…¨ï¼Œé‡æ–°æ•´ç† batch_data
        
        # ç§»é™¤ 'record_date' å’Œ 'hour'ï¼Œå› ä¸ºå®ƒä»¬å·²ç»å•ç‹¬å¤„ç†
        if 'record_date' in final_keys:
             final_keys.remove('record_date')
        if 'hour' in final_keys:
             final_keys.remove('hour')
             
        # [DEBUG] æ‰“å°ä¸€ä¸‹ final_keys å’Œ batch_data çš„æ ·ä¾‹ï¼Œæ–¹ä¾¿è°ƒè¯•
        if len(batch_data) > 0:
             print(f"[DEBUG] Cache Update for {target_date_str}: {len(batch_data)} records")
             # print(f"[DEBUG] Keys: {final_keys}")
             # print(f"[DEBUG] Sample Row: {batch_data[0]}")
        else:
             print(f"[DEBUG] Cache Update for {target_date_str}: NO DATA to update.")
             if weather_row:
                 print(f"[DEBUG] Weather Row found but no data mapped? Weather Keys: {weather_row.keys()}")
             else:
                 print(f"[DEBUG] No Weather Row and No Power Data.")
        
        clean_batch = []
        for row in batch_data:
            clean_row = {"record_date": row["record_date"], "hour": row["hour"]}
            for k in final_keys:
                clean_row[k] = row.get(k) # é»˜è®¤ä¸º None
            clean_batch.append(clean_row)
            
        # æ„å»º INSERT ... ON DUPLICATE KEY UPDATE è¯­å¥
        field_list = [f"`{k}`" for k in final_keys]
        param_list = [f":{k}" for k in final_keys]
        
        # UPDATE éƒ¨åˆ†
        update_parts = [f"`{k}`=VALUES(`{k}`)" for k in final_keys]
        
        # æ³¨æ„: è¿™é‡Œçš„ record_date å’Œ hour éœ€è¦æ˜¾å¼åŠ å…¥ VALUES åˆ—è¡¨ï¼Œä½†ä¸åœ¨ UPDATE åˆ—è¡¨(ä¸»é”®)
        sql = f"""
            INSERT INTO {table_name} 
            (`record_date`, `hour`, {', '.join(field_list)})
            VALUES (:record_date, :hour, {', '.join(param_list)})
            ON DUPLICATE KEY UPDATE
            {', '.join(update_parts)}
        """
        
        with db_manager.engine.begin() as conn:
             try:
                conn.execute(text(sql), clean_batch)
             except Exception as e:
                 print(f"âš ï¸ SQL Execution Failed for {target_date_str}: {e}")
                 import traceback
                 traceback.print_exc()
                 raise e # é‡æ–°æŠ›å‡ºä»¥ä¾¿ä¸Šå±‚æ•è·
            
        return len(clean_batch)
    
    return 0

def normalize_record_time(val, date_str):
    """æ ‡å‡†åŒ–æ—¶é—´å­—æ®µï¼Œå¤„ç† timedelta å’Œ datetime"""
    try:
        # 1. å·²ç»æ˜¯ datetime
        if isinstance(val, datetime.datetime):
            return val
            
        # 2. æ˜¯ timedelta (Python/Pandas/NumPy)
        # æ³¨æ„: pd.Timedelta ä¹Ÿæ˜¯ timedelta çš„å­ç±» (åœ¨æŸäº›ç‰ˆæœ¬ä¸­)ï¼Œæˆ–è€…è¡Œä¸ºç±»ä¼¼
        # åˆ†å¼€æ£€æŸ¥æ›´ç¨³å¦¥
        is_delta = isinstance(val, (datetime.timedelta, pd.Timedelta, np.timedelta64))
        
        if is_delta:
            base_date = pd.to_datetime(date_str)
            return base_date + val
            
        # 3. å°è¯• pd.to_datetime (é’ˆå¯¹å­—ç¬¦ä¸²æˆ– timestamp)
        # å¦‚æœ val æ˜¯ timedelta ç±»å‹çš„å­—ç¬¦ä¸² (å¦‚ "00:15:00")ï¼Œpd.to_datetime å¯èƒ½ä¼šæŠ¥é”™æˆ–è¡Œä¸ºä¸ç¬¦åˆé¢„æœŸ
        # æ‰€ä»¥å…ˆå°è¯•è½¬ timedelta
        try:
            base_date = pd.to_datetime(date_str)
            delta = pd.to_timedelta(val)
            return base_date + delta
        except:
            pass

        return pd.to_datetime(val)
    except:
        # 4. æœ€åçš„å°è¯•
        try:
            base_date = pd.to_datetime(date_str)
            # å‡è®¾ val æ˜¯æŸç§å¯ä»¥è½¬ä¸º timedelta çš„ä¸œè¥¿
            delta = pd.to_timedelta(val)
            return base_date + delta
        except:
            # æ‰“å°é”™è¯¯ä»¥ä¾¿è°ƒè¯•ï¼Œä½†åœ¨ç”Ÿäº§ç¯å¢ƒä¸­å¯èƒ½å¤ªåµ
            # print(f"Failed to normalize time: {val} type: {type(val)}")
            return None

if __name__ == "__main__":
    uvicorn.run("api:app", host="0.0.0.0", port=8000, reload=True)