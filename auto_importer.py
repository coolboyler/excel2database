import pandas as pd
import re
import datetime

class AutoImporterGenerator:
    def __init__(self):
        # è‹±æ–‡ç¿»è¯‘æ˜ å°„å­—å…¸ (ç®€å•ç¤ºä¾‹ï¼Œå¯æ‰©å±•)
        self.translation_map = {
            "ç”µå‚åç§°": "power_plant_name",
            "æœºç»„åç§°": "generator_name",
            "æœ€å°æŠ€æœ¯å‡ºåŠ›": "min_technical_output",
            "æœ€å°æŠ€æœ¯å‡ºåŠ›(MW)": "min_technical_output",
            "é¢å®šå‡ºåŠ›": "rated_output",
            "é¢å®šå‡ºåŠ›(MW)": "rated_output",
            "æ—¥æœŸ": "maintenance_date", # ä¿®æ­£ï¼šæ—¥æœŸé€šå¸¸æ˜¯æ£€ä¿®æ—¥æœŸ
            "æ—¶é—´": "record_time",
            "ç±»å‹": "type",
            "å¤‡æ³¨": "remarks",
            "åºå·": "seq_no",
            "å…ƒä»¶åç§°": "component_name", # æ·»åŠ 
            "è®¾å¤‡åç§°": "device_name",
            "ç”µå‹ç­‰çº§": "voltage_level",
            "ç”µå‹ç­‰çº§(Kv)": "voltage_level", # æ·»åŠ 
            "åœç”µèŒƒå›´": "outage_scope",
            "åœç”µæ—¶é—´": "outage_time",
            "é€ç”µæ—¶é—´": "restore_time",
            "å·¥ä½œå†…å®¹": "work_content",
            "æ£€ä¿®æ€§è´¨": "maintenance_type",
            "ç”³è¯·å•ä½": "applicant",
            # Sheetåç¿»è¯‘
            "æœºç»„æ£€ä¿®é¢„æµ‹ä¿¡æ¯": "unit_maintenance_prediction",
            "æœºç»„æŠ€æœ¯å‚æ•°": "unit_technical_parameters",
            "æ£€ä¿®è®¡åˆ’": "maintenance_plan",
            "è¾“å˜ç”µæ£€ä¿®é¢„æµ‹ä¿¡æ¯": "transmission_maintenance", # æ·»åŠ 
            # New mappings from user request
            "æ¸©åº¦": "temperature", "å¤©æ°”": "weather", "é£å‘": "wind_direction", "é£é€Ÿ": "wind_speed",
            "é™é›¨æ¦‚ç‡": "precipitation_probability", "ä½“æ„Ÿæ¸©åº¦": "apparent_temperature",
            "æ¹¿åº¦": "humidity", "ç´«å¤–çº¿": "uv_index", "äº‘é‡": "cloud_cover", "é™é›¨é‡": "rainfall",
            "æ˜ŸæœŸ": "week_day", "å¤©": "day",
            "ç»Ÿè°ƒé¢„æµ‹": "dispatch_forecast", "Aç±»ç”µæºé¢„æµ‹": "class_a_power_forecast",
            "Bç±»ç”µæºé¢„æµ‹": "class_b_power_forecast", "åœ°æ–¹ç”µæºé¢„æµ‹": "local_power_forecast",
            "è¥¿ç”µä¸œé€ç”µæºé¢„æµ‹": "west_to_east_power_forecast", "ç²¤æ¸¯æ¾³é¢„æµ‹": "guangdong_hongkong_macau_forecast",
            "å‘ç”µæ€»é¢„æµ‹": "total_generation_forecast", "ç°è´§æ–°èƒ½æºDæ—¥é¢„æµ‹": "spot_new_energy_day_ahead_forecast",
            "ç»Ÿè°ƒæ–°èƒ½æºå…‰ä¼é¢„æµ‹": "dispatch_new_energy_pv_forecast", "ç»Ÿè°ƒæ–°èƒ½æºé£ç”µé¢„æµ‹": "dispatch_new_energy_wind_forecast",
            "æ°´ç”µï¼ˆå«æŠ½è“„ï¼‰é¢„æµ‹": "hydro_power_forecast_incl_pumped", "æŠ½è“„å‡ºåŠ›é¢„æµ‹": "pumped_storage_output_forecast",
            "å®é™…ç»Ÿè°ƒè´Ÿè·": "actual_dispatch_load", "Aç±»ç”µæºå®é™…": "actual_class_a_power",
            "Bç±»ç”µæºå®é™…": "actual_class_b_power", "åœ°æ–¹ç”µæºå®é™…": "actual_local_power",
            "è¥¿ç”µä¸œé€å®é™…": "actual_west_to_east_power", "ç²¤æ¸¯è”ç»œå®é™…": "actual_guangdong_hongkong_link",
            "æ–°èƒ½æºæ€»å®é™…": "actual_total_new_energy", "æ°´ç”µå«æŠ½è“„å®é™…": "actual_hydro_power_incl_pumped",
            "ç»Ÿè°ƒè´Ÿè·åå·®": "dispatch_load_deviation",
            # ... æ·»åŠ æ›´å¤šæ˜ å°„
        }
        
    def translate_col(self, col_name):
        """å°è¯•ç¿»è¯‘åˆ—åï¼Œå¦‚æœæ‰¾ä¸åˆ°åˆ™æ‹¼éŸ³æˆ–ä¿ç•™"""
        # 1. æŸ¥å­—å…¸
        clean_col = str(col_name).strip()
        if clean_col in self.translation_map:
            return self.translation_map[clean_col]
        
        # 2. å°è¯•éƒ¨åˆ†åŒ¹é… (ä¾‹å¦‚ "æœ€å°æŠ€æœ¯å‡ºåŠ›(MW)" -> "min_technical_output")
        for k, v in self.translation_map.items():
            if k in clean_col:
                return v
        
        # 3. å¦‚æœæ˜¯çº¯ä¸­æ–‡ï¼Œç®€å•è½¬æ‹¼éŸ³? è¿™é‡Œæš‚æ—¶ç”¨ safe_name
        # å®é™…ç”Ÿäº§ç¯å¢ƒå¯ä»¥ä½¿ç”¨ pypinyin åº“
        # è¿™é‡Œä»…åš safe å¤„ç†
        return clean_col.replace("(", "_").replace(")", "_").replace(" ", "_").strip()

    def analyze_and_generate(self, file_path):
        """
        åˆ†æExcelæ–‡ä»¶å¹¶ç”ŸæˆImporterä»£ç 
        """
        try:
            xls = pd.ExcelFile(file_path)
            sheet_names = xls.sheet_names
        except Exception as e:
            return {"error": str(e)}

        # æ¸…ç†æ–‡ä»¶åä¸­çš„æ—¥æœŸéƒ¨åˆ†ï¼Œç”Ÿæˆé€šç”¨çš„å‡½æ•°å
        # ä¾‹å¦‚: "2023-01-01_Test" -> "Test"
        base_name = file_path.split("/")[-1].replace(".xlsx", "")
        # ç§»é™¤æ—¥æœŸæ¨¡å¼ YYYY-MM-DD æˆ– YYYYMMDD
        base_name_clean = re.sub(r'\d{4}[-_]?\d{1,2}[-_]?\d{1,2}', '', base_name)
        # ç§»é™¤å¯èƒ½å‰©ä¸‹çš„é¦–å°¾åˆ†éš”ç¬¦
        base_name_clean = re.sub(r'^[-_]+|[-_]+$', '', base_name_clean)
        
        # å¦‚æœæ¸…ç†åä¸ºç©ºï¼ˆä¾‹å¦‚æ–‡ä»¶åå°±æ˜¯æ—¥æœŸï¼‰ï¼Œåˆ™ä½¿ç”¨ Generic
        if not base_name_clean.strip():
            base_name_clean = "Generic"
            
        filename_clean = self.clean_name(base_name_clean)
        
        analysis_result = {
            "filename": file_path.split("/")[-1],
            "sheets": [],
            "generated_code": ""
        }

        code_buffer = []
        
        # 1. API Usage Snippet
        code_buffer.append(f"# ==========================================")
        code_buffer.append(f"# 1. å°†ä»¥ä¸‹ä»£ç æ·»åŠ åˆ° api.py çš„ import_file å‡½æ•°ä¸­")
        code_buffer.append(f"# ==========================================")
        code_buffer.append(f"    # elif '{filename_clean}' in filename:")
        code_buffer.append(f"    #     method = importer.import_{filename_clean}")
        code_buffer.append(f"")
        code_buffer.append(f"# ==========================================")
        code_buffer.append(f"# 2. å°†ä»¥ä¸‹ä»£ç æ·»åŠ åˆ° pred_reader.py çš„ PowerDataImporter ç±»ä¸­")
        code_buffer.append(f"# ==========================================")
        code_buffer.append(f"")
        
        # 2. Main Import Method
        code_buffer.append(f"    def import_{filename_clean}(self, excel_file):")
        code_buffer.append(f"        \"\"\"è‡ªåŠ¨ç”Ÿæˆçš„å¯¼å…¥å‡½æ•°: {analysis_result['filename']} (ç±»)\"\"\"")
        code_buffer.append(f"        try:")
        code_buffer.append(f"            sheet_dict = pd.read_excel(excel_file, sheet_name=None, header=0)")
        code_buffer.append(f"        except Exception as e:")
        code_buffer.append(f"            print(f\"âŒ æ— æ³•è¯»å–Excel: {{e}}\")")
        code_buffer.append(f"            return False, None, 0, []")
        code_buffer.append(f"")
        code_buffer.append(f"        all_records = []")
        code_buffer.append(f"        data_date = None")
        code_buffer.append(f"        ")
        code_buffer.append(f"        # å°è¯•ä»æ–‡ä»¶åæå–æ—¥æœŸ")
        code_buffer.append(f"        match = re.search(r'(\d{{4}}-\d{{1,2}}-\d{{1,2}})', str(excel_file))")
        code_buffer.append(f"        if match:")
        code_buffer.append(f"            data_date = datetime.datetime.strptime(match.group(1), '%Y-%m-%d').date()")
        code_buffer.append(f"        else:")
        code_buffer.append(f"            # å¦‚æœæ–‡ä»¶åæ²¡æ—¥æœŸï¼Œå°è¯•ç”¨å½“å¤©æˆ–æŠ›å‡ºè­¦å‘Š")
        code_buffer.append(f"            print(f\"âš ï¸ æœªèƒ½åœ¨æ–‡ä»¶åä¸­è¯†åˆ«æ—¥æœŸï¼Œé»˜è®¤ä½¿ç”¨ä»Šæ—¥\")")
        code_buffer.append(f"            data_date = datetime.date.today()")
        code_buffer.append(f"")
        code_buffer.append(f"        # æ ¹æ®æ–‡ä»¶åè¯†åˆ«ç±»å‹")
        code_buffer.append(f"        file_name = str(excel_file)")
        code_buffer.append(f"        chinese_match = re.search(r'([\u4e00-\u9fff]+)', file_name)")
        code_buffer.append(f"        if chinese_match:")
        code_buffer.append(f"            data_type = chinese_match.group(1)")
        code_buffer.append(f"            print(f\"ğŸ“ æ–‡ä»¶ç±»å‹è¯†åˆ«: {{data_type}}\")")
        code_buffer.append(f"        else:")
        code_buffer.append(f"            data_type = \"è‡ªåŠ¨å¯¼å…¥\"")
        code_buffer.append(f"            print(f\"âš ï¸ æœªèƒ½åœ¨æ–‡ä»¶åä¸­æ‰¾åˆ°æ±‰å­—ï¼Œé»˜è®¤ç±»å‹: {{data_type}}\")")
        code_buffer.append(f"")
        code_buffer.append(f"        sheet_names = list(sheet_dict.keys())")
        code_buffer.append(f"")

        for i, sheet_name in enumerate(sheet_names):
            df = pd.read_excel(file_path, sheet_name=sheet_name, header=0)
            
            # åˆ†æSheetç»“æ„
            sheet_info = self.analyze_sheet(df, sheet_name)
            analysis_result["sheets"].append(sheet_info)

            # ç”Ÿæˆå¯¹åº”çš„å¤„ç†å‡½æ•°è°ƒç”¨ - ä½¿ç”¨ç´¢å¼•è€Œéåç§°
            func_name = f"_process_{filename_clean}_sheet_{i+1}"
            code_buffer.append(f"        # å¤„ç†ç¬¬ {i+1} ä¸ª Sheet (åŸå: {sheet_name})")
            code_buffer.append(f"        if len(sheet_names) > {i}:")
            code_buffer.append(f"            current_sheet_name = sheet_names[{i}]")
            code_buffer.append(f"            records = self.{func_name}(sheet_dict[current_sheet_name], data_date, current_sheet_name, data_type)")
            code_buffer.append(f"            all_records.extend(records)")
            code_buffer.append(f"")
        
        code_buffer.append(f"        if not all_records:")

        code_buffer.append(f"            print(\"âŒ æ²¡æœ‰ç”Ÿæˆä»»ä½•æœ‰æ•ˆè®°å½•\")")
        code_buffer.append(f"            return False, None, 0, []")
        code_buffer.append(f"")
        code_buffer.append(f"        # ä¿å­˜åˆ°æ•°æ®åº“")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ generic_table
        has_generic = any(s["pattern_type"] == "generic_table" for s in analysis_result["sheets"])
        if has_generic:
            code_buffer.append(f"        # ä½¿ç”¨è‡ªå®šä¹‰ä¿å­˜æ–¹æ³• (åŒ…å«Generic Table)")
            code_buffer.append(f"        return self.save_to_{filename_clean}_database(all_records, data_date)")
        else:
            code_buffer.append(f"        # ä½¿ç”¨é€šç”¨ä¿å­˜æ–¹æ³•")
            code_buffer.append(f"        return self.save_to_database(all_records, data_date)")
            
        code_buffer.append(f"")

        # 3. Helper Methods
        for i, sheet_info in enumerate(analysis_result["sheets"]):
            sheet_name = sheet_info["name"]
            func_name = f"_process_{filename_clean}_sheet_{i+1}"
            func_code = self.generate_func_code(func_name, sheet_info)
            code_buffer.append(func_code)
            code_buffer.append("")
            
        # 4. Generate Custom Save Method (Optional)
        # åªæœ‰å½“åŒ…å« generic_table æ—¶æ‰ç”Ÿæˆè‡ªå®šä¹‰ä¿å­˜æ–¹æ³•
        if any(s["pattern_type"] == "generic_table" for s in analysis_result["sheets"]):
            save_method_code = self.generate_custom_save_method(filename_clean, analysis_result["sheets"])
            code_buffer.append(save_method_code)
            code_buffer.append("")

        analysis_result["generated_code"] = "\n".join(code_buffer)
        return analysis_result

    def generate_custom_save_method(self, filename_clean, sheets):
        """
        ç”Ÿæˆè‡ªå®šä¹‰ä¿å­˜æ–¹æ³• (save_to_..._database)
        é’ˆå¯¹ Generic Table ç”Ÿæˆç‰¹å®šçš„ CREATE TABLE å’Œ INSERT è¯­å¥
        """
        lines = []
        # ä½¿ç”¨æ›´ç®€æ´çš„è¡¨åï¼Œä¸åŒ…å« custom_ å‰ç¼€
        table_name = f"{filename_clean.lower()}"
        
        lines.append(f"    def save_to_{filename_clean}_database(self, records, data_date):")
        lines.append(f"        \"\"\"ä¿å­˜ {filename_clean} æ•°æ®åˆ°è‡ªå®šä¹‰è¡¨ {table_name}\"\"\"")
        lines.append(f"        if not records:")
        lines.append(f"            print(\"âŒ æ²¡æœ‰å¯ä¿å­˜çš„è®°å½•\")")
        lines.append(f"            return True, None, 0, []")
        lines.append(f"")
        lines.append(f"        # 1. è¿‡æ»¤æ— æ•ˆè®°å½•")
        lines.append(f"        valid_records = []")
        lines.append(f"        for r in records:")
        lines.append(f"            if isinstance(r, dict):")
        lines.append(f"                r['record_date'] = data_date")
        lines.append(f"                valid_records.append(r)")
        lines.append(f"")
        lines.append(f"        if not valid_records:")
        lines.append(f"            return False, None, 0, []")
        lines.append(f"")
        
        # é’ˆå¯¹æ¯ä¸ª sheet åº”è¯¥æœ‰ç‹¬ç«‹çš„è¡¨ç»“æ„è®¾è®¡ï¼Œä½†å¦‚æœéƒ½ç”¨åŒä¸€ä¸ª save æ–¹æ³•ï¼Œ
        # æˆ‘ä»¬éœ€è¦åˆ¤æ–­è®°å½•å±äºå“ªä¸ª sheetï¼Œæˆ–è€…åˆ›å»ºä¸€ä¸ªè¶…çº§å®½è¡¨ã€‚
        # ç”¨æˆ·è¦æ±‚ "é’ˆå¯¹ä¸èƒ½è¯†åˆ«çš„sheetéƒ½è¦å„è‡ªåšå¯¹åº”çš„è¡¨ç»“æ„è®¾è®¡"
        # è¿™æ„å‘³ç€æˆ‘ä»¬éœ€è¦ä¸ºæ¯ä¸ª sheet ç”Ÿæˆä¸€ä¸ªç‹¬ç«‹çš„è¡¨ã€‚
        # ä½†æ˜¯ï¼Œæˆ‘ä»¬åªæœ‰ä¸€ä¸ª save æ–¹æ³•å…¥å£ã€‚
        # è§£å†³æ–¹æ¡ˆï¼šåœ¨ save æ–¹æ³•å†…éƒ¨ï¼Œæ ¹æ® sheet_name åˆ†å‘åˆ°ä¸åŒçš„è¡¨ã€‚
        
        lines.append(f"        # åˆ†å‘åˆ°ä¸åŒçš„è¡¨ (æ ¹æ® sheet_name)")
        lines.append(f"        # è‡ªåŠ¨ç”Ÿæˆçš„è¡¨åæ˜ å°„")
        
        # æ”¶é›† generic sheets
        generic_sheets = [s for s in sheets if s["pattern_type"] == "generic_table"]
        
        lines.append(f"        try:")
        lines.append(f"            with self.db_manager.engine.begin() as conn:")
        
        for i, sheet in enumerate(generic_sheets):
            sheet_safe_name = self.clean_name(sheet["name"]).lower()
            # åŸºç¡€Sheetå (å»é™¤æ—¥æœŸ)ï¼Œç”¨äºåŒ¹é…å’Œè¡¨åç”Ÿæˆ
            base_sheet_name = self.remove_date_from_sheetname(sheet["name"])
            
            # å°è¯•ç¿»è¯‘ Base Sheet Name
            translated_base_name = self.translate_col(base_sheet_name)
            # å¦‚æœç¿»è¯‘å¤±è´¥ï¼ˆè¿”å›äº†æ‹¼éŸ³æˆ–åŸå§‹å€¼ï¼‰ï¼Œä¸”åŸå§‹å€¼åŒ…å«ä¸­æ–‡ï¼Œåˆ™è¿™é‡Œå¯èƒ½è¿˜æ˜¯æœ‰é—®é¢˜
            # ä½† translate_col ç›®å‰åªæ˜¯æŸ¥è¡¨ï¼Œæˆ‘ä»¬éœ€è¦æ‰©å±•å®ƒæ”¯æŒæ›´å¤šæˆ–è€…è®©ç”¨æˆ·å»å¡«
            # è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ translated_base_name ä½œä¸ºè¡¨åçš„ä¸€éƒ¨åˆ†
            base_sheet_safe = self.clean_name(translated_base_name).lower()
            
            # è¡¨åè§„åˆ™: æ–‡ä»¶å_base_sheetå
            current_table_name = f"{filename_clean.lower()}_{base_sheet_safe}"
            if filename_clean.lower() == base_sheet_safe or not base_sheet_safe:
                current_table_name = f"{filename_clean.lower()}_data"
                
            lines.append(f"                # --- å¤„ç† Sheet: {sheet['name']} (Base: {base_sheet_name}) -> è¡¨: {current_table_name} ---")
            # ä½¿ç”¨æ›´å®½å®¹çš„åŒ¹é…é€»è¾‘: åªè¦ base_sheet_name åœ¨ sheet_name ä¸­å³å¯ (ä¸”ä¸åŒ…å«æ—¥æœŸå¹²æ‰°)
            # æˆ–è€…ï¼Œå¦‚æœ base_sheet_name å¾ˆçŸ­ï¼Œå¯èƒ½ä¼šè¯¯åˆ¤ã€‚
            # æ›´å®‰å…¨çš„åšæ³•: å‡è®¾ import é˜¶æ®µä¼ è¿›æ¥çš„ sheet_name æ˜¯å®Œæ•´çš„ã€‚
            # æˆ‘ä»¬åœ¨ä»£ç é‡Œä¹ŸåšåŒæ ·çš„ clean æ“ä½œæ¥æ¯”è¾ƒ?
            # ä¸ºäº†ç®€å•æœ‰æ•ˆï¼Œæˆ‘ä»¬ç”Ÿæˆä¸€è¡Œä»£ç æ¥æ£€æŸ¥:
            lines.append(f"                current_sheet_records = []")
            lines.append(f"                for r in valid_records:")
            lines.append(f"                    r_sheet = str(r.get('sheet_name', ''))")
            lines.append(f"                    # ç§»é™¤æ—¥æœŸåæ¯”è¾ƒ")
            lines.append(f"                    r_base = re.sub(r'\\d{{4}}[-/]?\\d{{1,2}}[-/]?\\d{{1,2}}', '', r_sheet).replace('()', '').strip()")
            lines.append(f"                    # å¦‚æœ base_name åŒ…å«åœ¨å¤„ç†åçš„ r_base ä¸­ï¼Œæˆ–è€… r_base åŒ…å« base_name")
            lines.append(f"                    if '{base_sheet_name}' in r_base or r_base == '{base_sheet_name}':")
            lines.append(f"                        current_sheet_records.append(r)")
            
            lines.append(f"                if current_sheet_records:")
            lines.append(f"                    # 2. åˆ›å»ºè¡¨")
            lines.append(f"                    create_sql = f\"\"\"")
            lines.append(f"                    CREATE TABLE IF NOT EXISTS `{current_table_name}` (")
            lines.append(f"                        `id` bigint(20) NOT NULL AUTO_INCREMENT,")
            lines.append(f"                        `record_date` date DEFAULT NULL,")
            lines.append(f"                        `sheet_name` varchar(255) DEFAULT NULL,")
            lines.append(f"                        `type` varchar(100) DEFAULT NULL,")
            
            # ç”Ÿæˆè¯¥ sheet ç‰¹æœ‰çš„åˆ—
            for col in sorted(sheet["columns"]):
                safe_col = self.translate_col(col)
                lines.append(f"                        `{safe_col}` text COMMENT '{col}',")
                
            lines.append(f"                        `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,")
            lines.append(f"                        PRIMARY KEY (`id`),")
            lines.append(f"                        KEY `idx_record_date` (`record_date`)")
            lines.append(f"                    ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;")
            lines.append(f"                    \"\"\"")
            lines.append(f"                    conn.execute(text(create_sql))")
            lines.append(f"")
            lines.append(f"                    # 3. åˆ é™¤æ—§æ•°æ®")
            lines.append(f"                    conn.execute(text(f\"DELETE FROM {current_table_name} WHERE record_date = :date\"), {{'date': data_date}})")
            lines.append(f"")
            lines.append(f"                    # 4. æ’å…¥æ–°æ•°æ®")
            
            # ç”Ÿæˆ INSERT è¯­å¥
            col_map = {}
            for col in sorted(sheet["columns"]):
                safe = self.translate_col(col)
                col_map[col] = safe
            
            # å…³é”®ä¿®å¤ï¼šæ„å»ºæ¸…æ´—åçš„è®°å½•ç”¨äºæ’å…¥ (ä¿®å¤ç»‘å®šå‚æ•°åŒ…å«ç‰¹æ®Šå­—ç¬¦çš„é—®é¢˜)
            lines.append(f"                    sanitized_records = []")
            lines.append(f"                    for r in current_sheet_records:")
            lines.append(f"                        new_r = {{'record_date': r['record_date'], 'sheet_name': r['sheet_name'], 'type': r['type']}}")
            # åŠ¨æ€æ˜ å°„ï¼šcol_map åŒ…å«äº† {åŸå§‹åˆ—å: å®‰å…¨åˆ—å}
            lines.append(f"                        for original_col, safe_col in {col_map}.items():")
            lines.append(f"                            if original_col in r:")
            lines.append(f"                                new_r[safe_col] = r[original_col]")
            lines.append(f"                        sanitized_records.append(new_r)")
            
            insert_cols = ['record_date', 'sheet_name', 'type']
            insert_params = [':record_date', ':sheet_name', ':type']
            
            for col in col_map:
                safe_name = col_map[col]
                insert_cols.append(f'`{safe_name}`')
                insert_params.append(f':{safe_name}') # ä½¿ç”¨å®‰å…¨åä½œä¸ºå‚æ•°å
            
            lines.append(f"                    insert_sql = text(f\"INSERT INTO {current_table_name} ({', '.join(insert_cols)}) VALUES ({', '.join(insert_params)})\")")
            lines.append(f"                    conn.execute(insert_sql, sanitized_records)")
            lines.append(f"                    print(f\"âœ… å·²ä¿å­˜ {{len(current_sheet_records)}} æ¡è®°å½•åˆ° {current_table_name}\")")
            
            # 5. è·å–é¢„è§ˆæ•°æ® (é’ˆå¯¹ç¬¬ä¸€ä¸ª sheet æˆ–åˆå¹¶é¢„è§ˆ)
            # è¿™é‡Œç®€å•å–å‰ 10 æ¡
            lines.append(f"                    if i == 0: # ä»…é¢„è§ˆç¬¬ä¸€ä¸ªåŒ¹é…è¡¨çš„")
            lines.append(f"                        preview_data = sanitized_records[:10]")
            lines.append(f"")

        lines.append(f"                # è¿”å›ç»“æœï¼ŒåŒ…æ‹¬é¢„è§ˆæ•°æ®")
        lines.append(f"                return True, \"{table_name}_*\", len(valid_records), preview_data if 'preview_data' in locals() else []")
        lines.append(f"        except Exception as e:")
        lines.append(f"            print(f\"âŒ ä¿å­˜å¤±è´¥: {{e}}\")")
        lines.append(f"            return False, None, 0, []")
        
        return "\n".join(lines)

    def analyze_sheet(self, df, sheet_name):
        """
        åˆ†æå•ä¸ªSheetçš„ç»“æ„
        """
        # æ ‡å‡†åŒ–åˆ—å
        df.columns = [str(c).strip() for c in df.columns]
        columns = df.columns.tolist()
        
        # é¢„è§ˆæ•°æ® (å‰3è¡Œï¼Œè½¬dict)
        preview = df.head(3).where(pd.notnull(df), None).to_dict(orient='records')

        # 1. æ£€æµ‹æ˜¯å¦åŒ…å«æ—¶é—´åˆ— (00:00 - 23:45)
        time_cols = [c for c in columns if re.match(r"^\d{1,2}:\d{2}$", c) or re.match(r"^\d{1,2}:\d{2}:\d{2}$", c)]
        
        pattern_type = "unknown"
        
        if len(time_cols) > 5:
            pattern_type = "time_series_matrix"  # çŸ©é˜µå¼ï¼šåˆ—æ˜¯æ—¶é—´ï¼Œè¡Œæ˜¯æŒ‡æ ‡
        elif "æ—¥æœŸ" in columns and ("ç±»å‹" in columns or "é€šé“åç§°" in columns):
             pattern_type = "standard_list" # æ ‡å‡†åˆ—è¡¨ï¼šæœ‰æ—¥æœŸã€ç±»å‹ã€å€¼
        else:
            pattern_type = "generic_table" # é€šç”¨è¡¨æ ¼

        return {
            "name": sheet_name,
            "rows": len(df),
            "cols": len(columns),
            "columns": columns,
            "pattern_type": pattern_type,
            "time_cols": time_cols,
            "preview": preview
        }

    def generate_func_code(self, func_name, sheet_info):
        """
        æ ¹æ®åˆ†æç»“æœç”Ÿæˆå‡½æ•°ä»£ç 
        """
        pattern_type = sheet_info["pattern_type"]
        columns = sheet_info["columns"]
        
        lines = []
        lines.append(f"    def {func_name}(self, df, data_date, sheet_name, data_type):")
        lines.append(f"        \"\"\"è‡ªåŠ¨ç”Ÿæˆçš„å¤„ç†å‡½æ•°: {sheet_info['name']} (æ¨¡å¼: {pattern_type})\"\"\"")
        lines.append(f"        records = []")
        lines.append(f"        df = df.dropna(how='all')")
        lines.append(f"        df.columns = [str(c).strip() for c in df.columns]")
        lines.append(f"        ")

        if pattern_type == "time_series_matrix":
            # ç”ŸæˆçŸ©é˜µå¼å¤„ç†ä»£ç 
            lines.append(f"        # è¯†åˆ«æ—¶é—´åˆ—")
            lines.append(f"        time_cols = [c for c in df.columns if re.match(r'^\\d{{1,2}}:\\d{{2}}$', c)]")
            lines.append(f"        ")
            lines.append(f"        for _, row in df.iterrows():")
            
            # çŒœæµ‹ channel_name åˆ—
            candidate_name_cols = [c for c in columns if c not in sheet_info["time_cols"] and "æ—¥æœŸ" not in c]
            name_col = candidate_name_cols[0] if candidate_name_cols else "Unknown"
            
            lines.append(f"            # å‡è®¾ '{name_col}' åˆ—æ˜¯æŒ‡æ ‡åç§°")
            lines.append(f"            channel_name = str(row.get('{name_col}', 'Unknown')).strip()")
            lines.append(f"            ")
            lines.append(f"            for t in time_cols:")
            lines.append(f"                val = row[t]")
            lines.append(f"                if pd.isna(val): continue")
            lines.append(f"                ")
            lines.append(f"                records.append({{")
            lines.append(f"                    'record_date': data_date,")
            lines.append(f"                    'record_time': t,")
            lines.append(f"                    'channel_name': channel_name,")
            lines.append(f"                    'value': val,")
            lines.append(f"                    'sheet_name': sheet_name,")
            lines.append(f"                    'type': data_type,")
            lines.append(f"                    'created_at': datetime.datetime.now()")
            lines.append(f"                }})")

        elif pattern_type == "standard_list":
            # ç”Ÿæˆæ ‡å‡†åˆ—è¡¨å¤„ç†ä»£ç 
            lines.append(f"        # æ ‡å‡†åˆ—è¡¨å¤„ç†")
            lines.append(f"        for _, row in df.iterrows():")
            
            # æ™ºèƒ½æ˜ å°„
            col_date = "æ—¥æœŸ" if "æ—¥æœŸ" in columns else None
            col_type = "ç±»å‹" if "ç±»å‹" in columns else ("é€šé“åç§°" if "é€šé“åç§°" in columns else None)
            
            # å¯»æ‰¾æ•°å€¼åˆ— (æ’é™¤æ—¥æœŸå’Œç±»å‹)
            value_cols = [c for c in columns if c not in [col_date, col_type]]
            
            if col_date:
                lines.append(f"            # è§£ææ—¥æœŸ")
                lines.append(f"            r_date = pd.to_datetime(row['{col_date}']).date() if pd.notna(row['{col_date}']) else data_date")
            else:
                lines.append(f"            r_date = data_date")

            if col_type:
                 lines.append(f"            channel = str(row['{col_type}']).strip()")
            else:
                 lines.append(f"            channel = 'Default'")

            # éå†å‰©ä½™åˆ—ä½œä¸ºå€¼
            lines.append(f"            ")
            lines.append(f"            # éå†å¯èƒ½çš„æ•°å€¼åˆ—")
            lines.append(f"            value_cols = {value_cols}")
            lines.append(f"            for col in value_cols:")
            lines.append(f"                val = row[col]")
            lines.append(f"                if pd.isna(val): continue")
            lines.append(f"                ")
            lines.append(f"                # å¦‚æœæœ‰å¤šåˆ—æ•°å€¼ï¼Œå°†åˆ—åæ‹¼æ¥åˆ° channel_name")
            lines.append(f"                final_channel = f'{{channel}}-{{col}}' if len(value_cols) > 1 else channel")
            lines.append(f"                ")
            lines.append(f"                records.append({{")
            lines.append(f"                    'record_date': r_date,")
            lines.append(f"                    'record_time': None,")
            lines.append(f"                    'channel_name': final_channel,")
            lines.append(f"                    'value': val,")
            lines.append(f"                    'sheet_name': sheet_name,")
            lines.append(f"                    'type': data_type,")
            lines.append(f"                    'created_at': datetime.datetime.now()")
            lines.append(f"                }})")

        else:
            # é€šç”¨è¡¨æ ¼å¤„ç† (æ˜ å°„æ‰€æœ‰åˆ—)
            lines.append(f"        # é€šç”¨è¡¨æ ¼å¤„ç† (ç›´æ¥æ˜ å°„æ‰€æœ‰åˆ—)")
            lines.append(f"        for _, row in df.iterrows():")
            lines.append(f"            record = {{")
            lines.append(f"                'record_date': data_date,")
            lines.append(f"                'sheet_name': sheet_name,")
            lines.append(f"                'type': data_type,")
            lines.append(f"                'created_at': datetime.datetime.now()")
            lines.append(f"            }}")
            lines.append(f"            # åŠ¨æ€æ˜ å°„æ‰€æœ‰åˆ—")
            lines.append(f"            for col in df.columns:")
            lines.append(f"                val = row[col]")
            lines.append(f"                if pd.notna(val):")
            lines.append(f"                    record[col] = val")
            lines.append(f"            records.append(record)")

        lines.append(f"        return records")
        return "\n".join(lines)

    def clean_name(self, name):
        """æ¸…ç†åç§°ç”¨äºå‡½æ•°å"""
        # ç§»é™¤éå­—æ¯æ•°å­—å­—ç¬¦
        cleaned = re.sub(r'[^a-zA-Z0-9_]', '_', str(name))
        # ç§»é™¤é‡å¤çš„ä¸‹åˆ’çº¿
        cleaned = re.sub(r'_+', '_', cleaned)
        # ç§»é™¤é¦–å°¾ä¸‹åˆ’çº¿
        return cleaned.strip('_')

    def remove_date_from_sheetname(self, sheet_name):
        """ç§»é™¤Sheetåä¸­çš„æ—¥æœŸ (ä¾‹å¦‚ 'Info(2025-12-23)' -> 'Info')"""
        # ç§»é™¤ (YYYY-MM-DD) æˆ– (YYYY/MM/DD) æˆ– (YYYYMMDD)
        s = re.sub(r'\(\d{4}[-/]?\d{1,2}[-/]?\d{1,2}\)', '', str(sheet_name))
        # ç§»é™¤ YYYY-MM-DD (æ— æ‹¬å·)
        s = re.sub(r'\d{4}[-/]?\d{1,2}[-/]?\d{1,2}', '', s)
        return s.strip()
