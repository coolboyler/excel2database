import pandas as pd
import datetime
import re
from sqlalchemy import text
from database import DatabaseManager

class PowerDataImporter:
    def __init__(self):
        self.db_manager = DatabaseManager()
        pass

    # ===============================
    # ä¸»å…¥å£ï¼šå¯¼å…¥æ‰€æœ‰sheet
    # ===============================
    def import_power_data(self, excel_file):
        """è‡ªåŠ¨å¯¼å…¥Excelä¸­æ‰€æœ‰Sheetçš„æ•°æ®ï¼Œæ—¥æœŸè‡ªåŠ¨è¯†åˆ«"""
        sheet_dict = self.read_excel_data(excel_file)
        if not sheet_dict:
            return False, None, 0, []

        all_records = []
        table_name = None
        data_type = None

        for sheet_name, df in sheet_dict.items():
            # === è‡ªåŠ¨è¯†åˆ«æ—¥æœŸ ===
            match = re.search(r"\((\d{4}-\d{2}-\d{2})\)", sheet_name)
            data_date = datetime.datetime.strptime(match.group(1), "%Y-%m-%d").date()

            # === æ ¹æ®æ–‡ä»¶åè¯†åˆ«ç±»å‹ ===
            file_name = str(excel_file)
            
            chinese_match = re.search(r'([\u4e00-\u9fff]+)', file_name)
            if chinese_match:
                data_type = chinese_match.group(1)
                print(f"ğŸ“ æ–‡ä»¶ç±»å‹è¯†åˆ«: {data_type}")
            else:
                print(f"âš ï¸ æœªèƒ½åœ¨æ–‡ä»¶åä¸­æ‰¾åˆ°æ±‰å­—ï¼š{file_name}ï¼Œè·³è¿‡ã€‚")
                return False, None, 0, []

            print(f"\nğŸ“˜ æ­£åœ¨å¤„ç† {sheet_name} | æ—¥æœŸ: {data_date} | ç±»å‹: {data_type}")

            records = self.process_24h_data(df, data_date, sheet_name, data_type)
            all_records.extend(records)

        if not all_records:
            print("âŒ æ²¡æœ‰ä»»ä½•æœ‰æ•ˆæ•°æ®è¢«å¯¼å…¥")
            return False, None, 0, []

        # === ä¿å­˜æ•°æ®åº“ ===
        success, table_name, record_count, preview_data = self.save_to_database(all_records, data_date)
        return success, table_name, record_count, preview_data

    # ===============================
    # è¯»å–æ‰€æœ‰sheet
    # ===============================
    def read_excel_data(self, excel_file):
        """è¯»å–Excelä¸­æ‰€æœ‰Sheet"""
        try:
            sheet_dict = pd.read_excel(excel_file, sheet_name=None, header=0)
            print(f"âœ… æˆåŠŸè¯»å–Excelï¼Œå…± {len(sheet_dict)} ä¸ªSheet: {list(sheet_dict.keys())}")
            return sheet_dict
        except Exception as e:
            print(f"âŒ è¯»å–Excelå¤±è´¥: {e}")
            return None

    # ===============================
    # å¤„ç†å•ä¸ªsheetçš„24å°æ—¶æ•°æ®
    # ===============================
    def process_24h_data(self, df, data_date, sheet_name, data_type):
        """å¤„ç†å•ä¸ªSheetï¼ˆè¡Œå¼ç»“æ„ï¼‰çš„24å°æ—¶æ•°æ®"""
        records = []

        # æ ‡å‡†åŒ–åˆ—å
        df.columns = [str(c).strip() for c in df.columns]

        # æ£€æŸ¥æ•°æ®æ ¼å¼ï¼šæœ‰"é€šé“åç§°"åˆ—è¿˜æ˜¯æœ‰"ç±»å‹"åˆ—
        if "é€šé“åç§°" in df.columns:
            records = self._process_channel_format(df, data_date, sheet_name, data_type)
        elif "ç±»å‹" in df.columns:
            records = self._process_type_format(df, data_date, sheet_name, data_type)
        else:
            print(f"âš ï¸ æœªæ‰¾åˆ° 'é€šé“åç§°' æˆ– 'ç±»å‹' åˆ—ï¼Œè·³è¿‡ã€‚å¯ç”¨åˆ—: {list(df.columns)}")
            return records

        print(f"âœ… {data_type} å¯¼å…¥ {len(records)} æ¡è®°å½•")
        return records

    def _process_channel_format(self, df, data_date, sheet_name, data_type):
        """å¤„ç†æœ‰'é€šé“åç§°'åˆ—çš„æ•°æ®æ ¼å¼"""
        records = []

        # ç›´æ¥ä½¿ç”¨æ‰€æœ‰æœ‰é€šé“åç§°çš„è¡Œ
        valid_rows = df[df["é€šé“åç§°"].notna()]
        if valid_rows.empty:
            print(f"âš ï¸ Sheetä¸­æ— æœ‰æ•ˆé€šé“ï¼Œé€šé“åˆ—å€¼ä¸º: {df['é€šé“åç§°'].unique().tolist()}")
            return records

        # æå–æ‰€æœ‰æ—¶é—´åˆ—ï¼ˆä¸€èˆ¬ä»00:00åˆ°23:45ï¼‰
        time_cols = [c for c in df.columns if re.match(r"\d{2}:\d{2}", c)]
        if not time_cols:
            print(f"âš ï¸ æ²¡æœ‰å‘ç°æ—¶é—´åˆ—: {list(df.columns)}")
            return records

        # éå†æ¯ä¸€è¡Œï¼ˆä¸€ä¸ªé€šé“ï¼‰
        for _, row in valid_rows.iterrows():
            channel_name = row["é€šé“åç§°"]

            for t in time_cols:
                # å¤„ç†NaNå€¼ï¼Œè·³è¿‡NULLå€¼
                value = row[t]
                if pd.isna(value):
                    continue  # è·³è¿‡è¿™ä¸ªè®°å½•
                
                record = {
                    "record_date": data_date,
                    "record_time": t,
                    "channel_name": channel_name,
                    "value": value,
                    "type": data_type,
                    "sheet_name": sheet_name,
                    "created_at": datetime.datetime.now(),
                }
                records.append(record)

        return records

    def _process_type_format(self, df, data_date, sheet_name, data_type):
        """å¤„ç†æœ‰'ç±»å‹'åˆ—çš„æ•°æ®æ ¼å¼"""
        records = []

        # ç›´æ¥ä½¿ç”¨æ‰€æœ‰æœ‰ç±»å‹åç§°çš„è¡Œ
        valid_rows = df[df["ç±»å‹"].notna()]
        if valid_rows.empty:
            print(f"âš ï¸ Sheetä¸­æ— æœ‰æ•ˆç±»å‹ï¼Œç±»å‹åˆ—å€¼ä¸º: {df['ç±»å‹'].unique().tolist()}")
            return records

        # æå–æ‰€æœ‰æ—¶é—´åˆ—ï¼ˆä¸€èˆ¬ä»00:00åˆ°23:45ï¼‰
        time_cols = [c for c in df.columns if re.match(r"\d{2}:\d{2}", c)]
        if not time_cols:
            print(f"âš ï¸ æ²¡æœ‰å‘ç°æ—¶é—´åˆ—: {list(df.columns)}")
            return records

        # éå†æ¯ä¸€è¡Œï¼ˆä¸€ä¸ªç±»å‹ï¼‰
        for _, row in valid_rows.iterrows():
            channel_name = row["ç±»å‹"]  # å°†"ç±»å‹"åˆ—çš„å€¼ä½œä¸ºchannel_name

            for t in time_cols:
                # å¤„ç†NaNå€¼ï¼Œè·³è¿‡NULLå€¼
                value = row[t]
                if pd.isna(value):
                    continue  # è·³è¿‡è¿™ä¸ªè®°å½•
                
                record = {
                    "record_date": data_date,
                    "record_time": t,
                    "channel_name": channel_name,
                    "value": value,
                    "type": data_type,
                    "sheet_name": sheet_name,
                    "created_at": datetime.datetime.now(),
                }
                records.append(record)

        return records

    # -------------------------------
    # æ•°æ®ä¿å­˜
    # -------------------------------

    # def save_to_database(self, records, data_date):
    #     """æŒ‰æ—¥æœŸè‡ªåŠ¨åˆ›å»ºè¡¨å¹¶ä¿å­˜æ•°æ®"""
    #     if not records:
    #         print("âŒ æ²¡æœ‰å¯ä¿å­˜çš„è®°å½•")
    #         return False, None, 0, []

    #     # ğŸ§© 1. å¦‚æœä¼ å…¥çš„æ˜¯ DataFrameï¼Œè½¬æˆ list[dict]
    #     if isinstance(records, pd.DataFrame):
    #         records = records.to_dict# è·å–å‰5è¡Œæ•°æ®é¢„è§ˆ
    #     preview_stmt = text(f"SELECT * FROM {table_name} LIMIT 5")
    #     result = conn.execute(preview_stmt)
    #     # ä¿®å¤ï¼šæ­£ç¡®å¤„ç†SQLAlchemyè¡Œå¯¹è±¡
    #     preview_data = []
    #     for row in result:
    #         # å°†è¡Œå¯¹è±¡è½¬æ¢ä¸ºå­—å…¸
    #         preview_data.append(dict(zip(result.keys(), row)))(orient="records")
    def save_to_database(self, records, data_date):
        """æŒ‰æ—¥æœŸè‡ªåŠ¨åˆ›å»ºè¡¨å¹¶ä¿å­˜æ•°æ®"""
        if not records:
            print("âŒ æ²¡æœ‰å¯ä¿å­˜çš„è®°å½•")
            return False, None, 0, []

        # ğŸ§© 1. å¦‚æœä¼ å…¥çš„æ˜¯ DataFrameï¼Œè½¬æˆ list[dict]
        if isinstance(records, pd.DataFrame):
            records = records.to_dict(orient="records")

        if not isinstance(records, list):
            print(f"âŒ records ç±»å‹é”™è¯¯: {type(records)}ï¼Œåº”ä¸º list[dict]")
            return False, None, 0, []

        # ğŸ§© 2. è¿‡æ»¤æ— æ•ˆè®°å½•
        valid_records = []
        for i, r in enumerate(records):
            if not isinstance(r, dict):
                continue
            required_fields = ["record_date", "record_time", "channel_name", "value", "type", "sheet_name"]
            if not all(k in r for k in required_fields):
                continue
            # è½¬ record_date
            if isinstance(r["record_date"], str):
                r["record_date"] = pd.to_datetime(r["record_date"]).date()
            valid_records.append(r)

        if not valid_records:
            print("âŒ æ²¡æœ‰å¯ä¿å­˜çš„æœ‰æ•ˆè®°å½•")
            return False, None, 0, []

        # --- ç”ŸæˆæŒ‰å¤©è¡¨å ---
        table_name = f"power_data_{data_date.strftime('%Y%m%d')}"
        preview_data = []

        try:
            with self.db_manager.engine.begin() as conn:
                # --- åˆ›å»ºè¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰ ---
                create_table_sql = f"""
                CREATE TABLE IF NOT EXISTS {table_name} (
                    id BIGINT AUTO_INCREMENT PRIMARY KEY,
                    record_date DATE NOT NULL,
                    record_time TIME,
                    type VARCHAR(255),
                    channel_name VARCHAR(255),
                    value DECIMAL(10,2),
                    sheet_name VARCHAR(255)
                );
                """
                conn.execute(text(create_table_sql))
                print(f"âœ… è¡¨ {table_name} å·²å­˜åœ¨æˆ–åˆ›å»ºæˆåŠŸ")

                # --- æ‰¹é‡æ’å…¥ ---
                insert_stmt = text(f"""
                INSERT INTO {table_name} 
                (record_date, record_time, type, channel_name, value, sheet_name)
                VALUES (:record_date, :record_time, :type, :channel_name, :value, :sheet_name)
                """)

                batch_size = 200
                for i in range(0, len(valid_records), batch_size):
                    batch = valid_records[i:i + batch_size]
                    conn.execute(insert_stmt, batch)
                    print(f"ğŸ’¾ å·²æ’å…¥ç¬¬ {i // batch_size + 1} æ‰¹æ•°æ® ({len(batch)} æ¡)")

                count_stmt = text(f"SELECT COUNT(*) FROM {table_name}")
                count = conn.execute(count_stmt).scalar()
                
                # è·å–å‰5è¡Œæ•°æ®é¢„è§ˆ
                preview_stmt = text(f"SELECT * FROM {table_name} ORDER BY id DESC LIMIT 5")
                result = conn.execute(preview_stmt)
                # ä¿®å¤ï¼šæ­£ç¡®å¤„ç†SQLAlchemyè¡Œå¯¹è±¡
                preview_data = []
                for row in result:
                    # å°†è¡Œå¯¹è±¡è½¬æ¢ä¸ºå­—å…¸
                    preview_data.append(dict(zip(result.keys(), row)))
                
                print(f"âœ… æ•°æ®åº“ä¿å­˜æˆåŠŸ: {count} æ¡è®°å½•")
                return True, table_name, count, preview_data

        except Exception as e:
            print(f"âŒ æ•°æ®åº“ä¿å­˜å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False, None, 0, []

    def import_custom_excel(self, excel_file):
        """å¯¼å…¥æŒ‡å®šçš„5ä¸ªsheetï¼Œå¹¶æŒ‰å›ºå®šè§„åˆ™æ˜ å°„"""
        try:
            # è¯»å–æ‰€æœ‰sheet
            sheet_dict = pd.read_excel(excel_file, sheet_name=None, header=None)
        except Exception as e:
            print(f"âŒ æ— æ³•è¯»å–Excel: {e}")
            return False
        file_name = str(excel_file)
        
        chinese_match = re.search(r'([\u4e00-\u9fff]+)', file_name)
        if chinese_match:
            data_type = chinese_match.group(1) + "å®é™…ä¿¡æ¯"
            print(f"ğŸ“ æ–‡ä»¶ç±»å‹è¯†åˆ«: {data_type}")
        else:
            print(f"âš ï¸ æœªèƒ½åœ¨æ–‡ä»¶åä¸­æ‰¾åˆ°æ±‰å­—ï¼š{file_name}ï¼Œè·³è¿‡ã€‚")
            return False
        sheet_names = list(sheet_dict.keys())
        print(f"ğŸ“˜ æ£€æµ‹åˆ° {len(sheet_names)} ä¸ªSheet: {sheet_names}")

        # è¦å¤„ç†çš„sheetç¼–å·ï¼ˆ1-basedï¼‰
        target_indexes = [0, 1, 3, 4, 5,6,-2,-1]  # å¯¹åº”ç¬¬1,2,4,5,6ä¸ªsheet

        all_records = []

        for i in target_indexes:
            if i >= len(sheet_names):
                print(f"âš ï¸ Excelä¸­ä¸å­˜åœ¨ç¬¬{i+1}ä¸ªsheetï¼Œè·³è¿‡")
                continue

            sheet_name = sheet_names[i]
            df = sheet_dict[sheet_name]
            print(f"\nğŸ”¹ æ­£åœ¨å¤„ç† Sheet {i+1}: {sheet_name}")

            # ç»Ÿä¸€è¯†åˆ«æ—¥æœŸ
            match = re.search(r"\((\d{4}-\d{2}-\d{2})\)", sheet_name)
            data_date = datetime.datetime.strptime(match.group(1), "%Y-%m-%d").date()
            # æ ¹æ®sheetåºå·è°ƒç”¨ä¸åŒæ˜ å°„å‡½æ•°
            if i in [0, 3, 4]:  # ç¬¬1,4,5ä¸ªsheetï¼šæ—¶åˆ»â†’channel_name
                records = self._process_time_as_channel(df, data_date, sheet_name, data_type)
            elif i in [1, 5]:  # ç¬¬2,6ä¸ªsheetï¼šç¬¬ä¸€è¡Œâ†’channel_name
                records = self._process_first_row_as_channel(df, data_date, sheet_name, data_type)
            elif i in [6]:
                records = self._process_fsc_as_channel(df, data_date, sheet_name, data_type)
            # elif i in [-2]:
            #     records = self._process_new_as_table(df, data_date, sheet_name, data_type)
            else:
                print(f"âš ï¸ ç¬¬{i+1}ä¸ªsheetæœªå®šä¹‰å¤„ç†è§„åˆ™ï¼Œè·³è¿‡")
                continue

            print(f"âœ… Sheet{i+1} å¤„ç†å®Œæˆï¼Œå…± {len(records)} æ¡è®°å½•")
            all_records.extend(records)

        if not all_records:
            print("âŒ æ²¡æœ‰ç”Ÿæˆä»»ä½•æœ‰æ•ˆè®°å½•")
            return False

        return self.save_to_database(all_records, data_date)

    def _process_time_as_channel(self, df, data_date, sheet_name, data_type):
        """å°†æ—¶åˆ»åˆ—åæ˜ å°„ä¸ºchannel_name"""
        records = []
        df = df.dropna(how="all")  # åˆ é™¤ç©ºè¡Œ

        # å¦‚æœç¬¬ä¸€åˆ—æ˜¯ â€œæ—¶åˆ»â€ å­—æ ·
        if str(df.iloc[0, 0]).strip() == "æ—¶åˆ»":
            df.columns = [str(c).strip() for c in df.iloc[0]]  # ç¬¬ä¸€è¡Œä½œåˆ—å
            df = df[1:]  # å»æ‰æ ‡é¢˜è¡Œ
        else:
            df.columns = [str(c).strip() for c in df.columns]

        # æŸ¥æ‰¾æ—¶é—´åˆ—ï¼ˆå½¢å¦‚ 00:00ã€01:15ï¼‰
        time_cols = [c for c in df.columns if re.match(r"\d{2}:\d{2}", c)]
        if not time_cols:
            print(f"âš ï¸ æœªæ‰¾åˆ°æ—¶é—´åˆ—: {df.columns.tolist()}")
            return []
        # éå†æ¯ä¸€è¡Œï¼ˆæ¯ä¸€ç±»æŒ‡æ ‡ï¼‰
        for _, row in df.iterrows():
            # è·³è¿‡æ— æ•ˆè¡Œæˆ–æ ‡é¢˜è¡Œ
            if not isinstance(row[time_cols[0]], (int, float)) and not str(row[time_cols[0]]).replace('.', '', 1).isdigit():
                continue

            # æŒ‡æ ‡åï¼ˆæ¯”å¦‚ â€œç»Ÿè°ƒè´Ÿè·(MW)â€ï¼‰
            indicator_name = str(row.get("æ—¶åˆ»") or row.index[0]).strip()

            for t in time_cols:
                value = row[t]
                if pd.isna(value):
                    continue
                try:
                    value = float(value)
                except:
                    continue  # è·³è¿‡éæ•°å€¼çš„å•å…ƒæ ¼
                record = {
                    "record_date": data_date,
                    "record_time": t,
                    "channel_name": indicator_name,  # ç”¨æŒ‡æ ‡åä½œé€šé“å
                    "value": value,
                    "type": data_type,
                    "sheet_name": sheet_name,
                    "created_at": datetime.datetime.now(),
                }
                records.append(record)
        return records

    def _process_fsc_as_channel(self, df, data_date, sheet_name, data_type):
        """å°†æ—¶åˆ»åˆ—åæ˜ å°„ä¸ºchannel_name"""
        records = []
        df = df.dropna(how="all")  # åˆ é™¤ç©ºè¡Œ
        df.columns = [str(c).strip() for c in df.iloc[0]]  # ç¬¬ä¸€è¡Œä½œåˆ—å
        df = df[1:]  # å»æ‰æ ‡é¢˜è¡Œ
        
        first_col = df.columns[0]
        second_col = df.columns[1]
       
        # æŸ¥æ‰¾æ—¶é—´åˆ—ï¼ˆå½¢å¦‚ 00:00ã€01:15 æˆ–æ•°å­—æ ¼å¼ 0, 1, 2...ï¼‰
        time_cols = [c for c in df.columns if re.match(r"\d{2}:\d{2}", c)]
        if not time_cols:
            print(f"âš ï¸ æœªæ‰¾åˆ°æ—¶é—´åˆ—: {df.columns.tolist()}")
            return []

        # éå†æ¯ä¸€è¡Œï¼ˆæ¯ä¸€ç±»æŒ‡æ ‡ï¼‰
        for _, row in df.iterrows():
            # è·³è¿‡æ— æ•ˆè¡Œæˆ–æ ‡é¢˜è¡Œ
            if not isinstance(row[time_cols[0]], (int, float)) and not str(row[time_cols[0]]).replace('.', '', 1).isdigit():
                continue
            
            # ç”Ÿæˆ channel_nameï¼šç¬¬ä¸€åˆ—å’Œç¬¬äºŒåˆ—ç”¨ä¸‹åˆ’çº¿è¿æ¥
            channel_name = f"{row[first_col]}_{row[second_col]}"

            for t in time_cols:
                value = row[t]
                if pd.isna(value):
                    continue
                try:
                    value = float(value)
                except:
                    continue  # è·³è¿‡éæ•°å€¼çš„å•å…ƒæ ¼

                record = {
                    "record_date": data_date,
                    "record_time": t,
                    "channel_name": channel_name,  # ç”¨æŒ‡æ ‡åä½œé€šé“å
                    "value": value,
                    "type": data_type,
                    "sheet_name": sheet_name,
                    "created_at": datetime.datetime.now(),
                }
                records.append(record)
        return records
    def _process_first_row_as_channel(self, df, data_date, sheet_name, data_type):
        """
        å¤„ç†æ ¼å¼ï¼š
        æœ€é«˜è´Ÿè·(MW) | æœ€ä½è´Ÿè·(MW) | å¹³å‡è´Ÿè·(MW)
        243330.375    | 182924.0156  | 212967.9509
        """
        records = []
        # åˆ é™¤ç©ºè¡Œä¸ç©ºåˆ—
        df = df.dropna(how="all").dropna(axis=1, how="all")
        if df.empty:
            print(f"âš ï¸ Sheet {sheet_name} ä¸ºç©ºï¼Œè·³è¿‡ã€‚")
            return records

        # ç¬¬ä¸€è¡Œä½œä¸º channel_name
        channel_names = [str(c).strip() for c in df.iloc[0].tolist()]
        df = df.iloc[1:]  # ä»ç¬¬äºŒè¡Œå¼€å§‹ä¸ºæ•°æ®
        if df.empty:
            print(f"âš ï¸ Sheet {sheet_name} ä»…æœ‰è¡¨å¤´ï¼Œæ— æ•°æ®ã€‚")
            return records

        for _, row in df.iterrows():
            for col_idx, value in enumerate(row):
                if col_idx >= len(channel_names):
                    continue
                if pd.isna(value):
                    continue
                record = {
                    "record_date": data_date,
                    "record_time": None,  # æ²¡æœ‰æ—¶é—´åˆ—
                    "channel_name": channel_names[col_idx],
                    "value": value,
                    "type": data_type,
                    "sheet_name": sheet_name,
                    "created_at": datetime.datetime.now(),
                }
                records.append(record)

        print(f"âœ… Sheet {sheet_name} è§£æå®Œæˆï¼Œå…± {len(records)} æ¡è®°å½•ã€‚")
        return records

    def import_custom_excel_pred(self, excel_file):
            """å¯¼å…¥æŒ‡å®šçš„5ä¸ªsheetï¼Œå¹¶æŒ‰å›ºå®šè§„åˆ™æ˜ å°„"""
            try:
                # è¯»å–æ‰€æœ‰sheet
                sheet_dict = pd.read_excel(excel_file, sheet_name=None, header=0)
            except Exception as e:
                print(f"âŒ æ— æ³•è¯»å–Excel: {e}")
                return False
            file_name = str(excel_file)
            
            chinese_match = re.search(r'([\u4e00-\u9fff]+)', file_name)
            if chinese_match:
                data_type = chinese_match.group(1) + "é¢„æµ‹ä¿¡æ¯"
                print(f"ğŸ“ æ–‡ä»¶ç±»å‹è¯†åˆ«: {data_type}")
            else:
                print(f"âš ï¸ æœªèƒ½åœ¨æ–‡ä»¶åä¸­æ‰¾åˆ°æ±‰å­—ï¼š{file_name}ï¼Œè·³è¿‡ã€‚")
                return False
            sheet_names = list(sheet_dict.keys())
            print(f"ğŸ“˜ æ£€æµ‹åˆ° {len(sheet_names)} ä¸ªSheet: {sheet_names}")

            # è¦å¤„ç†çš„sheetç¼–å·ï¼ˆ1-basedï¼‰
            target_indexes = [0, 1, 2, -3, -2, -1]  # å¯¹åº”ç¬¬1,2,4,5,6ä¸ªsheet

            all_records = []

            for i in target_indexes:
                if i >= len(sheet_names):
                    print(f"âš ï¸ Excelä¸­ä¸å­˜åœ¨ç¬¬{i+1}ä¸ªsheetï¼Œè·³è¿‡")
                    continue

                sheet_name = sheet_names[i]
                df = sheet_dict[sheet_name]
                print(f"\nğŸ”¹ æ­£åœ¨å¤„ç† Sheet {i+1}: {sheet_name}")

                # ç»Ÿä¸€è¯†åˆ«æ—¥æœŸ
                match = re.search(r"\((\d{4}-\d{2}-\d{2})\)", sheet_name)
                data_date = datetime.datetime.strptime(match.group(1), "%Y-%m-%d").date()

                # æ ¹æ®sheetåºå·è°ƒç”¨ä¸åŒæ˜ å°„å‡½æ•°
                if i in [0]:  # ç¬¬1ä¸ªsheetï¼šæ—¶åˆ»â†’channel_name
                    records = self._process_time_as_channel(df, data_date, sheet_name, data_type)
                elif i in [1]: 
                    records = self._process_1_channel(df, data_date, sheet_name, data_type)
                elif i in [2]:  # ç¬¬3ä¸ªsheetï¼šæ—¶åˆ»â†’channel_name
                    records = self._process_type_date_value(df, data_date, sheet_name, data_type)
                elif i in [-3]:  # ç¬¬4,5ä¸ªsheetï¼šæ—¶åˆ»â†’channel_name
                    records = self._process_3_channel(df, data_date, sheet_name, data_type)
                elif i in [-2, -1]:  # ç¬¬7,8ä¸ªsheetï¼šç¬¬ä¸€è¡Œâ†’channel_name
                    records = self._process_2_channel(df, data_date, sheet_name, data_type)
                else:
                    print(f"âš ï¸ ç¬¬{i+1}ä¸ªsheetæœªå®šä¹‰å¤„ç†è§„åˆ™ï¼Œè·³è¿‡")
                    continue

                print(f"âœ… Sheet{i+1} å¤„ç†å®Œæˆï¼Œå…± {len(records)} æ¡è®°å½•")
                all_records.extend(records)

            if not all_records:
                print("âŒ æ²¡æœ‰ç”Ÿæˆä»»ä½•æœ‰æ•ˆè®°å½•")
                return False

            return self.save_to_database(all_records, data_date)
        
    def _process_1_channel(self, df, data_date, sheet_name, data_type):
        """
        å¤šæŒ‡æ ‡æ—¶åˆ»å‹sheetå¤„ç†ï¼š
        - è¯†åˆ«â€œç±»å‹ + ç”µæºç±»å‹â€ä¸º channel_name
        - ä½¿ç”¨â€œæ—¥æœŸâ€åˆ—ä½œä¸º record_date
        - æ—¶é—´åˆ—ä¸º 00:00ã€00:15 ç­‰å¸¸è§„æ ¼å¼
        """
        import datetime
        import pandas as pd
        import re

        records = []
        df = df.dropna(how="all")
        df.columns = [str(c).strip() for c in df.columns]

        # 1ï¸âƒ£ æ‰¾æ—¶é—´åˆ—ï¼ˆå¦‚ 00:00ã€01:15 ç­‰ï¼‰
        time_cols = [c for c in df.columns if re.match(r"^\d{1,2}:\d{2}$", c)]
        if not time_cols:
            print(f"âš ï¸ æœªæ‰¾åˆ°æ—¶é—´åˆ—: {df.columns.tolist()}")
            return []

        # 2ï¸âƒ£ è¯†åˆ«è¾…åŠ©åˆ—
        col_type = "ç±»å‹" if "ç±»å‹" in df.columns else None
        col_date = "æ—¥æœŸ" if "æ—¥æœŸ" in df.columns else None
        col_power = "ç”µæºç±»å‹" if "ç”µæºç±»å‹" in df.columns else None

        # 3ï¸âƒ£ éå†æ¯ä¸€è¡Œï¼ˆæ¯ä¸ªé€šé“ï¼‰
        for _, row in df.iterrows():
            # --- æ—¥æœŸåˆ— ---
            record_date = data_date
            if col_date and pd.notna(row[col_date]):
                try:
                    # è‡ªåŠ¨è¯†åˆ«æ—¥æœŸæ ¼å¼
                    record_date = pd.to_datetime(str(row[col_date]), errors="coerce").date()
                except:
                    record_date = data_date

            # --- é€šé“åï¼šç±»å‹ + ç”µæºç±»å‹ ---
            parts = []
            if col_type and pd.notna(row[col_type]):
                parts.append(str(row[col_type]).strip())
            if col_power and pd.notna(row[col_power]):
                parts.append(str(row[col_power]).strip())
            if not parts:
                continue
            channel_name = "-".join(parts)

            # --- éå†æ—¶é—´åˆ— ---
            for t in time_cols:
                value = row[t]
                if pd.isna(value):
                    continue
                try:
                    value = float(value)
                except:
                    continue

                records.append({
                    "record_date": record_date,        # ç¡®ä¿æ˜¯ date ç±»å‹
                    "record_time": t,                  # å¦‚ 00:00
                    "channel_name": channel_name,      # å¦‚ "ç°è´§æ–°èƒ½æºæ€»å‡ºåŠ›(MW)-é£ç”µ"
                    "value": value,
                    "type": data_type,
                    "sheet_name": sheet_name,
                    "created_at": datetime.datetime.now(),
                })

        print(f"âœ… {sheet_name} è§£æå®Œæˆï¼Œå…± {len(records)} æ¡è®°å½•")
        return records

    def _process_3_channel(self, df, data_date, sheet_name, data_type):
        """
        å°†å¤šåˆ—é€šé“å‹Sheetå¤„ç†æˆè®°å½•åˆ—è¡¨ï¼Œæ¯åˆ—è§†ä¸ºä¸€ä¸ªé€šé“ã€‚
        ç»“æ„ç¤ºä¾‹ï¼š
        åºå· | æ—¥æœŸ | å¿…å¼€æœºç»„å®¹é‡(MW) | å¿…åœæœºç»„å®¹é‡(MW)
        """
        import datetime
        import pandas as pd

        print(f"ğŸ”¹ æ­£åœ¨å¤„ç† Sheet: {sheet_name}")

        records = []

        # 1ï¸âƒ£ åˆ é™¤æ— ç”¨åˆ—
        if "åºå·" in df.columns:
            df = df.drop(columns=["åºå·"])

        # 2ï¸âƒ£ ç¡®ä¿æ—¥æœŸå­—æ®µå­˜åœ¨
        if "æ—¥æœŸ" not in df.columns:
            print(f"âš ï¸ æœªæ‰¾åˆ°æ—¥æœŸåˆ—ï¼Œè·³è¿‡ {sheet_name}")
            return []

        df = df.dropna(how="all")
        df.columns = [str(c).strip() for c in df.columns]

        # 3ï¸âƒ£ éå†æ¯ä¸€è¡Œ
        for _, row in df.iterrows():
            # æ—¥æœŸ
            record_date = data_date
            if pd.notna(row["æ—¥æœŸ"]):
                try:
                    record_date = pd.to_datetime(str(row["æ—¥æœŸ"]), errors="coerce").date()
                except:
                    record_date = data_date

            # 4ï¸âƒ£ éå†é€šé“åˆ—ï¼ˆé™¤â€œæ—¥æœŸâ€å¤–ï¼‰
            for col in df.columns:
                if col in ["æ—¥æœŸ"]:
                    continue
                value = row[col]
                if pd.isna(value):
                    continue
                try:
                    value = float(value)
                except:
                    continue

                records.append({
                    "record_date": record_date,
                    "record_time": None,
                    "channel_name": col,
                    "value": value,
                    "type": data_type,
                    "sheet_name": sheet_name,
                    "created_at": datetime.datetime.now(),
                })

        print(f"âœ… {sheet_name} å¤„ç†å®Œæˆï¼Œå…± {len(records)} æ¡è®°å½•")
        return records
    
    def _process_2_channel(self, df, data_date, sheet_name, data_type):
        """
        å¤„ç†æœºç»„åå•è¡¨ï¼š
        - channel_name = ç”µå‚åç§°-æœºç»„åç§°-ç±»å‹
        - value é»˜è®¤ä¸º 1
        """
        import datetime
        import pandas as pd

        records = []

        if df.empty:
            print(f"âš ï¸ {sheet_name} è¡¨ä¸ºç©ºï¼Œè·³è¿‡")
            return []

        df = df.dropna(how="all")
        df.columns = [str(c).strip() for c in df.columns]

        # å¿…è¦åˆ—
        col_date = "æ—¥æœŸ" if "æ—¥æœŸ" in df.columns else None
        col_plant = "ç”µå‚åç§°" if "ç”µå‚åç§°" in df.columns else None
        col_unit = "æœºç»„åç§°" if "æœºç»„åç§°" in df.columns else None
        col_type = "ç±»å‹" if "ç±»å‹" in df.columns else None

        for _, row in df.iterrows():
            # æ—¥æœŸ
            record_date = data_date
            if col_date and pd.notna(row[col_date]):
                try:
                    record_date = pd.to_datetime(str(row[col_date]), errors="coerce").date()
                except:
                    record_date = data_date

            # channel_name æ‹¼æ¥
            parts = []
            for col in [col_plant, col_unit, col_type]:
                if col and pd.notna(row[col]):
                    parts.append(str(row[col]).strip())
            if not parts:
                continue
            channel_name = "-".join(parts)

            # æ·»åŠ è®°å½•
            records.append({
                "record_date": record_date,
                "channel_name": channel_name,
                "record_time": None,
                "value": None,
                "type": data_type,
                "sheet_name": sheet_name,
                "created_at": datetime.datetime.now(),
            })

        print(f"âœ… {sheet_name} å¤„ç†å®Œæˆï¼Œå…± {len(records)} æ¡è®°å½•")
        return records

    def _process_type_date_value(self, df, data_date, sheet_name, data_type):
        """å¤„ç†ç±»ä¼¼ 'ç±»å‹ æ—¥æœŸ æ•°å€¼' çš„ç»“æ„ï¼ˆæ— æ—¶é—´åˆ—ï¼Œrecord_dateä¸ºdateç±»å‹ï¼‰"""
        records = []
        df = df.dropna(how="all")
        df.columns = [str(c).strip() for c in df.columns]

        # æŸ¥æ‰¾åˆ—
        col_type = "ç±»å‹" if "ç±»å‹" in df.columns else None
        col_date = "æ—¥æœŸ" if "æ—¥æœŸ" in df.columns else None

        # æŸ¥æ‰¾æ•°å€¼åˆ—ï¼ˆæ’é™¤æ‰å·²çŸ¥åˆ—ï¼‰
        value_cols = [c for c in df.columns if c not in [col_type, col_date]]
        if not value_cols:
            print(f"âš ï¸ æœªæ‰¾åˆ°æ•°å€¼åˆ—: {df.columns.tolist()}")
            return []

        value_col = value_cols[0]  # é»˜è®¤åªå–ç¬¬ä¸€åˆ—æ•°å€¼

        for _, row in df.iterrows():
            channel_name = str(row[col_type]).strip() if col_type else "æœªçŸ¥ç±»å‹"
            raw_date = str(row[col_date]).strip() if col_date and pd.notna(row[col_date]) else None

            # === æ—¥æœŸè§£æé€»è¾‘ ===
            parsed_date = None
            if raw_date:
                # 1. å¦‚æœæ˜¯æ ‡å‡†æ—¥æœŸæ ¼å¼
                try:
                    parsed_date = pd.to_datetime(raw_date).date()
                except Exception:
                    pass

                # 2. å¦‚æœæ˜¯å½¢å¦‚ â€œ2025å¹´ç¬¬38å‘¨(09.15~09.21)â€
                if parsed_date is None:
                    match = re.search(r"\((\d{2})\.(\d{2})", raw_date)
                    year_match = re.search(r"(\d{4})å¹´", raw_date)
                    if match and year_match:
                        year = int(year_match.group(1))
                        month = int(match.group(1))
                        day = int(match.group(2))
                        parsed_date = datetime.date(year, month, day)

            # å¦‚æœéƒ½è§£æå¤±è´¥ï¼Œåˆ™ç”¨ data_date å…œåº•
            if parsed_date is None:
                parsed_date = pd.to_datetime(data_date).date()

            # æ•°å€¼
            try:
                value = float(row[value_col])
            except:
                continue

            record = {
                "record_date": parsed_date,
                "record_time": datetime.datetime.now().time(),
                "channel_name": channel_name,
                "value": value,
                "type": data_type,
                "sheet_name": sheet_name,
                "created_at": datetime.datetime.now(),
            }
            records.append(record)

        return records
    
    def import_point_data(self, excel_file):
        """è‡ªåŠ¨å¯¼å…¥Excelç¬¬ä¸€ä¸ªSheetçš„æ•°æ®ï¼Œå¹¶æŒ‰åˆ—æ±‚å‡å€¼"""
        import re
        import datetime
        import pandas as pd

        try:
            xls = pd.ExcelFile(excel_file)
            first_sheet_name = xls.sheet_names[0]  # âœ… è·å–ç¬¬ä¸€ä¸ª sheet å
            df = pd.read_excel(excel_file, sheet_name=first_sheet_name, header=0)
            print(f"âœ… æˆåŠŸè¯»å– Excel: {excel_file}, sheet: {first_sheet_name}")
        except Exception as e:
            print(f"âŒ è¯»å– Excel å¤±è´¥: {e}")
            return False, None, 0, []

        # è‡ªåŠ¨è¯†åˆ«æ—¥æœŸ
        match = re.search(r"\((\d{4}-\d{2}-\d{2})\)", first_sheet_name)
        data_date = datetime.datetime.strptime(match.group(1), "%Y-%m-%d").date()

        # æ ¹æ®æ–‡ä»¶åè¯†åˆ«ç±»å‹
        file_name = str(excel_file)
        chinese_match = re.search(r'([\u4e00-\u9fff]+)', file_name)
        if chinese_match:
            data_type = chinese_match.group(1)
            print(f"ğŸ“ æ–‡ä»¶ç±»å‹è¯†åˆ«: {data_type}")
        else:
            print(f"âš ï¸ æœªèƒ½åœ¨æ–‡ä»¶åä¸­æ‰¾åˆ°æ±‰å­—ï¼š{file_name}ï¼Œè·³è¿‡ã€‚")
            return False, None, 0, []

        print(f"\nğŸ“˜ æ­£åœ¨å¤„ç† {first_sheet_name} | æ—¥æœŸ: {data_date} | ç±»å‹: {data_type}")

        # æŒ‰åˆ—æ±‚å‡å€¼å¹¶ç”Ÿæˆ records
        records = self.process_mean_by_column(df, data_date, first_sheet_name, data_type)

        if not records:
            print("âŒ æ²¡æœ‰ä»»ä½•æœ‰æ•ˆæ•°æ®è¢«å¯¼å…¥")
            return False, None, 0, []

        # ä¿å­˜åˆ°æ•°æ®åº“
        success, table_name, record_count, preview_data = self.save_to_database(records, data_date)
        print(f"âœ… æ•°æ®ä¿å­˜æˆåŠŸï¼Œè¡¨å: {table_name}ï¼Œè®°å½•æ•°: {record_count}")
        return success, table_name, record_count, preview_data

    def process_mean_by_column(self, df, data_date, sheet_name, data_type):
        """
        é’ˆå¯¹èŠ‚ç‚¹ç”µä»·ç­‰è¡¨æ ¼ï¼šå¯¹æ¯ä¸€åˆ—ï¼ˆä»ç¬¬3åˆ—å¼€å§‹ï¼‰æ±‚å‡å€¼ï¼Œå¹¶ç”Ÿæˆè®°å½•
        æ¯ä¸€åˆ—ã®å‡å€¤ãƒ‡ãƒ¼ã‚¿æ”¾åœ¨æœ€å¾Œï¼Œå…¶ä»–ãƒ‡ãƒ¼ã‚¿æŒ‰é †åºéƒ½å­˜ä¸€ä¸‹
        """
        records = []

        # æ ‡å‡†åŒ–åˆ—å
        df.columns = [str(c).strip() for c in df.columns]
        # print(f"COLUMNS: {df.columns.tolist()}")

        # è·å–æ—¶é—´åˆ—ï¼ˆç¬¬3åˆ—åŠä¹‹åï¼‰
        time_cols = df.columns[2:]
        if time_cols.empty or len(time_cols) == 0:
            print(f"âš ï¸ Sheet {sheet_name} æ²¡æœ‰å‘ç°æ—¶é—´åˆ—")
            return records

        # å°†æ—¶é—´åˆ—æŒ‰æ¯4ä¸ªåˆ†ç»„ï¼ˆæ¯å°æ—¶4ä¸ª15åˆ†é’Ÿé—´éš”ï¼‰
        time_groups = {}
        for t in time_cols:
            # ä» "HH:MM" æ ¼å¼ä¸­æå–å°æ—¶
            hour = t.split(':')[0]
            if hour not in time_groups:
                time_groups[hour] = []
            time_groups[hour].append(t)

        # å…ˆä¿å­˜åŸæœ‰çš„æ•°æ®ï¼ˆæŒ‰å°æ—¶åˆ†ç»„ï¼‰
        # é¢„å…ˆè®¡ç®—æ¯è¡Œæ¯å°æ—¶çš„å‡å€¼
        hourly_means = {}  # {(row_index, hour): mean_value}
        
        for _, row in df.iterrows():
            # æ£€æŸ¥ç¬¬ä¸€åˆ—æ˜¯å¦æœ‰æœ‰æ•ˆæ•°æ®ï¼Œå¦‚æœæ²¡æœ‰åˆ™è·³è¿‡ï¼ˆå¤„ç†æ ‡é¢˜è¡Œï¼‰
            channel_name = row.iloc[0]  # ç¬¬ä¸€åˆ—ä½œä¸ºé€šé“åç§°
            if pd.isna(channel_name) or channel_name == "":
                continue
                
            # ä¸ºæ¯è¡Œæ¯å°æ—¶è®¡ç®—å‡å€¼
            for hour, times in time_groups.items():
                # è®¡ç®—è¯¥å°æ—¶å†…å››ä¸ªæ—¶é—´ç‚¹çš„å‡å€¼
                values = []
                for t in times:
                    value = row[t]
                    if not pd.isna(value):
                        values.append(value)
                
                # å¦‚æœæœ‰æœ‰æ•ˆå€¼ï¼Œåˆ™è®¡ç®—å‡å€¼
                if values:
                    hourly_mean = sum(values) / len(values)
                    hourly_means[(_, hour)] = hourly_mean
                    
                    record = {
                        "record_date": pd.to_datetime(data_date).date(),
                        "record_time": f"{hour}:00",  # æŒ‰å°æ—¶å­˜å‚¨
                        "channel_name": channel_name,
                        "value": round(hourly_mean, 2),  # ä½¿ç”¨è¯¥å°æ—¶å†…å››ä¸ªæ—¶é—´ç‚¹çš„å‡å€¤
                        "type": data_type,
                        "sheet_name": sheet_name,
                        "created_at": pd.Timestamp.now(),
                    }
                    records.append(record)

        # å†æ·»åŠ æ¯å°æ—¶çš„å‡å€¤ãƒ‡ãƒ¼ã‚¿ï¼ˆæ‰€æœ‰è¡Œåœ¨è¯¥å°æ—¶çš„å‡å€¤ï¼‰
        for hour, times in time_groups.items():
            # è·å–è¿™äº›æ—¶é—´ç‚¹çš„å€¤å¹¶è¨ˆç®—å‡å€¤
            values = []
            for t in times:
                # è¨ˆç®—è©²æ™‚é–“ç‚¹åœ¨æ‰€æœ‰è¡Œä¸­çš„å‡å€¤
                mean_value = df[t].mean()
                values.append(mean_value)
            
            # è¨ˆç®—4ã¤ã®æ™‚é–“ç‚¹ã®ç·å‡å€¤
            if values:
                overall_mean = sum(values) / len(values)
                record = {
                    "record_date": pd.to_datetime(data_date).date(),
                    "record_time": f"{hour}:00",   # "HH:00" ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
                    "channel_name": f"{data_type}_å‡å€¼",
                    "value": round(overall_mean, 2),
                    "type": data_type,
                    "sheet_name": sheet_name,
                    "created_at": pd.Timestamp.now(),
                }
                records.append(record)

        print(f"âœ… {data_type} å‡å€¤ç”Ÿæˆ {len(records)} æ¡è¨˜éŒ„")
        return records

    def query_daily_averages(self, date_list, data_type_keyword="æ—¥å‰èŠ‚ç‚¹ç”µä»·"):
        """
        æŸ¥è¯¢å¤šå¤©çš„å‡å€¼æ•°æ®ï¼ˆé€‚ç”¨äºå·²è®¡ç®—å¥½çš„å‡å€¼è®°å½•ï¼‰
        
        Args:
            date_list (list): æ—¥æœŸåˆ—è¡¨ï¼Œæ ¼å¼ä¸º "YYYY-MM-DD"
            data_type_keyword (str): æ•°æ®ç±»å‹å…³é”®å­—ï¼Œç”¨äºç­›é€‰ç‰¹å®šç±»å‹çš„æ•°æ®
            
        Returns:
            dict: åŒ…å«æŸ¥è¯¢ç»“æœçš„å­—å…¸
        """
        try:
            # æ„é€ è¡¨ååˆ—è¡¨
            table_names = []
            for date_str in date_list:
                # å°†æ—¥æœŸæ ¼å¼è½¬æ¢ä¸ºè¡¨åæ ¼å¼ (YYYY-MM-DD -> YYYYMMDD)
                date_obj = datetime.datetime.strptime(date_str, "%Y-%m-%d")
                table_name = f"power_data_{date_obj.strftime('%Y%m%d')}"
                print(f"ğŸ” æŸ¥è¯¢è¡¨: {table_name}")
                table_names.append(table_name)
                
            
            # éªŒè¯è¡¨æ˜¯å¦å­˜åœ¨
            existing_tables = self.db_manager.get_tables()
            valid_tables = [table for table in table_names if table in existing_tables]
            
            if not valid_tables:
                return {"data": [], "total": 0, "message": "æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ•°æ®è¡¨"}
            
            # æ„é€ UNIONæŸ¥è¯¢è¯­å¥ï¼šæŸ¥æ‰¾åŒ…å«æŒ‡å®šå…³é”®å­—å’Œ"å‡å€¼"çš„è®°å½•
            union_parts = []
            for table in valid_tables:
                union_parts.append(f""" SELECT * FROM {table} WHERE channel_name LIKE '%å‡å€¼%' AND channel_name LIKE '%{data_type_keyword}%'""")
            
            if not union_parts:
                return {"data": [], "total": 0, "message": "æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„æ•°æ®"}
                
            union_query = " UNION ALL ".join(union_parts)
            print(f"ğŸš€ æ‰§è¡ŒUNIONæŸ¥è¯¢: {union_query}")
            final_query = f"""
                SELECT * FROM ({union_query}) as combined_data
                ORDER BY record_date, record_time
            """
            
            # æ‰§è¡ŒæŸ¥è¯¢
            result = self.db_manager.complex_query(final_query)
            # print(f"âœ… æŸ¥è¯¢æˆåŠŸï¼Œå…± {len(result)} æ¡è®°å½•")
            # print(result)
            
            # æ„é€ è¿”å›ç»“æœ
            return {
                "data": result.get("data"),
                "total": result.get("total"),
                "message": "æŸ¥è¯¢æˆåŠŸ"
            }
            
        except Exception as e:
            print(f"âŒ æŸ¥è¯¢å¤šå¤©å‡å€¼æ•°æ®å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return {"data": [], "total": 0, "message": f"æŸ¥è¯¢å¤±è´¥: {str(e)}"}



    def save_to_new_table(self, records, table_name):
        """
        ä¿å­˜è®°å½•åˆ°æ–°è¡¨
        
        Args:
            records: è®°å½•åˆ—è¡¨
            table_name: è¡¨å
            
        Returns:
            tuple: (success: bool, table_name: str, record_count: int, preview_data: list)
        """
        if not records:
            print("âŒ æ²¡æœ‰å¯ä¿å­˜çš„è®°å½•")
            return False, None, 0, []

        # å¦‚æœä¼ å…¥çš„æ˜¯ DataFrameï¼Œè½¬æˆ list[dict]
        if isinstance(records, pd.DataFrame):
            records = records.to_dict(orient="records")

        if not isinstance(records, list):
            print(f"âŒ records ç±»å‹é”™è¯¯: {type(records)}ï¼Œåº”ä¸º list[dict]")
            return False, None, 0, []

        # è¿‡æ»¤æ— æ•ˆè®°å½•
        valid_records = []
        for i, r in enumerate(records):
            if not isinstance(r, dict):
                continue
            # ä¸å¼ºåˆ¶è¦æ±‚ç‰¹å®šå­—æ®µï¼Œå› ä¸ºæˆ‘ä»¬æ˜¯åˆ›å»ºæ–°è¡¨
            valid_records.append(r)

        if not valid_records:
            print("âŒ æ²¡æœ‰å¯ä¿å­˜çš„æœ‰æ•ˆè®°å½•")
            return False, None, 0, []

        preview_data = []

        try:
            with self.db_manager.engine.begin() as conn:
                # åˆ›å»ºæ–°è¡¨
                create_table_sql = f"""
                CREATE TABLE {table_name} (
                    id BIGINT AUTO_INCREMENT PRIMARY KEY,
                    record_date DATE,
                    record_time VARCHAR(10),
                    type VARCHAR(255),
                    channel_name VARCHAR(255),
                    value DECIMAL(15,4),
                    sheet_name VARCHAR(255),
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
                """
                try:
                    conn.execute(text(create_table_sql))
                    print(f"âœ… è¡¨ {table_name} åˆ›å»ºæˆåŠŸ")
                except Exception as e:
                    if "already exists" in str(e) or "already exist" in str(e):
                        print(f"âš ï¸ è¡¨ {table_name} å·²å­˜åœ¨")
                    else:
                        raise e

                # å‡†å¤‡æ’å…¥è¯­å¥ï¼ˆåªæ’å…¥å­˜åœ¨çš„å­—æ®µï¼‰
                if valid_records:
                    # è·å–ç¬¬ä¸€æ¡è®°å½•çš„å­—æ®µ
                    sample_record = valid_records[0]
                    fields = [k for k in sample_record.keys() if k in [
                        "record_date", "record_time", "type", "channel_name", "value", "sheet_name"]]
                    
                    field_placeholders = ", ".join(fields)
                    value_placeholders = ", ".join([f":{field}" for field in fields])
                    
                    insert_stmt = text(f"""
                    INSERT INTO {table_name} ({field_placeholders})
                    VALUES ({value_placeholders})
                    """)

                    # æ‰¹é‡æ’å…¥
                    batch_size = 200
                    for i in range(0, len(valid_records), batch_size):
                        batch = valid_records[i:i + batch_size]
                        # åªä¿ç•™å­˜åœ¨çš„å­—æ®µ
                        filtered_batch = []
                        for record in batch:
                            filtered_record = {k: v for k, v in record.items() if k in fields}
                            filtered_batch.append(filtered_record)
                        
                        conn.execute(insert_stmt, filtered_batch)
                        print(f"ğŸ’¾ å·²æ’å…¥ç¬¬ {i // batch_size + 1} æ‰¹æ•°æ® ({len(filtered_batch)} æ¡)")

                # è·å–è®°å½•æ€»æ•°
                count_stmt = text(f"SELECT COUNT(*) FROM {table_name}")
                count = conn.execute(count_stmt).scalar()
                
                # è·å–å‰5è¡Œæ•°æ®é¢„è§ˆ
                preview_stmt = text(f"SELECT * FROM {table_name} ORDER BY id DESC LIMIT 5")
                result = conn.execute(preview_stmt)
                # ä¿®å¤ï¼šæ­£ç¡®å¤„ç†SQLAlchemyè¡Œå¯¹è±¡
                preview_data = []
                for row in result:
                    # å°†è¡Œå¯¹è±¡è½¬æ¢ä¸ºå­—å…¸
                    preview_data.append(dict(zip(result.keys(), row)))
                
                print(f"âœ… æ•°æ®åº“ä¿å­˜æˆåŠŸ: {count} æ¡è®°å½•")
                return True, table_name, count, preview_data

        except Exception as e:
            print(f"âŒ æ•°æ®åº“ä¿å­˜å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False, None, 0, []
