import pandas as pd
import datetime
import re
from sqlalchemy import text
from database import DatabaseManager

class PowerDataImporter:
    def __init__(self):
        self.db_manager = DatabaseManager()
        pass

    # ===============================
    # ä¸»å…¥å£ï¼šå¯¼å…¥æ‰€æœ‰sheet
    # ===============================
    def import_power_data(self, excel_file):
        """è‡ªåŠ¨å¯¼å…¥Excelä¸­æ‰€æœ‰Sheetçš„æ•°æ®ï¼Œæ—¥æœŸè‡ªåŠ¨è¯†åˆ«"""
        sheet_dict = self.read_excel_data(excel_file)
        if not sheet_dict:
            return False, None, 0, []

        all_records = []
        table_name = None
        data_type = None

        for sheet_name, df in sheet_dict.items():
            # === è‡ªåŠ¨è¯†åˆ«æ—¥æœŸ ===
            match = re.search(r"\((\d{4}-\d{2}-\d{2})\)", sheet_name)
            data_date = datetime.datetime.strptime(match.group(1), "%Y-%m-%d").date()

            # === æ ¹æ®æ–‡ä»¶åè¯†åˆ«ç±»å‹ ===
            file_name = str(excel_file)
            
            chinese_match = re.search(r'([\u4e00-\u9fff]+)', file_name)
            if chinese_match:
                data_type = chinese_match.group(1)
                print(f"ğŸ“ æ–‡ä»¶ç±»å‹è¯†åˆ«: {data_type}")
            else:
                print(f"âš ï¸ æœªèƒ½åœ¨æ–‡ä»¶åä¸­æ‰¾åˆ°æ±‰å­—ï¼š{file_name}ï¼Œè·³è¿‡ã€‚")
                return False, None, 0, []

            print(f"\nğŸ“˜ æ­£åœ¨å¤„ç† {sheet_name} | æ—¥æœŸ: {data_date} | ç±»å‹: {data_type}")

            records = self.process_24h_data(df, data_date, sheet_name, data_type)
            all_records.extend(records)

        if not all_records:
            print("âŒ æ²¡æœ‰ä»»ä½•æœ‰æ•ˆæ•°æ®è¢«å¯¼å…¥")
            return False, None, 0, []

        # === ä¿å­˜æ•°æ®åº“ ===
        success, table_name, record_count, preview_data = self.save_to_database(all_records, data_date)
        return success, table_name, record_count, preview_data

    # ===============================
    # è¯»å–æ‰€æœ‰sheet
    # ===============================
    def read_excel_data(self, excel_file):
        """è¯»å–Excelä¸­æ‰€æœ‰Sheet"""
        try:
            sheet_dict = pd.read_excel(excel_file, sheet_name=None, header=0)
            print(f"âœ… æˆåŠŸè¯»å–Excelï¼Œå…± {len(sheet_dict)} ä¸ªSheet: {list(sheet_dict.keys())}")
            return sheet_dict
        except Exception as e:
            print(f"âŒ è¯»å–Excelå¤±è´¥: {e}")
            return None

    # ===============================
    # å¤„ç†å•ä¸ªsheetçš„24å°æ—¶æ•°æ®
    # ===============================
    def process_24h_data(self, df, data_date, sheet_name, data_type):
        """å¤„ç†å•ä¸ªSheetï¼ˆè¡Œå¼ç»“æ„ï¼‰çš„24å°æ—¶æ•°æ®"""
        records = []

        # æ ‡å‡†åŒ–åˆ—å
        df.columns = [str(c).strip() for c in df.columns]

        # æ£€æŸ¥æ•°æ®æ ¼å¼ï¼šæœ‰"é€šé“åç§°"åˆ—è¿˜æ˜¯æœ‰"ç±»å‹"åˆ—
        if "é€šé“åç§°" in df.columns:
            records = self._process_channel_format(df, data_date, sheet_name, data_type)
        elif "ç±»å‹" in df.columns:
            records = self._process_type_format(df, data_date, sheet_name, data_type)
        else:
            print(f"âš ï¸ æœªæ‰¾åˆ° 'é€šé“åç§°' æˆ– 'ç±»å‹' åˆ—ï¼Œè·³è¿‡ã€‚å¯ç”¨åˆ—: {list(df.columns)}")
            return records

        print(f"âœ… {data_type} å¯¼å…¥ {len(records)} æ¡è®°å½•")
        return records

    def _process_channel_format(self, df, data_date, sheet_name, data_type):
        """å¤„ç†æœ‰'é€šé“åç§°'åˆ—çš„æ•°æ®æ ¼å¼"""
        records = []

        # ç›´æ¥ä½¿ç”¨æ‰€æœ‰æœ‰é€šé“åç§°çš„è¡Œ
        valid_rows = df[df["é€šé“åç§°"].notna()]
        if valid_rows.empty:
            print(f"âš ï¸ Sheetä¸­æ— æœ‰æ•ˆé€šé“ï¼Œé€šé“åˆ—å€¼ä¸º: {df['é€šé“åç§°'].unique().tolist()}")
            return records

        # æå–æ‰€æœ‰æ—¶é—´åˆ—ï¼ˆä¸€èˆ¬ä»00:00åˆ°23:45ï¼‰
        time_cols = [c for c in df.columns if re.match(r"\d{2}:\d{2}", c)]
        if not time_cols:
            print(f"âš ï¸ æ²¡æœ‰å‘ç°æ—¶é—´åˆ—: {list(df.columns)}")
            return records

        # éå†æ¯ä¸€è¡Œï¼ˆä¸€ä¸ªé€šé“ï¼‰
        for _, row in valid_rows.iterrows():
            channel_name = row["é€šé“åç§°"]

            for t in time_cols:
                # å¤„ç†NaNå€¼ï¼Œè·³è¿‡NULLå€¼
                value = row[t]
                if pd.isna(value):
                    continue  # è·³è¿‡è¿™ä¸ªè®°å½•
                
                record = {
                    "record_date": data_date,
                    "record_time": t,
                    "channel_name": channel_name,
                    "value": value,
                    "type": data_type,
                    "sheet_name": sheet_name,
                    "created_at": datetime.datetime.now(),
                }
                records.append(record)

        return records

    def _process_type_format(self, df, data_date, sheet_name, data_type):
        """å¤„ç†æœ‰'ç±»å‹'åˆ—çš„æ•°æ®æ ¼å¼"""
        records = []

        # ç›´æ¥ä½¿ç”¨æ‰€æœ‰æœ‰ç±»å‹åç§°çš„è¡Œ
        valid_rows = df[df["ç±»å‹"].notna()]
        if valid_rows.empty:
            print(f"âš ï¸ Sheetä¸­æ— æœ‰æ•ˆç±»å‹ï¼Œç±»å‹åˆ—å€¼ä¸º: {df['ç±»å‹'].unique().tolist()}")
            return records

        # æå–æ‰€æœ‰æ—¶é—´åˆ—ï¼ˆä¸€èˆ¬ä»00:00åˆ°23:45ï¼‰
        time_cols = [c for c in df.columns if re.match(r"\d{2}:\d{2}", c)]
        if not time_cols:
            print(f"âš ï¸ æ²¡æœ‰å‘ç°æ—¶é—´åˆ—: {list(df.columns)}")
            return records

        # éå†æ¯ä¸€è¡Œï¼ˆä¸€ä¸ªç±»å‹ï¼‰
        for _, row in valid_rows.iterrows():
            channel_name = row["ç±»å‹"]  # å°†"ç±»å‹"åˆ—çš„å€¼ä½œä¸ºchannel_name

            for t in time_cols:
                # å¤„ç†NaNå€¼ï¼Œè·³è¿‡NULLå€¼
                value = row[t]
                if pd.isna(value):
                    continue  # è·³è¿‡è¿™ä¸ªè®°å½•
                
                record = {
                    "record_date": data_date,
                    "record_time": t,
                    "channel_name": channel_name,
                    "value": value,
                    "type": data_type,
                    "sheet_name": sheet_name,
                    "created_at": datetime.datetime.now(),
                }
                records.append(record)

        return records
    # ä¿å­˜æ•°æ®åˆ°æ•°æ®åº“
    def save_to_database(self, records, data_date):
        """æŒ‰æ—¥æœŸè‡ªåŠ¨åˆ›å»ºè¡¨å¹¶ä¿å­˜æ•°æ®"""
        if not records:
            print("âŒ æ²¡æœ‰å¯ä¿å­˜çš„è®°å½•")
            return False, None, 0, []

        # ğŸ§© 1. å¦‚æœä¼ å…¥çš„æ˜¯ DataFrameï¼Œè½¬æˆ list[dict]
        if isinstance(records, pd.DataFrame):
            records = records.to_dict(orient="records")

        if not isinstance(records, list):
            print(f"âŒ records ç±»å‹é”™è¯¯: {type(records)}ï¼Œåº”ä¸º list[dict]")
            return False, None, 0, []

        # ğŸ§© 2. è¿‡æ»¤æ— æ•ˆè®°å½•
        valid_records = []
        for i, r in enumerate(records):
            if not isinstance(r, dict):
                continue
            required_fields = ["record_date", "record_time", "channel_name", "value", "type", "sheet_name"]
            if not all(k in r for k in required_fields):
                continue
            # è½¬ record_date
            if isinstance(r["record_date"], str):
                r["record_date"] = pd.to_datetime(r["record_date"]).date()
            valid_records.append(r)

        if not valid_records:
            print("âŒ æ²¡æœ‰å¯ä¿å­˜çš„æœ‰æ•ˆè®°å½•")
            return False, None, 0, []

        # --- ç”ŸæˆæŒ‰å¤©è¡¨å ---
        table_name = f"power_data_{data_date.strftime('%Y%m%d')}"
        preview_data = []

        try:
            with self.db_manager.engine.begin() as conn:
                # --- åˆ›å»ºè¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰ ---
                create_table_sql = f"""
                CREATE TABLE IF NOT EXISTS {table_name} (
                    id BIGINT AUTO_INCREMENT PRIMARY KEY,
                    record_date DATE NOT NULL,
                    record_time TIME,
                    type VARCHAR(255),
                    channel_name VARCHAR(255),
                    value DECIMAL(10,2),
                    sheet_name VARCHAR(255)
                );
                """
                conn.execute(text(create_table_sql))
                print(f"âœ… è¡¨ {table_name} å·²å­˜åœ¨æˆ–åˆ›å»ºæˆåŠŸ")

                # --- æ‰¹é‡æ’å…¥ ---
                insert_stmt = text(f"""
                INSERT INTO {table_name} 
                (record_date, record_time, type, channel_name, value, sheet_name)
                VALUES (:record_date, :record_time, :type, :channel_name, :value, :sheet_name)
                """)

                batch_size = 200
                for i in range(0, len(valid_records), batch_size):
                    batch = valid_records[i:i + batch_size]
                    conn.execute(insert_stmt, batch)
                    print(f"ğŸ’¾ å·²æ’å…¥ç¬¬ {i // batch_size + 1} æ‰¹æ•°æ® ({len(batch)} æ¡)")

                count_stmt = text(f"SELECT COUNT(*) FROM {table_name} WHERE record_date = :record_date")
                count = conn.execute(count_stmt, {"record_date": data_date}).scalar()
                
                # è·å–å‰5è¡Œæ•°æ®é¢„è§ˆ
                preview_stmt = text(f"SELECT * FROM {table_name} WHERE record_date = :record_date ORDER BY id DESC LIMIT 5")
                result = conn.execute(preview_stmt, {"record_date": data_date})
                # ä¿®å¤ï¼šæ­£ç¡®å¤„ç†SQLAlchemyè¡Œå¯¹è±¡
                preview_data = []
                for row in result:
                    # å°†è¡Œå¯¹è±¡è½¬æ¢ä¸ºå­—å…¸
                    preview_data.append(dict(zip(result.keys(), row)))
                
                print(f"âœ… æ•°æ®åº“ä¿å­˜æˆåŠŸ: {count} æ¡è®°å½•")
                return True, table_name, count, preview_data

        except Exception as e:
            print(f"âŒ æ•°æ®åº“ä¿å­˜å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False, None, 0, []
    def save_to_outage_database(self, records, data_date):
        """ä¿å­˜åœç”µæ•°æ®åˆ°å›ºå®šè¡¨ power_outage"""
        if not records:
            print("âŒ æ²¡æœ‰å¯ä¿å­˜çš„è®°å½•")
            return False, None, 0, []

        # ğŸ§© 1. å¦‚æœä¼ å…¥çš„æ˜¯ DataFrameï¼Œè½¬æˆ list[dict]
        if isinstance(records, pd.DataFrame):
            records = records.to_dict(orient="records")

        if not isinstance(records, list):
            print(f"âŒ records ç±»å‹é”™è¯¯: {type(records)}ï¼Œåº”ä¸º list[dict]")
            return False, None, 0, []

        # ğŸ§© 2. è¿‡æ»¤æ— æ•ˆè®°å½•
        valid_records = []
        for i, r in enumerate(records):
            if not isinstance(r, dict):
                continue
            required_fields = ["device_name", "voltage_level", "device_type", "device_code", 
                        "planned_power_off_time", "actual_power_off_time", "planned_power_on_time","actual_power_on_time"]
            if not all(k in r for k in required_fields):
                continue
            # æ·»åŠ  record_date å­—æ®µ
            r["record_date"] = data_date
            valid_records.append(r)

        if not valid_records:
            print("âŒ æ²¡æœ‰å¯ä¿å­˜çš„æœ‰æ•ˆè®°å½•")
            return False, None, 0, []

        # --- ä½¿ç”¨å›ºå®šè¡¨å ---
        table_name = "power_outage"
        preview_data = []

        try:
            with self.db_manager.engine.begin() as conn:
                # --- åˆ›å»ºè¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰ ---
                create_table_sql = f"""
                CREATE TABLE IF NOT EXISTS `{table_name}` (
                    `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT 'è‡ªå¢ä¸»é”®',
                    `record_date` date NOT NULL COMMENT 'è®°å½•æ—¥æœŸ',
                    `device_name` varchar(200) NOT NULL COMMENT 'è®¾å¤‡åç§°ï¼ˆå¦‚101å˜å‹å™¨å¼€å…³ã€220kV#1ä¸»å˜ï¼‰',
                    `voltage_level` varchar(50) DEFAULT NULL COMMENT 'ç”µå‹ç­‰çº§ï¼ˆå…è®¸ä¸ºç©ºï¼Œéƒ¨åˆ†è®¾å¤‡å¯èƒ½æœªè®°å½•ï¼‰',
                    `device_type` varchar(100) NOT NULL COMMENT 'è®¾å¤‡ç±»å‹ï¼ˆå¦‚å¼€å…³ã€ä¸»å˜ã€æ¯çº¿ï¼‰',
                    `device_code` varchar(50) NOT NULL COMMENT 'è®¾å¤‡ç¼–å·ï¼ˆå”¯ä¸€æ ‡è¯†ï¼‰',
                    `planned_power_off_time` datetime DEFAULT NULL COMMENT 'è®¡åˆ’åœç”µæ—¥æœŸæ—¶é—´ï¼ˆæ ¼å¼ï¼šYYYY-MM-DD HH:MM:SSï¼‰',
                    `actual_power_off_time` datetime DEFAULT NULL COMMENT 'å®é™…åœç”µæ—¥æœŸæ—¶é—´ï¼ˆæ ¼å¼ï¼šYYYY-MM-DD HH:MM:SSï¼‰',
                    `planned_power_on_time` datetime DEFAULT NULL COMMENT 'è®¡åˆ’å¤ç”µæ—¥æœŸæ—¶é—´ï¼ˆæ ¼å¼ï¼šYYYY-MM-DD HH:MM:SSï¼‰',
                    `actual_power_on_time` datetime DEFAULT NULL COMMENT 'å®é™…å¤ç”µæ—¥æœŸæ—¶é—´ï¼ˆæ ¼å¼ï¼šYYYY-MM-DD HH:MM:SSï¼‰',
                    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT 'è®°å½•åˆ›å»ºæ—¶é—´',
                    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT 'è®°å½•æ›´æ–°æ—¶é—´',
                    `sheet_name` varchar(255) DEFAULT NULL COMMENT 'æ•°æ®æ¥æºè¡¨å',
                    PRIMARY KEY (`id`),
                    UNIQUE KEY `uk_device_code` (`device_code`) COMMENT 'è®¾å¤‡ç¼–å·å”¯ä¸€çº¦æŸ'
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='è®¾å¤‡åœç”µè®°å½•ä¿¡æ¯è¡¨';
                """
                conn.execute(text(create_table_sql))
                print(f"âœ… è¡¨ {table_name} å·²å­˜åœ¨æˆ–åˆ›å»ºæˆåŠŸ")

                # --- æ‰¹é‡æ’å…¥ ---
                insert_stmt = text(f"""
                INSERT IGNORE INTO {table_name} 
                (device_name, record_date, voltage_level, device_type, device_code, planned_power_off_time, actual_power_off_time, planned_power_on_time, actual_power_on_time, sheet_name)
                VALUES (:device_name, :record_date, :voltage_level, :device_type, :device_code, STR_TO_DATE(:planned_power_off_time, '%Y%m%d_%H:%i:%s'), STR_TO_DATE(:actual_power_off_time, '%Y%m%d_%H:%i:%s'), STR_TO_DATE(:planned_power_on_time, '%Y%m%d_%H:%i:%s'), STR_TO_DATE(:actual_power_on_time, '%Y%m%d_%H:%i:%s'), :sheet_name)
                """)

                batch_size = 200
                for i in range(0, len(valid_records), batch_size):
                    batch = valid_records[i:i + batch_size]
                    conn.execute(insert_stmt, batch)
                    print(f"ğŸ’¾ å·²æ’å…¥ç¬¬ {i // batch_size + 1} æ‰¹æ•°æ® ({len(batch)} æ¡)")
                # è·å–æ’å…¥çš„æ•°æ®æ€»é‡
                count_stmt = text(f"SELECT COUNT(*) FROM {table_name} WHERE record_date = :record_date")
                count = conn.execute(count_stmt, {"record_date": data_date}).scalar()

                print(f"âœ… {table_name} æ•°æ®åº“ä¿å­˜æˆåŠŸ: {count} æ¡è®°å½•")
                return True, table_name, count, []
        
        except Exception as e:
            print(f"âŒ æ•°æ®åº“ä¿å­˜å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False, None, 0, []    
    def save_to_ynjichu_database(self, records, data_date):
        """ä¿å­˜åœç”µæ•°æ®åˆ°å›ºå®šè¡¨ power_ynjichu"""
        if not records:
            print("âŒ æ²¡æœ‰å¯ä¿å­˜çš„è®°å½•")
            return True, None, 0, []

        # ğŸ§© 1. å¦‚æœä¼ å…¥çš„æ˜¯ DataFrameï¼Œè½¬æˆ list[dict]
        if isinstance(records, pd.DataFrame):
            records = records.to_dict(orient="records")

        if not isinstance(records, list):
            print(f"âŒ records ç±»å‹é”™è¯¯: {type(records)}ï¼Œåº”ä¸º list[dict]")
            return False, None, 0, []

        # ğŸ§© 2. è¿‡æ»¤æ— æ•ˆè®°å½•
        valid_records = []
        for i, r in enumerate(records):
            if not isinstance(r, dict):
                continue
            # æ·»åŠ  record_date å­—æ®µ
            r["record_date"] = data_date
            valid_records.append(r)

        if not valid_records:
            print("âŒ æ²¡æœ‰å¯ä¿å­˜çš„æœ‰æ•ˆè®°å½•")
            return True, None, 0, []

        # --- ä½¿ç”¨å›ºå®šè¡¨å ---
        table_name = "power_jizujichu"
        preview_data = []

        try:
            with self.db_manager.engine.begin() as conn:
                # --- åˆ›å»ºè¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰---
                create_table_sql = f"""
                CREATE TABLE IF NOT EXISTS `power_ynjichu` (
                `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT 'è‡ªå¢ä¸»é”®ï¼Œå”¯ä¸€æ ‡è¯†ä¸€æ¡è®°å½•',
                `record_date` date NOT NULL COMMENT 'è®°å½•æ—¥æœŸ',
                `unit_group_name` varchar(200) DEFAULT NULL COMMENT 'æœºç»„ç¾¤åï¼ˆå…è®¸ä¸ºç©ºï¼‰',
                `power_plant_id` varchar(50) DEFAULT NULL COMMENT 'ç”µå‚IDï¼ˆå…è®¸ä¸ºç©ºï¼‰',
                `power_plant_name` varchar(200) DEFAULT NULL COMMENT 'ç”µå‚åç§°ï¼ˆå…è®¸ä¸ºç©ºï¼‰',
                `unit_id` varchar(50) DEFAULT NULL COMMENT 'æœºç»„IDï¼ˆå…è®¸ä¸ºç©ºï¼‰',
                `unit_name` varchar(200) DEFAULT NULL COMMENT 'æœºç»„åç§°ï¼ˆå…è®¸ä¸ºç©ºï¼‰',
                `proportion` decimal(10,4) DEFAULT NULL COMMENT 'æ‰€å æ¯”ä¾‹ï¼ˆå…è®¸ä¸ºç©ºï¼Œå¦‚0.35è¡¨ç¤º35%ï¼‰',
                `sheet_name` varchar(255) DEFAULT NULL COMMENT 'æ•°æ®æ¥æºè¡¨åï¼ˆå…è®¸ä¸ºç©ºï¼‰',
                `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT 'è®°å½•å…¥åº“æ—¶é—´ï¼ˆè‡ªåŠ¨ç”Ÿæˆï¼‰',
                `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT 'è®°å½•æ›´æ–°æ—¶é—´ï¼ˆè‡ªåŠ¨æ›´æ–°ï¼‰',
                PRIMARY KEY (`id`),
                KEY `idx_unit_group` (`unit_group_name`) COMMENT 'æœºç»„ç¾¤åç´¢å¼•'
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='æœºç»„ç¾¤-æœºç»„åˆ†é…æ¯”ä¾‹è®°å½•è¡¨ï¼ˆæ‰€æœ‰å­—æ®µå…è®¸ä¸ºç©ºï¼‰';
                """
                conn.execute(text(create_table_sql))
                print(f"âœ… è¡¨ {table_name} å·²å­˜åœ¨æˆ–åˆ›å»ºæˆåŠŸ")

                # åˆ é™¤è¯¥æ—¥æœŸçš„æ—§æ•°æ®
                conn.execute(text(f"DELETE FROM {table_name} WHERE record_date = :record_date"), 
                             {"record_date": data_date})
                print(f"ğŸ—‘ï¸ å·²åˆ é™¤ {data_date} çš„æ—§æ•°æ®")

                # --- æ‰¹é‡æ’å…¥ ---
                insert_stmt = text(f"""
                INSERT IGNORE INTO {table_name} 
                (record_date, unit_group_name, power_plant_id, power_plant_name, unit_id, unit_name, proportion, sheet_name)
                VALUES 
                (:record_date, :unit_group_name, :power_plant_id, :power_plant_name, :unit_id, :unit_name, :proportion, :sheet_name)
                """)
                
                # æ‰¹é‡æ’å…¥æ•°æ®
                batch_size = 200
                for i in range(0, len(valid_records), batch_size):
                    batch = valid_records[i:i + batch_size]
                    conn.execute(insert_stmt, batch)
                    print(f"ğŸ’¾ å·²æ’å…¥ç¬¬ {i // batch_size + 1} æ‰¹æ•°æ® ({len(batch)} æ¡)")

                # è·å–æ’å…¥çš„æ•°æ®æ€»é‡
                count_stmt = text(f"SELECT COUNT(*) FROM {table_name} WHERE record_date = :record_date")
                count = conn.execute(count_stmt, {"record_date": data_date}).scalar()
                
                # è·å–é¢„è§ˆæ•°æ®
                preview_stmt = text(f"SELECT * FROM {table_name} WHERE record_date = :record_date LIMIT 5")
                preview_result = conn.execute(preview_stmt, {"record_date": data_date})
                for row in preview_result:
                    preview_data.append(dict(row._mapping))

                print(f"âœ… {table_name} æ•°æ®åº“ä¿å­˜æˆåŠŸ: {count} æ¡è®°å½•")
                return True, table_name, count, []

        except Exception as e:
            print(f"âŒ {table_name} æ•°æ®åº“ä¿å­˜å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False, None, 0, []
    
    def save_to_internal_database(self, records, data_date):
        """ä¿å­˜å‘ç”µæœºå¹²é¢„è®°å½•åˆ°å›ºå®šè¡¨ generator_intervention_records"""
        if not records:
            print("âŒ æ²¡æœ‰å¯ä¿å­˜çš„è®°å½•")
            return False, None, 0, []

        # ğŸ§© 1. å¦‚æœä¼ å…¥çš„æ˜¯ DataFrameï¼Œè½¬æˆ list[dict]
        if isinstance(records, pd.DataFrame):
            records = records.to_dict(orient="records")

        if not isinstance(records, list):
            print(f"âŒ records ç±»å‹é”™è¯¯: {type(records)}ï¼Œåº”ä¸º list[dict]")
            return False, None, 0, []

        # ğŸ§© 2. è¿‡æ»¤æ— æ•ˆè®°å½•
        valid_records = []
        for i, r in enumerate(records):
            if not isinstance(r, dict):
                continue
            required_fields = ["object_name", "object_id", "intervention_start_time", "intervention_end_time",
                               "pre_intervention_max", "pre_intervention_min", "post_intervention_max", "post_intervention_min",
                               "intervention_reason"]
            if not all(k in r for k in required_fields):
                continue
            r["record_date"] = data_date
            valid_records.append(r)

        if not valid_records:
            print("âŒ æ²¡æœ‰å¯ä¿å­˜çš„æœ‰æ•ˆè®°å½•")
            return False, None, 0, []

        # --- ä½¿ç”¨å›ºå®šè¡¨å ---
        table_name = "power_intervention"
        preview_data = []

        try:
            with self.db_manager.engine.begin() as conn:
                # --- åˆ›å»ºè¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰ ---
                create_table_sql = f"""
                CREATE TABLE IF NOT EXISTS `{table_name}` (
                  `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT 'è‡ªå¢ä¸»é”®',
                  `record_date` date NOT NULL COMMENT 'è®°å½•æ—¥æœŸ',
                  `sheet_name` varchar(255) DEFAULT NULL COMMENT 'æ•°æ®æ¥æºè¡¨å',
                  `object_name` varchar(200) NOT NULL COMMENT 'å¯¹è±¡åç§°ï¼ˆå¦‚ç‰›è¿œå‚#2å‘ç”µæœºï¼‰',
                  `object_id` varchar(50) NOT NULL COMMENT 'å¯¹è±¡IDï¼ˆå”¯ä¸€æ ‡è¯†ï¼Œå¦‚40813871689367554ï¼‰',
                  `intervention_start_time` datetime DEFAULT NULL COMMENT 'å¹²é¢„å¼€å§‹æ—¶é—´ï¼ˆæ ¼å¼ï¼šYYYY-MM-DD HH:MM:SSï¼‰',
                  `intervention_end_time` datetime DEFAULT NULL COMMENT 'å¹²é¢„ç»“æŸæ—¶é—´ï¼ˆæ ¼å¼ï¼šYYYY-MM-DD HH:MM:SSï¼‰',
                  `pre_intervention_max` decimal(10,3) DEFAULT NULL COMMENT 'å¹²é¢„å‰æœ€å¤§å€¼',
                  `pre_intervention_min` decimal(10,3) DEFAULT NULL COMMENT 'å¹²é¢„å‰æœ€å°å€¼',
                  `post_intervention_max` decimal(10,3) DEFAULT NULL COMMENT 'å¹²é¢„åæœ€å¤§å€¼',
                  `post_intervention_min` decimal(10,3) DEFAULT NULL COMMENT 'å¹²é¢„åæœ€å°å€¼',
                  `intervention_reason` varchar(500) DEFAULT NULL COMMENT 'å¹²é¢„åŸå› ï¼ˆå¦‚é…åˆç”µå‚å·¥ä½œ:ä¼˜åŒ–å¼€æœºæ›²çº¿ï¼‰',
                  `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT 'è®°å½•åˆ›å»ºæ—¶é—´',
                  `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT 'è®°å½•æ›´æ–°æ—¶é—´',
                  PRIMARY KEY (`id`),
                  KEY `idx_object_id` (`object_id`) COMMENT 'å¯¹è±¡IDç´¢å¼•ï¼Œç”¨äºå…³è”æŸ¥è¯¢',
                  KEY `idx_intervention_time` (`intervention_start_time`, `intervention_end_time`) COMMENT 'å¹²é¢„æ—¶é—´ç´¢å¼•ï¼Œç”¨äºæ—¶é—´èŒƒå›´æŸ¥è¯¢'
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='å‘ç”µæœºå¹²é¢„è®°å½•ä¿¡æ¯è¡¨';
                """
                conn.execute(text(create_table_sql))
                print(f"âœ… è¡¨ {table_name} å·²å­˜åœ¨æˆ–åˆ›å»ºæˆåŠŸ")

                # --- æ‰¹é‡æ’å…¥ ---
                insert_stmt = text(f"""
                INSERT IGNORE INTO {table_name} 
                (record_date, sheet_name, object_name, object_id, intervention_start_time, intervention_end_time, 
                 pre_intervention_max, pre_intervention_min, post_intervention_max, post_intervention_min, 
                 intervention_reason)
                VALUES (:record_date, :sheet_name, :object_name, :object_id, :intervention_start_time, :intervention_end_time,
                        :pre_intervention_max, :pre_intervention_min, :post_intervention_max, :post_intervention_min,
                        :intervention_reason)
                """)

                batch_size = 200
                for i in range(0, len(valid_records), batch_size):
                    batch = valid_records[i:i + batch_size]
                    conn.execute(insert_stmt, batch)
                    print(f"ğŸ’¾ å·²æ’å…¥ç¬¬ {i // batch_size + 1} æ‰¹æ•°æ® ({len(batch)} æ¡)")

                count_stmt = text(f"SELECT COUNT(*) FROM {table_name}")
                count = conn.execute(count_stmt).scalar()
                
                # è·å–å‰5è¡Œæ•°æ®é¢„è§ˆ
                preview_stmt = text(f"SELECT * FROM {table_name} ORDER BY id DESC LIMIT 5")
                result = conn.execute(preview_stmt)
                # ä¿®å¤ï¼šæ­£ç¡®å¤„ç†SQLAlchemyè¡Œå¯¹è±¡
                preview_data = []
                for row in result:
                    # å°†è¡Œå¯¹è±¡è½¬æ¢ä¸ºå­—å…¸
                    preview_data.append(dict(zip(result.keys(), row)))
                
                print(f"âœ… æ•°æ®åº“ä¿å­˜æˆåŠŸ: {count} æ¡è®°å½•")
                return True, table_name, count, []

        except Exception as e:
            print(f"âŒ æ•°æ®åº“ä¿å­˜å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False, None, 0, []
    
    def import_custom_excel(self, excel_file):
        """å¯¼å…¥æŒ‡å®šçš„5ä¸ªsheetï¼Œå¹¶æŒ‰å›ºå®šè§„åˆ™æ˜ å°„"""
        try:
            # è¯»å–æ‰€æœ‰sheet
            sheet_dict = pd.read_excel(excel_file, sheet_name=None, header=None)
        except Exception as e:
            print(f"âŒ æ— æ³•è¯»å–Excel: {e}")
            return False
        file_name = str(excel_file)
        
        chinese_match = re.search(r'([\u4e00-\u9fff]+)', file_name)
        if chinese_match:
            data_type = chinese_match.group(1) + "å®é™…ä¿¡æ¯"
            print(f"ğŸ“ æ–‡ä»¶ç±»å‹è¯†åˆ«: {data_type}")
        else:
            print(f"âš ï¸ æœªèƒ½åœ¨æ–‡ä»¶åä¸­æ‰¾åˆ°æ±‰å­—ï¼š{file_name}ï¼Œè·³è¿‡ã€‚")
            return False
        sheet_names = list(sheet_dict.keys())
        print(f"ğŸ“˜ æ£€æµ‹åˆ° {len(sheet_names)} ä¸ªSheet: {sheet_names}")

        # è¦å¤„ç†çš„sheetç¼–å·ï¼ˆ1-basedï¼‰
        target_indexes = [0, 1, 3, 4, 5,6,-2,-1]  # å¯¹åº”ç¬¬1,2,4,5,6ä¸ªsheet

        all_records = []
        outage_records = []
        ineternal_records = []

        for i in target_indexes:
            if i >= len(sheet_names):
                print(f"âš ï¸ Excelä¸­ä¸å­˜åœ¨ç¬¬{i+1}ä¸ªsheetï¼Œè·³è¿‡")
                continue

            sheet_name = sheet_names[i]
            df = sheet_dict[sheet_name]
            print(f"\nğŸ”¹ æ­£åœ¨å¤„ç† Sheet {i+1}: {sheet_name}")

            # ç»Ÿä¸€è¯†åˆ«æ—¥æœŸ
            match = re.search(r"\((\d{4}-\d{2}-\d{2})\)", sheet_name)
            data_date = datetime.datetime.strptime(match.group(1), "%Y-%m-%d").date()
            # æ ¹æ®sheetåºå·è°ƒç”¨ä¸åŒæ˜ å°„å‡½æ•°
            if i in [0, 3, 4]:  # ç¬¬1,4,5ä¸ªsheetï¼šæ—¶åˆ»â†’channel_name
                records = self._process_time_as_channel(df, data_date, sheet_name, data_type)
            elif i in [1, 5]:  # ç¬¬2,6ä¸ªsheetï¼šç¬¬ä¸€è¡Œâ†’channel_name
                records = self._process_first_row_as_channel(df, data_date, sheet_name, data_type)
            elif i in [6]:
                records = self._process_fsc_as_channel(df, data_date, sheet_name, data_type)
            elif i in [-2]:
                outage_records = self._process_outage_as_table(df, data_date, sheet_name)
            elif i in [-1]:
                ineternal_records = self._process_internal_as_table(df, data_date, sheet_name)
            else:
                print(f"âš ï¸ ç¬¬{i+1}ä¸ªsheetæœªå®šä¹‰å¤„ç†è§„åˆ™ï¼Œè·³è¿‡")
                continue

            print(f"âœ… Sheet{i+1} å¤„ç†å®Œæˆï¼Œå…± {len(records)} æ¡è®°å½•")
            all_records.extend(records)
        
        if not outage_records:
            print("âŒ æ²¡æœ‰ç”Ÿæˆä»»ä½•åœç”µè®°å½•")
            return False
        if not all_records:
            print("âŒ æ²¡æœ‰ç”Ÿæˆä»»ä½•æœ‰æ•ˆè®°å½•")
            return False

        success1, table_name1, count1, preview_data1 = self.save_to_database(all_records, data_date)
        success2, table_name2, count2, preview_data2 = self.save_to_outage_database(outage_records, data_date)
        success3, table_name3, count3, preview_data3 = self.save_to_internal_database(ineternal_records, data_date)
        
        # è¿”å›ä¸¤ä¸ªæ“ä½œçš„ç»“æœ
        return (success1, table_name1, count1, preview_data1), (success2, table_name2, count2, preview_data2),(success3, table_name3, count3, preview_data3)
    def _process_time_as_channel(self, df, data_date, sheet_name, data_type):
        """å°†æ—¶åˆ»åˆ—åæ˜ å°„ä¸ºchannel_name"""
        records = []
        df = df.dropna(how="all")  # åˆ é™¤ç©ºè¡Œ

        # å¦‚æœç¬¬ä¸€åˆ—æ˜¯ â€œæ—¶åˆ»â€ å­—æ ·
        if str(df.iloc[0, 0]).strip() == "æ—¶åˆ»":
            df.columns = [str(c).strip() for c in df.iloc[0]]  # ç¬¬ä¸€è¡Œä½œåˆ—å
            df = df[1:]  # å»æ‰æ ‡é¢˜è¡Œ
        else:
            df.columns = [str(c).strip() for c in df.columns]

        # æŸ¥æ‰¾æ—¶é—´åˆ—ï¼ˆå½¢å¦‚ 00:00ã€01:15ï¼‰
        time_cols = [c for c in df.columns if re.match(r"\d{2}:\d{2}", c)]
        if not time_cols:
            print(f"âš ï¸ æœªæ‰¾åˆ°æ—¶é—´åˆ—: {df.columns.tolist()}")
            return []
        # éå†æ¯ä¸€è¡Œï¼ˆæ¯ä¸€ç±»æŒ‡æ ‡ï¼‰
        for _, row in df.iterrows():
            # è·³è¿‡æ— æ•ˆè¡Œæˆ–æ ‡é¢˜è¡Œ
            if not isinstance(row[time_cols[0]], (int, float)) and not str(row[time_cols[0]]).replace('.', '', 1).isdigit():
                continue

            # æŒ‡æ ‡åï¼ˆæ¯”å¦‚ â€œç»Ÿè°ƒè´Ÿè·(MW)â€ï¼‰
            indicator_name = str(row.get("æ—¶åˆ»") or row.index[0]).strip()

            for t in time_cols:
                value = row[t]
                if pd.isna(value):
                    continue
                try:
                    value = float(value)
                except:
                    continue  # è·³è¿‡éæ•°å€¼çš„å•å…ƒæ ¼
                record = {
                    "record_date": data_date,
                    "record_time": t,
                    "channel_name": indicator_name,  # ç”¨æŒ‡æ ‡åä½œé€šé“å
                    "value": value,
                    "type": data_type,
                    "sheet_name": sheet_name,
                    "created_at": datetime.datetime.now(),
                }
                records.append(record)
        return records
    def _process_fsc_as_channel(self, df, data_date, sheet_name, data_type):
        """å°†æ—¶åˆ»åˆ—åæ˜ å°„ä¸ºchannel_name"""
        records = []
        df = df.dropna(how="all")  # åˆ é™¤ç©ºè¡Œ
        if df.empty:
            print(f"è­¦å‘Šï¼šsheet '{sheet_name}' æ— æœ‰æ•ˆæ•°æ®ï¼ˆæ‰€æœ‰è¡Œéƒ½æ˜¯ç©ºè¡Œï¼‰")
            return records  # è¿”å›ç©ºåˆ—è¡¨ï¼Œé¿å…åç»­æŠ¥é”™
        df.columns = [str(c).strip() for c in df.iloc[0]]  # ç¬¬ä¸€è¡Œä½œåˆ—å
        df = df[1:]  # å»æ‰æ ‡é¢˜è¡Œ
        
        first_col = df.columns[0]
        second_col = df.columns[1]
       
        # æŸ¥æ‰¾æ—¶é—´åˆ—ï¼ˆå½¢å¦‚ 00:00ã€01:15 æˆ–æ•°å­—æ ¼å¼ 0, 1, 2...ï¼‰
        time_cols = [c for c in df.columns if re.match(r"\d{2}:\d{2}", c)]
        if not time_cols:
            print(f"âš ï¸ æœªæ‰¾åˆ°æ—¶é—´åˆ—: {df.columns.tolist()}")
            return []

        # éå†æ¯ä¸€è¡Œï¼ˆæ¯ä¸€ç±»æŒ‡æ ‡ï¼‰
        for _, row in df.iterrows():
            # è·³è¿‡æ— æ•ˆè¡Œæˆ–æ ‡é¢˜è¡Œ
            if not isinstance(row[time_cols[0]], (int, float)) and not str(row[time_cols[0]]).replace('.', '', 1).isdigit():
                continue
            
            # ç”Ÿæˆ channel_nameï¼šç¬¬ä¸€åˆ—å’Œç¬¬äºŒåˆ—ç”¨ä¸‹åˆ’çº¿è¿æ¥
            channel_name = f"{row[first_col]}_{row[second_col]}"

            for t in time_cols:
                value = row[t]
                if pd.isna(value):
                    continue
                try:
                    value = float(value)
                except:
                    continue  # è·³è¿‡éæ•°å€¼çš„å•å…ƒæ ¼

                record = {
                    "record_date": data_date,
                    "record_time": t,
                    "channel_name": channel_name,  # ç”¨æŒ‡æ ‡åä½œé€šé“å
                    "value": value,
                    "type": data_type,
                    "sheet_name": sheet_name,
                    "created_at": datetime.datetime.now(),
                }
                records.append(record)
        return records
    
    def _process_3_as_channel(self, df, data_date, sheet_name):
        """
        å¤„ç†è®¾å¤‡ç”µå‹ç­‰çº§ä¿¡æ¯sheetï¼Œæå–è®¾å¤‡ç”µå‹ç­‰çº§æ•°æ®
        """
        records = []
        df = df.dropna(how="all")  # åˆ é™¤ç©ºè¡Œ
        
        if df.empty:
            print(f"è­¦å‘Šï¼šsheet '{sheet_name}' æ— æœ‰æ•ˆæ•°æ®ï¼ˆæ‰€æœ‰è¡Œéƒ½æ˜¯ç©ºè¡Œï¼‰")
            return records  # è¿”å›ç©ºåˆ—è¡¨ï¼Œé¿å…åç»­æŠ¥é”™

        # ç¡®ä¿åˆ—åæ­£ç¡®
        df.columns = [str(c).strip() for c in df.columns]
        
        # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
        required_columns = ["åºå·", "æ—¥æœŸ", "è®¾å¤‡åç§°", "ç”µå‹ç­‰çº§(kV)"]
        if not all(col in df.columns for col in required_columns):
            print(f"âš ï¸  sheet '{sheet_name}' ç¼ºå°‘å¿…è¦çš„åˆ—: {required_columns}")
            return records

        # éå†æ¯ä¸€è¡Œæ•°æ®
        for _, row in df.iterrows():
            # è·³è¿‡ç©ºè¡Œ
            if pd.isna(row["åºå·"]) and pd.isna(row["æ—¥æœŸ"]) and pd.isna(row["è®¾å¤‡åç§°"]):
                continue
                
            # å¤„ç†åºå·å­—æ®µ
            def convert_serial_number(value):
                if pd.isna(value):
                    return None
                try:
                    return int(value)
                except:
                    return None

            record = {
                "serial_number": convert_serial_number(row["åºå·"]),
                "record_date": data_date,  # ä½¿ç”¨ç»Ÿä¸€çš„æ—¥æœŸ
                "device_name": str(row["è®¾å¤‡åç§°"]) if not pd.isna(row["è®¾å¤‡åç§°"]) else None,
                "voltage_level": str(row["ç”µå‹ç­‰çº§(kV)"]) if not pd.isna(row["ç”µå‹ç­‰çº§(kV)"]) else None,
                "sheet_name": sheet_name
            }
            records.append(record)
            
        print(f"âœ… Sheet '{sheet_name}' è§£æå®Œæˆï¼Œå…± {len(records)} æ¡è®°å½•")
        return records

    def _process_4_as_channel(self, df, data_date, sheet_name):
        """
        å¤„ç†æœºç»„åŸºç¡€ä¿¡æ¯sheetï¼Œæå–æœºç»„ç¾¤ã€ç”µå‚å’Œæœºç»„ä¿¡æ¯
        """
        records = []
        df = df.dropna(how="all")  # åˆ é™¤ç©ºè¡Œ
        
        if df.empty:
            print(f"è­¦å‘Šï¼šsheet '{sheet_name}' æ— æœ‰æ•ˆæ•°æ®ï¼ˆæ‰€æœ‰è¡Œéƒ½æ˜¯ç©ºè¡Œï¼‰")
            return records  # è¿”å›ç©ºåˆ—è¡¨ï¼Œé¿å…åç»­æŠ¥é”™

        # ç¡®ä¿åˆ—åæ­£ç¡®
        df.columns = [str(c).strip() for c in df.columns]
        
        # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
        required_columns = ["æœºç»„ç¾¤å", "ç”µå‚ID", "ç”µå‚åç§°", "æœºç»„ID", "æœºç»„åç§°", "æ‰€å æ¯”ä¾‹"]
        if not all(col in df.columns for col in required_columns):
            print(f"âš ï¸  sheet '{sheet_name}' ç¼ºå°‘å¿…è¦çš„åˆ—: {required_columns}")
            return records

        # éå†æ¯ä¸€è¡Œæ•°æ®
        for _, row in df.iterrows():
            # è·³è¿‡ç©ºè¡Œ
            if pd.isna(row["æœºç»„ç¾¤å"]) and pd.isna(row["ç”µå‚ID"]) and pd.isna(row["æœºç»„ID"]):
                continue
                
            record = {
                "record_date": data_date,
                "unit_group_name": str(row["æœºç»„ç¾¤å"]) if not pd.isna(row["æœºç»„ç¾¤å"]) else None,
                "power_plant_id": str(row["ç”µå‚ID"]) if not pd.isna(row["ç”µå‚ID"]) else None,
                "power_plant_name": str(row["ç”µå‚åç§°"]) if not pd.isna(row["ç”µå‚åç§°"]) else None,
                "unit_id": str(row["æœºç»„ID"]) if not pd.isna(row["æœºç»„ID"]) else None,
                "unit_name": str(row["æœºç»„åç§°"]) if not pd.isna(row["æœºç»„åç§°"]) else None,
                "proportion": float(row["æ‰€å æ¯”ä¾‹"]) if not pd.isna(row["æ‰€å æ¯”ä¾‹"]) else None,
                "sheet_name": sheet_name
            }
            records.append(record)
            
        print(f"âœ… Sheet '{sheet_name}' è§£æå®Œæˆï¼Œå…± {len(records)} æ¡è®°å½•")
        return records

    def _process_5_channel(self, df, data_date, sheet_name):
        """
        å¤„ç†æœºç»„çº¦æŸä¿¡æ¯sheetï¼Œæå–æœºç»„ç¾¤çº¦æŸé…ç½®
        """
        records = []
        df = df.dropna(how="all")  # åˆ é™¤ç©ºè¡Œ
        
        if df.empty:
            print(f"è­¦å‘Šï¼šsheet '{sheet_name}' æ— æœ‰æ•ˆæ•°æ®ï¼ˆæ‰€æœ‰è¡Œéƒ½æ˜¯ç©ºè¡Œï¼‰")
            return records  # è¿”å›ç©ºåˆ—è¡¨ï¼Œé¿å…åç»­æŠ¥é”™

        # ç¡®ä¿åˆ—åæ­£ç¡®
        df.columns = [str(c).strip() for c in df.columns]
        
        # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
        required_columns = ["æœºç»„ç¾¤å", "ç”Ÿæ•ˆæ—¶é—´", "å¤±æ•ˆæ—¶é—´", "ç”µåŠ›çº¦æŸ", "ç”µé‡çº¦æŸ", "æœ€å¤§è¿è¡Œæ–¹å¼çº¦æŸ", "æœ€å°è¿è¡Œæ–¹å¼çº¦æŸ", "æœ€å¤§ç”µé‡", "æœ€å°ç”µé‡"]
        if not all(col in df.columns for col in required_columns):
            print(f"âš ï¸  sheet '{sheet_name}' ç¼ºå°‘å¿…è¦çš„åˆ—: {required_columns}")
            return records

        # éå†æ¯ä¸€è¡Œæ•°æ®
        for _, row in df.iterrows():
            # è·³è¿‡ç©ºè¡Œ
            if pd.isna(row["æœºç»„ç¾¤å"]) and pd.isna(row["ç”Ÿæ•ˆæ—¶é—´"]) and pd.isna(row["å¤±æ•ˆæ—¶é—´"]):
                continue
                
            # å¤„ç†çº¦æŸå­—æ®µï¼Œå°†"æ˜¯"/"å¦"è½¬æ¢ä¸º1/0
            def convert_constraint(value):
                if pd.isna(value):
                    return None
                if str(value).strip() == "æ˜¯":
                    return 1
                elif str(value).strip() == "å¦":
                    return 0
                else:
                    return None
                    
            # å¤„ç†æ•°å€¼å­—æ®µ
            def convert_numeric(value):
                if pd.isna(value):
                    return None
                try:
                    return float(value)
                except:
                    return None

            record = {
                "record_date": data_date,
                "unit_group_name": str(row["æœºç»„ç¾¤å"]) if not pd.isna(row["æœºç»„ç¾¤å"]) else None,
                "effective_time": str(row["ç”Ÿæ•ˆæ—¶é—´"]) if not pd.isna(row["ç”Ÿæ•ˆæ—¶é—´"]) else None,
                "expire_time": str(row["å¤±æ•ˆæ—¶é—´"]) if not pd.isna(row["å¤±æ•ˆæ—¶é—´"]) else None,
                "power_constraint": convert_constraint(row["ç”µåŠ›çº¦æŸ"]),
                "electricity_constraint": convert_constraint(row["ç”µé‡çº¦æŸ"]),
                "max_operation_constraint": convert_constraint(row["æœ€å¤§è¿è¡Œæ–¹å¼çº¦æŸ"]),
                "min_operation_constraint": convert_constraint(row["æœ€å°è¿è¡Œæ–¹å¼çº¦æŸ"]),
                "max_electricity": convert_numeric(row["æœ€å¤§ç”µé‡"]),
                "min_electricity": convert_numeric(row["æœ€å°ç”µé‡"]),
                "sheet_name": sheet_name
            }
            records.append(record)
            
        print(f"âœ… Sheet '{sheet_name}' è§£æå®Œæˆï¼Œå…± {len(records)} æ¡è®°å½•")
        return records

    def _process_5_as_channel(self, df, data_date, sheet_name, data_type):
        """å°†æ—¶åˆ»åˆ—åæ˜ å°„ä¸ºchannel_name"""
        records = []
        df = df.dropna(how="all")  # åˆ é™¤ç©ºè¡Œ
        if df.empty:
            print(f"è­¦å‘Šï¼šsheet '{sheet_name}' æ— æœ‰æ•ˆæ•°æ®ï¼ˆæ‰€æœ‰è¡Œéƒ½æ˜¯ç©ºè¡Œï¼‰")
            return records  # è¿”å›ç©ºåˆ—è¡¨ï¼Œé¿å…åç»­æŠ¥é”™
        
        first_col = df.columns[0]
        second_col = df.columns[1]
       
        # æŸ¥æ‰¾æ—¶é—´åˆ—ï¼ˆå½¢å¦‚ 00:00ã€01:15 æˆ–æ•°å­—æ ¼å¼ 0, 1, 2...ï¼‰
        time_cols = [c for c in df.columns if re.match(r"\d{2}:\d{2}", c)]
        if not time_cols:
            print(f"âš ï¸ æœªæ‰¾åˆ°æ—¶é—´åˆ—: {df.columns.tolist()}")
            return []

        # éå†æ¯ä¸€è¡Œï¼ˆæ¯ä¸€ç±»æŒ‡æ ‡ï¼‰
        for _, row in df.iterrows():
            # è·³è¿‡æ— æ•ˆè¡Œæˆ–æ ‡é¢˜è¡Œ
            if not isinstance(row[time_cols[0]], (int, float)) and not str(row[time_cols[0]]).replace('.', '', 1).isdigit():
                continue
            
            # ç”Ÿæˆ channel_nameï¼šç¬¬ä¸€åˆ—å’Œç¬¬äºŒåˆ—ç”¨ä¸‹åˆ’çº¿è¿æ¥
            channel_name = f"{row[first_col]}_{row[second_col]}"

            for t in time_cols:
                value = row[t]
                if pd.isna(value):
                    continue
                try:
                    value = float(value)
                except:
                    continue  # è·³è¿‡éæ•°å€¼çš„å•å…ƒæ ¼

                record = {
                    "record_date": data_date,
                    "record_time": t,
                    "channel_name": channel_name,  # ç”¨æŒ‡æ ‡åä½œé€šé“å
                    "value": value,
                    "type": data_type,
                    "sheet_name": sheet_name,
                    "created_at": datetime.datetime.now(),
                }
                records.append(record)
        return records

    def _process_first_row_as_channel(self, df, data_date, sheet_name, data_type):
        """
        å¤„ç†æ ¼å¼ï¼š
        æœ€é«˜è´Ÿè·(MW) | æœ€ä½è´Ÿè·(MW) | å¹³å‡è´Ÿè·(MW)
        243330.375    | 182924.0156  | 212967.9509
        """
        records = []
        # åˆ é™¤ç©ºè¡Œä¸ç©ºåˆ—
        df = df.dropna(how="all").dropna(axis=1, how="all")
        if df.empty:
            print(f"âš ï¸ Sheet {sheet_name} ä¸ºç©ºï¼Œè·³è¿‡ã€‚")
            return records

        # ç¬¬ä¸€è¡Œä½œä¸º channel_name
        channel_names = [str(c).strip() for c in df.iloc[0].tolist()]
        df = df.iloc[1:]  # ä»ç¬¬äºŒè¡Œå¼€å§‹ä¸ºæ•°æ®
        if df.empty:
            print(f"âš ï¸ Sheet {sheet_name} ä»…æœ‰è¡¨å¤´ï¼Œæ— æ•°æ®ã€‚")
            return records

        for _, row in df.iterrows():
            for col_idx, value in enumerate(row):
                if col_idx >= len(channel_names):
                    continue
                if pd.isna(value):
                    continue
                record = {
                    "record_date": data_date,
                    "record_time": None,  # æ²¡æœ‰æ—¶é—´åˆ—
                    "channel_name": channel_names[col_idx],
                    "value": value,
                    "type": data_type,
                    "sheet_name": sheet_name,
                    "created_at": datetime.datetime.now(),
                }
                records.append(record)

        print(f"âœ… Sheet {sheet_name} è§£æå®Œæˆï¼Œå…± {len(records)} æ¡è®°å½•ã€‚")
        return records

    def import_custom_excel_pred(self, excel_file):
            """å¯¼å…¥æŒ‡å®šçš„5ä¸ªsheetï¼Œå¹¶æŒ‰å›ºå®šè§„åˆ™æ˜ å°„"""
            try:
                # è¯»å–æ‰€æœ‰sheet
                sheet_dict = pd.read_excel(excel_file, sheet_name=None, header=0)
            except Exception as e:
                print(f"âŒ æ— æ³•è¯»å–Excel: {e}")
                return False
            file_name = str(excel_file)
            single_match = re.search(r"\((\d{4}-\d{1,2}-\d{1,2})", file_name)
            single_data_date_str = single_match.group(1)
            single_data_date = datetime.datetime.strptime(single_data_date_str, "%Y-%m-%d").date()
            print("è¯†åˆ«åˆ°çš„æ—¥æœŸï¼š", single_data_date)
            chinese_match = re.search(r'([\u4e00-\u9fff]+)', file_name)
            if chinese_match:
                data_type = chinese_match.group(1) + "é¢„æµ‹ä¿¡æ¯"
                print(f"ğŸ“ æ–‡ä»¶ç±»å‹è¯†åˆ«: {data_type}")
            else:
                print(f"âš ï¸ æœªèƒ½åœ¨æ–‡ä»¶åä¸­æ‰¾åˆ°æ±‰å­—ï¼š{file_name}ï¼Œè·³è¿‡ã€‚")
                return False
            sheet_names = list(sheet_dict.keys())
            print(f"ğŸ“˜ æ£€æµ‹åˆ° {len(sheet_names)} ä¸ªSheet: {sheet_names}")

            # è¦å¤„ç†çš„sheetç¼–å·ï¼ˆ1-basedï¼‰
            target_indexes = [0, 1, 2,3,4, 5,6,7,-5,-4,-3, -2, -1]  # å¯¹åº”ç¬¬1,2,4,5,6ä¸ªsheet

            all_records = []
            jichu_records = []
            yueshu_records = []
            ynjichu_records = []
            jizujichu_records = []
            jizuyueshu_records = []
            ynyueshu_records = []
            shubiandian_records = []

            for i in target_indexes:
                if i >= len(sheet_names):
                    print(f"âš ï¸ Excelä¸­ä¸å­˜åœ¨ç¬¬{i+1}ä¸ªsheetï¼Œè·³è¿‡")
                    continue

                sheet_name = sheet_names[i]
                df = sheet_dict[sheet_name]
                print(f"\nğŸ”¹ æ­£åœ¨å¤„ç† Sheet {i+1}: {sheet_name}")

                # ç»Ÿä¸€è¯†åˆ«æ—¥æœŸ
                match = re.search(r"\((\d{4}-\d{1,2}-\d{1,2})", sheet_name)

                if match:
                    # æå–æ•è·çš„æ—¥æœŸå­—ç¬¦ä¸²å¹¶è½¬æ¢ä¸ºdateç±»å‹
                    data_date_str = match.group(1)
                    data_date = datetime.datetime.strptime(data_date_str, "%Y-%m-%d").date()
                    print("è¯†åˆ«åˆ°çš„æ—¥æœŸï¼š", data_date)  # è¾“å‡ºï¼šè¯†åˆ«åˆ°çš„æ—¥æœŸï¼š2025-09-01ï¼ˆè‹¥è¾“å…¥æ˜¯2025-09-1ï¼Œä¼šè‡ªåŠ¨è¡¥0ä¸º2025-09-01ï¼‰
                else:
                    print("æœªè¯†åˆ«åˆ°æ—¥æœŸæ ¼å¼")

                # æ ¹æ®sheetåºå·è°ƒç”¨ä¸åŒæ˜ å°„å‡½æ•°
                if i in [0]:  # ç¬¬1ä¸ªsheetï¼šæ—¶åˆ»â†’channel_name
                    records = self._process_time_as_channel(df, data_date, sheet_name, data_type)
                elif i in [1]: 
                    records = self._process_1_channel(df, data_date, sheet_name, data_type)
                elif i in [2]:  # ç¬¬3ä¸ªsheetï¼šæ—¶åˆ»â†’channel_name
                    records = self._process_type_date_value(df, data_date, sheet_name, data_type)
                elif i in [3]: 
                    shubiandian_records = self._process_3_as_channel(df, data_date, sheet_name)
                elif i in [4]:  # ç¬¬4ä¸ªsheetï¼šç¬¬ä¸€è¡Œâ†’channel_name
                    jizujichu_records = self._process_4_as_channel(df, data_date, sheet_name)
                elif i in [5]:  # ç¬¬5ä¸ªsheetï¼šæ—¶åˆ»â†’channel_name
                    jizuyueshu_records = self._process_5_channel(df, data_date, sheet_name)
                elif i in [-5]:  # ç¬¬6ä¸ªsheetï¼šæ—¶åˆ»â†’channel_name
                    ynyueshu_records = self._process_5_channel(df, single_data_date, sheet_name)
                elif i in [-3]:  # ç¬¬4,5ä¸ªsheetï¼šæ—¶åˆ»â†’channel_name
                    records = self._process_3_channel(df, data_date, sheet_name, data_type)
                elif i in [-2, -1]:  # ç¬¬7,8ä¸ªsheetï¼šç¬¬ä¸€è¡Œâ†’channel_name
                    records = self._process_2_channel(df, data_date, sheet_name, data_type)
                elif i in [-4,6]:  # ç¬¬9ä¸ªsheet
                    records = self._process_5_as_channel(df, single_data_date, sheet_name, data_type)
                elif i in [7]:
                    ynjichu_records = self._process_4_as_channel(df, single_data_date, sheet_name)
                
                else:
                    print(f"âš ï¸ ç¬¬{i+1}ä¸ªsheetæœªå®šä¹‰å¤„ç†è§„åˆ™ï¼Œè·³è¿‡")
                    continue

                print(f"âœ… Sheet{i+1} å¤„ç†å®Œæˆï¼Œå…± {len(records)} æ¡è®°å½•")
                all_records.extend(records)
               
               
            jichu_records.extend(ynjichu_records)
            jichu_records.extend(jizujichu_records)
            yueshu_records.extend(jizuyueshu_records)
            yueshu_records.extend(ynyueshu_records)
                
            if not all_records:
                print("âŒ æ²¡æœ‰ç”Ÿæˆä»»ä½•æœ‰æ•ˆè®°å½•")
                return False
           
            success1, table_name1, count1, preview_data1 = self.save_to_database(all_records, data_date)
            success2, table_name2, count2, preview_data2 = self.save_to_jizujichu_database(jichu_records, data_date)
            success4, table_name4, count4, preview_data4 = self.save_to_jizuyueshu_database(yueshu_records, data_date)
            success5, table_name5, count5, preview_data5 = self.save_to_shubiandian_database(shubiandian_records, data_date)

            # è¿”å›ä¸¤ä¸ªæ“ä½œçš„ç»“æœ
            return (success1, table_name1, count1, preview_data1), (success2, table_name2, count2, preview_data2), (success4, table_name4, count4, preview_data4), (success5, table_name5, count5, preview_data5)

            # return self.save_to_database(all_records, data_date)
    
    def _process_1_channel(self, df, data_date, sheet_name, data_type):
        """
        å¤šæŒ‡æ ‡æ—¶åˆ»å‹sheetå¤„ç†ï¼š
        - è¯†åˆ«â€œç±»å‹ + ç”µæºç±»å‹â€ä¸º channel_name
        - ä½¿ç”¨â€œæ—¥æœŸâ€åˆ—ä½œä¸º record_date
        - æ—¶é—´åˆ—ä¸º 00:00ã€00:15 ç­‰å¸¸è§„æ ¼å¼
        """
        import datetime
        import pandas as pd
        import re

        records = []
        df = df.dropna(how="all")
        df.columns = [str(c).strip() for c in df.columns]

        # 1ï¸âƒ£ æ‰¾æ—¶é—´åˆ—ï¼ˆå¦‚ 00:00ã€01:15 ç­‰ï¼‰
        time_cols = [c for c in df.columns if re.match(r"^\d{1,2}:\d{2}$", c)]
        if not time_cols:
            print(f"âš ï¸ æœªæ‰¾åˆ°æ—¶é—´åˆ—: {df.columns.tolist()}")
            return []

        # 2ï¸âƒ£ è¯†åˆ«è¾…åŠ©åˆ—
        col_type = "ç±»å‹" if "ç±»å‹" in df.columns else None
        col_date = "æ—¥æœŸ" if "æ—¥æœŸ" in df.columns else None
        col_power = "ç”µæºç±»å‹" if "ç”µæºç±»å‹" in df.columns else None

        # 3ï¸âƒ£ éå†æ¯ä¸€è¡Œï¼ˆæ¯ä¸ªé€šé“ï¼‰
        for _, row in df.iterrows():
            # --- æ—¥æœŸåˆ— ---
            record_date = data_date
            if col_date and pd.notna(row[col_date]):
                try:
                    # è‡ªåŠ¨è¯†åˆ«æ—¥æœŸæ ¼å¼
                    record_date = pd.to_datetime(str(row[col_date]), errors="coerce").date()
                except:
                    record_date = data_date

            # --- é€šé“åï¼šç±»å‹ + ç”µæºç±»å‹ ---
            parts = []
            if col_type and pd.notna(row[col_type]):
                parts.append(str(row[col_type]).strip())
            if col_power and pd.notna(row[col_power]):
                parts.append(str(row[col_power]).strip())
            if not parts:
                continue
            channel_name = "-".join(parts)

            # --- éå†æ—¶é—´åˆ— ---
            for t in time_cols:
                value = row[t]
                if pd.isna(value):
                    continue
                try:
                    value = float(value)
                except:
                    continue

                records.append({
                    "record_date": record_date,        # ç¡®ä¿æ˜¯ date ç±»å‹
                    "record_time": t,                  # å¦‚ 00:00
                    "channel_name": channel_name,      # å¦‚ "ç°è´§æ–°èƒ½æºæ€»å‡ºåŠ›(MW)-é£ç”µ"
                    "value": value,
                    "type": data_type,
                    "sheet_name": sheet_name,
                    "created_at": datetime.datetime.now(),
                })

        print(f"âœ… {sheet_name} è§£æå®Œæˆï¼Œå…± {len(records)} æ¡è®°å½•")
        return records

    def _process_3_channel(self, df, data_date, sheet_name, data_type):
        """
        å°†å¤šåˆ—é€šé“å‹Sheetå¤„ç†æˆè®°å½•åˆ—è¡¨ï¼Œæ¯åˆ—è§†ä¸ºä¸€ä¸ªé€šé“ã€‚
        ç»“æ„ç¤ºä¾‹ï¼š
        åºå· | æ—¥æœŸ | å¿…å¼€æœºç»„å®¹é‡(MW) | å¿…åœæœºç»„å®¹é‡(MW)
        """
        import datetime
        import pandas as pd

        print(f"ğŸ”¹ æ­£åœ¨å¤„ç† Sheet: {sheet_name}")

        records = []

        # 1ï¸âƒ£ åˆ é™¤æ— ç”¨åˆ—
        if "åºå·" in df.columns:
            df = df.drop(columns=["åºå·"])

        # 2ï¸âƒ£ ç¡®ä¿æ—¥æœŸå­—æ®µå­˜åœ¨
        if "æ—¥æœŸ" not in df.columns:
            print(f"âš ï¸ æœªæ‰¾åˆ°æ—¥æœŸåˆ—ï¼Œè·³è¿‡ {sheet_name}")
            return []

        df = df.dropna(how="all")
        df.columns = [str(c).strip() for c in df.columns]

        # 3ï¸âƒ£ éå†æ¯ä¸€è¡Œ
        for _, row in df.iterrows():
            # æ—¥æœŸ
            record_date = data_date
            if pd.notna(row["æ—¥æœŸ"]):
                try:
                    record_date = pd.to_datetime(str(row["æ—¥æœŸ"]), errors="coerce").date()
                except:
                    record_date = data_date

            # 4ï¸âƒ£ éå†é€šé“åˆ—ï¼ˆé™¤â€œæ—¥æœŸâ€å¤–ï¼‰
            for col in df.columns:
                if col in ["æ—¥æœŸ"]:
                    continue
                value = row[col]
                if pd.isna(value):
                    continue
                try:
                    value = float(value)
                except:
                    continue

                records.append({
                    "record_date": record_date,
                    "record_time": None,
                    "channel_name": col,
                    "value": value,
                    "type": data_type,
                    "sheet_name": sheet_name,
                    "created_at": datetime.datetime.now(),
                })

        print(f"âœ… {sheet_name} å¤„ç†å®Œæˆï¼Œå…± {len(records)} æ¡è®°å½•")
        return records
    
    def _process_2_channel(self, df, data_date, sheet_name, data_type):
        """
        å¤„ç†æœºç»„åå•è¡¨ï¼š
        - channel_name = ç”µå‚åç§°-æœºç»„åç§°-ç±»å‹
        - value é»˜è®¤ä¸º 1
        """
        import datetime
        import pandas as pd

        records = []

        if df.empty:
            print(f"âš ï¸ {sheet_name} è¡¨ä¸ºç©ºï¼Œè·³è¿‡")
            return []

        df = df.dropna(how="all")
        df.columns = [str(c).strip() for c in df.columns]

        # å¿…è¦åˆ—
        col_date = "æ—¥æœŸ" if "æ—¥æœŸ" in df.columns else None
        col_plant = "ç”µå‚åç§°" if "ç”µå‚åç§°" in df.columns else None
        col_unit = "æœºç»„åç§°" if "æœºç»„åç§°" in df.columns else None
        col_type = "ç±»å‹" if "ç±»å‹" in df.columns else None

        for _, row in df.iterrows():
            # æ—¥æœŸ
            record_date = data_date
            if col_date and pd.notna(row[col_date]):
                try:
                    record_date = pd.to_datetime(str(row[col_date]), errors="coerce").date()
                except:
                    record_date = data_date

            # channel_name æ‹¼æ¥
            parts = []
            for col in [col_plant, col_unit, col_type]:
                if col and pd.notna(row[col]):
                    parts.append(str(row[col]).strip())
            if not parts:
                continue
            channel_name = "-".join(parts)

            # æ·»åŠ è®°å½•
            records.append({
                "record_date": record_date,
                "channel_name": channel_name,
                "record_time": None,
                "value": None,
                "type": data_type,
                "sheet_name": sheet_name,
                "created_at": datetime.datetime.now(),
            })

        print(f"âœ… {sheet_name} å¤„ç†å®Œæˆï¼Œå…± {len(records)} æ¡è®°å½•")
        return records

    def _process_type_date_value(self, df, data_date, sheet_name, data_type):
        """å¤„ç†ç±»ä¼¼ 'ç±»å‹ æ—¥æœŸ æ•°å€¼' çš„ç»“æ„ï¼ˆæ— æ—¶é—´åˆ—ï¼Œrecord_dateä¸ºdateç±»å‹ï¼‰"""
        records = []
        df = df.dropna(how="all")
        df.columns = [str(c).strip() for c in df.columns]

        # æŸ¥æ‰¾åˆ—
        col_type = "ç±»å‹" if "ç±»å‹" in df.columns else None
        col_date = "æ—¥æœŸ" if "æ—¥æœŸ" in df.columns else None

        # æŸ¥æ‰¾æ•°å€¼åˆ—ï¼ˆæ’é™¤æ‰å·²çŸ¥åˆ—ï¼‰
        value_cols = [c for c in df.columns if c not in [col_type, col_date]]
        if not value_cols:
            print(f"âš ï¸ æœªæ‰¾åˆ°æ•°å€¼åˆ—: {df.columns.tolist()}")
            return []

        value_col = value_cols[0]  # é»˜è®¤åªå–ç¬¬ä¸€åˆ—æ•°å€¼

        for _, row in df.iterrows():
            channel_name = str(row[col_type]).strip() if col_type else "æœªçŸ¥ç±»å‹"
            raw_date = str(row[col_date]).strip() if col_date and pd.notna(row[col_date]) else None

            # === æ—¥æœŸè§£æé€»è¾‘ ===
            parsed_date = None
            if raw_date:
                # 1. å¦‚æœæ˜¯æ ‡å‡†æ—¥æœŸæ ¼å¼
                try:
                    parsed_date = pd.to_datetime(raw_date).date()
                except Exception:
                    pass

                # 2. å¦‚æœæ˜¯å½¢å¦‚ â€œ2025å¹´ç¬¬38å‘¨(09.15~09.21)â€
                if parsed_date is None:
                    match = re.search(r"\((\d{2})\.(\d{2})", raw_date)
                    year_match = re.search(r"(\d{4})å¹´", raw_date)
                    if match and year_match:
                        year = int(year_match.group(1))
                        month = int(match.group(1))
                        day = int(match.group(2))
                        parsed_date = datetime.date(year, month, day)

            # å¦‚æœéƒ½è§£æå¤±è´¥ï¼Œåˆ™ç”¨ data_date å…œåº•
            if parsed_date is None:
                parsed_date = pd.to_datetime(data_date).date()

            # æ•°å€¼
            try:
                value = float(row[value_col])
            except:
                continue

            record = {
                "record_date": parsed_date,
                "record_time": datetime.datetime.now().time(),
                "channel_name": channel_name,
                "value": value,
                "type": data_type,
                "sheet_name": sheet_name,
                "created_at": datetime.datetime.now(),
            }
            records.append(record)

        return records
    
    def import_point_data(self, excel_file):
        """è‡ªåŠ¨å¯¼å…¥Excelç¬¬ä¸€ä¸ªSheetçš„æ•°æ®ï¼Œå¹¶æŒ‰åˆ—æ±‚å‡å€¼"""
        import re
        import datetime
        import pandas as pd

        try:
            xls = pd.ExcelFile(excel_file)
            first_sheet_name = xls.sheet_names[0]  # âœ… è·å–ç¬¬ä¸€ä¸ª sheet å
            df = pd.read_excel(excel_file, sheet_name=first_sheet_name, header=0)
            print(f"âœ… æˆåŠŸè¯»å– Excel: {excel_file}, sheet: {first_sheet_name}")
        except Exception as e:
            print(f"âŒ è¯»å– Excel å¤±è´¥: {e}")
            return False, None, 0, []

        # è‡ªåŠ¨è¯†åˆ«æ—¥æœŸ
        # é¦–å…ˆå°è¯•åŒ¹é…æ‹¬å·ä¸­çš„æ—¥æœŸæ ¼å¼ "(2025-09-29)"
        match = re.search(r"\((\d{4}-\d{2}-\d{2})\)", first_sheet_name)
        if match:
            data_date = datetime.datetime.strptime(match.group(1), "%Y-%m-%d").date()
        else:
            # å¦‚æœæ²¡æœ‰æ‹¬å·ï¼Œåˆ™å°è¯•ç›´æ¥åŒ¹é…æ—¥æœŸæ ¼å¼ "2025-09-29"
            match = re.search(r"(\d{4}-\d{2}-\d{2})", first_sheet_name)
            if match:
                data_date = datetime.datetime.strptime(match.group(1), "%Y-%m-%d").date()
            else:
                print(f"âŒ æ— æ³•ä» sheet åç§° '{first_sheet_name}' ä¸­æå–æ—¥æœŸ")
                return False, None, 0, []

        # æ ¹æ®æ–‡ä»¶åè¯†åˆ«ç±»å‹
        file_name = str(excel_file)
        chinese_match = re.search(r'([\u4e00-\u9fff]+)', file_name)
        if chinese_match:
            data_type = chinese_match.group(1)
            print(f"ğŸ“ æ–‡ä»¶ç±»å‹è¯†åˆ«: {data_type}")
        else:
            print(f"âš ï¸ æœªèƒ½åœ¨æ–‡ä»¶åä¸­æ‰¾åˆ°æ±‰å­—ï¼š{file_name}ï¼Œè·³è¿‡ã€‚")
            return False, None, 0, []
        data_type = "å¹¿ä¸œ_" + data_type
        print(f"\nğŸ“˜ æ­£åœ¨å¤„ç† {first_sheet_name} | æ—¥æœŸ: {data_date} | ç±»å‹: {data_type}")
    
        # æŒ‰åˆ—æ±‚å‡å€¼å¹¶ç”Ÿæˆ records
        records = self.process_mean_by_column(df, data_date, first_sheet_name, data_type)

        if not records:
            print("âŒ æ²¡æœ‰ä»»ä½•æœ‰æ•ˆæ•°æ®è¢«å¯¼å…¥")
            return False, None, 0, []

        # ä¿å­˜åˆ°æ•°æ®åº“
        success, table_name, record_count, preview_data = self.save_to_database(records, data_date)
        print(f"âœ… æ•°æ®ä¿å­˜æˆåŠŸï¼Œè¡¨å: {table_name}ï¼Œè®°å½•æ•°: {record_count}")
        return success, table_name, record_count, preview_data
    def process_mean_by_column(self, df, data_date, sheet_name, data_type):
        """
        é’ˆå¯¹èŠ‚ç‚¹ç”µä»·ç­‰è¡¨æ ¼ï¼šå¯¹æ¯ä¸€åˆ—ï¼ˆä»ç¬¬3åˆ—å¼€å§‹ï¼‰æ±‚å‡å€¼ï¼Œå¹¶ç”Ÿæˆè®°å½•
        æ¯ä¸€åˆ—ã®å‡å€¤ãƒ‡ãƒ¼ã‚¿æ”¾åœ¨æœ€å¾Œï¼Œå…¶ä»–ãƒ‡ãƒ¼ã‚¿æŒ‰é †åºéƒ½å­˜ä¸€ä¸‹
        """
        records = []

        # æ ‡å‡†åŒ–åˆ—å
        df.columns = [str(c).strip() for c in df.columns]
        # print(f"COLUMNS: {df.columns.tolist()}")

        # è·å–æ—¶é—´åˆ—ï¼ˆç¬¬3åˆ—åŠä¹‹åï¼‰
        time_cols = df.columns[2:]
        if time_cols.empty or len(time_cols) == 0:
            print(f"âš ï¸ Sheet {sheet_name} æ²¡æœ‰å‘ç°æ—¶é—´åˆ—")
            return records

        # å°†æ—¶é—´åˆ—æŒ‰æ¯4ä¸ªåˆ†ç»„ï¼ˆæ¯å°æ—¶4ä¸ª15åˆ†é’Ÿé—´éš”ï¼‰
        time_groups = {}
        for t in time_cols:
            # ä» "HH:MM" æ ¼å¼ä¸­æå–å°æ—¶
            hour = t.split(':')[0]
            if hour not in time_groups:
                time_groups[hour] = []
            time_groups[hour].append(t)

        # å…ˆä¿å­˜åŸæœ‰çš„æ•°æ®ï¼ˆæŒ‰å°æ—¶åˆ†ç»„ï¼‰
        # é¢„å…ˆè®¡ç®—æ¯è¡Œæ¯å°æ—¶çš„å‡å€¼
        hourly_means = {}  # {(row_index, hour): mean_value}
        
        for _, row in df.iterrows():
            # æ£€æŸ¥ç¬¬ä¸€åˆ—æ˜¯å¦æœ‰æœ‰æ•ˆæ•°æ®ï¼Œå¦‚æœæ²¡æœ‰åˆ™è·³è¿‡ï¼ˆå¤„ç†æ ‡é¢˜è¡Œï¼‰
            channel_name = row.iloc[0]  # ç¬¬ä¸€åˆ—ä½œä¸ºé€šé“åç§°
            if pd.isna(channel_name) or channel_name == "":
                continue
                
            # ä¸ºæ¯è¡Œæ¯å°æ—¶è®¡ç®—å‡å€¼
            for hour, times in time_groups.items():
                # è®¡ç®—è¯¥å°æ—¶å†…å››ä¸ªæ—¶é—´ç‚¹çš„å‡å€¼
                values = []
                for t in times:
                    value = row[t]
                    if not pd.isna(value):
                        values.append(value)
                
                # å¦‚æœæœ‰æœ‰æ•ˆå€¼ï¼Œåˆ™è®¡ç®—å‡å€¼
                if values:
                    hourly_mean = sum(values) / len(values)
                    hourly_means[(_, hour)] = hourly_mean
                    
                    record = {
                        "record_date": pd.to_datetime(data_date).date(),
                        "record_time": f"{hour}:00",  # æŒ‰å°æ—¶å­˜å‚¨
                        "channel_name": channel_name,
                        "value": round(hourly_mean, 2),  # ä½¿ç”¨è¯¥å°æ—¶å†…å››ä¸ªæ—¶é—´ç‚¹çš„å‡å€¤
                        "type": "å¹¿ä¸œ_"+data_type,
                        "sheet_name": sheet_name,
                        "created_at": pd.Timestamp.now(),
                    }
                    records.append(record)

        # å†æ·»åŠ æ¯å°æ—¶çš„å‡å€¤ãƒ‡ãƒ¼ã‚¿ï¼ˆæ‰€æœ‰è¡Œåœ¨è¯¥å°æ—¶çš„å‡å€¤ï¼‰
        for hour, times in time_groups.items():
            # è·å–è¿™äº›æ—¶é—´ç‚¹çš„å€¤å¹¶è¨ˆç®—å‡å€¤
            values = []
            for t in times:
                # è¨ˆç®—è©²æ™‚é–“ç‚¹åœ¨æ‰€æœ‰è¡Œä¸­çš„å‡å€¤
                mean_value = df[t].mean()
                values.append(mean_value)
            
            # è¨ˆç®—4ã¤ã®æ™‚é–“ç‚¹ã®ç·å‡å€¤
            if values:
                overall_mean = sum(values) / len(values)
                record = {
                    "record_date": pd.to_datetime(data_date).date(),
                    "record_time": f"{hour}:00",   # "HH:00" ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
                    "channel_name": f"{data_type}_å‡å€¤",
                    "value": round(overall_mean, 2),
                    "type": "å¹¿ä¸œ_"+data_type,
                    "sheet_name": sheet_name,
                    "created_at": pd.Timestamp.now(),
                }
                records.append(record)

        print(f"âœ… {data_type} å‡å€¤ç”Ÿæˆ {len(records)} æ¡è¨˜éŒ„")
        return records

    def import_point_data_new(self, excel_file):
        """è‡ªåŠ¨å¯¼å…¥Excelç¬¬ä¸€ä¸ªSheetçš„æ•°æ®ï¼Œå¹¶æŒ‰åˆ—æ±‚å‡å€¼"""
        import re
        import datetime
        import pandas as pd

        try:
            xls = pd.ExcelFile(excel_file)
            first_sheet_name = xls.sheet_names[0]  # âœ… è·å–ç¬¬ä¸€ä¸ª sheet å
            df = pd.read_excel(excel_file, sheet_name=first_sheet_name, header=1)
            print(f"âœ… æˆåŠŸè¯»å– Excel: {excel_file}, sheet: {first_sheet_name}")
        except Exception as e:
            print(f"âŒ è¯»å– Excel å¤±è´¥: {e}")
            return False, None, 0, []
        
        # è‡ªåŠ¨è¯†åˆ«æ—¥æœŸ
        # é¦–å…ˆå°è¯•åŒ¹é…æ‹¬å·ä¸­çš„æ—¥æœŸæ ¼å¼ "(2025-09-29)"
        match = re.search(r"\((\d{4}-\d{2}-\d{2})\)", first_sheet_name)
        if match:
            data_date = datetime.datetime.strptime(match.group(1), "%Y-%m-%d").date()
        else:
            # å¦‚æœæ²¡æœ‰æ‹¬å·ï¼Œåˆ™å°è¯•ç›´æ¥åŒ¹é…æ—¥æœŸæ ¼å¼ "2025-09-29"
            match = re.search(r"(\d{4}-\d{2}-\d{2})", first_sheet_name)
            if match:
                data_date = datetime.datetime.strptime(match.group(1), "%Y-%m-%d").date()
            else:
                print(f"âŒ æ— æ³•ä» sheet åç§° '{first_sheet_name}' ä¸­æå–æ—¥æœŸ")
                return False, None, 0, []

        # æ ¹æ®æ–‡ä»¶åè¯†åˆ«ç±»å‹
        file_name = str(excel_file)
        chinese_match = re.search(r'([\u4e00-\u9fff]+)', file_name)
        if chinese_match:
            data_type = chinese_match.group(1)
            print(f"ğŸ“ æ–‡ä»¶ç±»å‹è¯†åˆ«: {data_type}")
        else:
            print(f"âš ï¸ æœªèƒ½åœ¨æ–‡ä»¶åä¸­æ‰¾åˆ°æ±‰å­—ï¼š{file_name}ï¼Œè·³è¿‡ã€‚")
            return False, None, 0, []

        print(f"\nğŸ“˜ æ­£åœ¨å¤„ç† {first_sheet_name} | æ—¥æœŸ: {data_date} | ç±»å‹: {data_type}")

        # æŒ‰åˆ—æ±‚å‡å€¼å¹¶ç”Ÿæˆ records
        records = self.process_point_new(df, data_date, first_sheet_name, data_type)

        if not records:
            print("âŒ æ²¡æœ‰ä»»ä½•æœ‰æ•ˆæ•°æ®è¢«å¯¼å…¥")
            return False, None, 0, []

        # ä¿å­˜åˆ°æ•°æ®åº“
        success, table_name, record_count, preview_data = self.save_to_database(records, data_date)
        print(f"âœ… æ•°æ®ä¿å­˜æˆåŠŸï¼Œè¡¨å: {table_name}ï¼Œè®°å½•æ•°: {record_count}")
        return success, table_name, record_count, preview_data
    
    def process_point_new(self, df, data_date, sheet_name, data_type):
        """
        é’ˆå¯¹èŠ‚ç‚¹ç”µä»·ç­‰è¡¨æ ¼ï¼šæŒ‰åŒºåŸŸåˆ’åˆ†è®¡ç®—æ¯å°æ—¶å‡å€¼
        """
        records = []

        # æ ‡å‡†åŒ–åˆ—å
        df.columns = [str(c).strip() for c in df.columns]

        # è·å–æ—¶é—´åˆ—ï¼ˆç¬¬3åˆ—åŠä¹‹åï¼‰
        time_cols = df.columns[2:]
        if time_cols.empty or len(time_cols) == 0:
            print(f"âš ï¸ Sheet {sheet_name} æ²¡æœ‰å‘ç°æ—¶é—´åˆ—")
            return records

        # å°†æ—¶é—´åˆ—æŒ‰æ¯4ä¸ªåˆ†ç»„ï¼ˆæ¯å°æ—¶4ä¸ª15åˆ†é’Ÿé—´éš”ï¼‰
        time_groups = {}
        for t in time_cols:
            hour = t.split(':')[0]
            if hour not in time_groups:
                time_groups[hour] = []
            time_groups[hour].append(t)

        # å…ˆä¿å­˜åŸæœ‰çš„æ•°æ®ï¼ˆæŒ‰å°æ—¶åˆ†ç»„ï¼‰
        for _, row in df.iterrows():
            region_name = row.iloc[0]
            region_name_clean = str(region_name).strip()

            # åªå¤„ç†å¹¿ä¸œå’Œäº‘å—ï¼Œæ’é™¤å…¶ä»–åœ°åŒº
            if "å¹¿ä¸œ" not in region_name_clean and "äº‘å—" not in region_name_clean:
                continue
                
            # æ£€æŸ¥ç¬¬ä¸€åˆ—æ˜¯å¦æœ‰æœ‰æ•ˆæ•°æ®
            channel_name = row.iloc[1]
            if pd.isna(channel_name) or channel_name == "":
                continue
                
            # ä¸ºæ¯è¡Œæ¯å°æ—¶è®¡ç®—å‡å€¼
            for hour, times in time_groups.items():
                values = []
                for t in times:
                    value = row[t]
                    if not pd.isna(value):
                        values.append(value)
                
                if values:
                    hourly_mean = sum(values) / len(values)
                    record = {
                        "record_date": pd.to_datetime(data_date).date(),
                        "record_time": f"{hour}:00",
                        "channel_name": channel_name,
                        "value": round(hourly_mean, 2),
                        "type": region_name + "_" + data_type,
                        "sheet_name": sheet_name,
                        "created_at": pd.Timestamp.now(),
                    }
                    records.append(record)

        # æŒ‰åŒºåŸŸåˆ†ç»„è®¡ç®—æ¯å°æ—¶çš„å‡å€¼
        region_groups = {}

        # å…ˆæŒ‰åŒºåŸŸåˆ†ç»„æ•°æ®
        for _, row in df.iterrows():
            region_name = row.iloc[0]
            region_name_clean = str(region_name).strip()

            # åªå¤„ç†å¹¿ä¸œå’Œäº‘å—ï¼Œæ’é™¤å…¶ä»–åœ°åŒº
            if "å¹¿ä¸œ" not in region_name_clean and "äº‘å—" not in region_name_clean:
                continue
                
            # æ£€æŸ¥ç¬¬ä¸€åˆ—æ˜¯å¦æœ‰æœ‰æ•ˆæ•°æ®
            channel_name = row.iloc[1]
            if pd.isna(channel_name) or channel_name == "":
                continue
            
            # åˆå§‹åŒ–åœ°åŒºåˆ†ç»„
            if region_name not in region_groups:
                region_groups[region_name] = []
            
            region_groups[region_name].append(row)

        # ä¸ºæ¯ä¸ªåŒºåŸŸè®¡ç®—æ¯å°æ—¶çš„å‡å€¼
        for region_name, region_rows in region_groups.items():
            for hour, times in time_groups.items():
                # è·å–è¯¥åŒºåŸŸæ‰€æœ‰è¡Œåœ¨è¿™äº›æ—¶é—´ç‚¹çš„å€¼å¹¶è®¡ç®—å‡å€¼
                values = []
                for t in times:
                    # è®¡ç®—è¯¥æ—¶é—´ç‚¹åœ¨è¯¥åŒºåŸŸæ‰€æœ‰è¡Œä¸­çš„å‡å€¼
                    region_values = [row[t] for row in region_rows if not pd.isna(row[t])]
                    if region_values:
                        mean_value = sum(region_values) / len(region_values)
                        values.append(mean_value)
                
                # è®¡ç®—4ä¸ªæ—¶é—´ç‚¹çš„æ€»å‡å€¼
                if values:
                    overall_mean = sum(values) / len(values)
                    record = {
                        "record_date": pd.to_datetime(data_date).date(),
                        "record_time": f"{hour}:00",
                        "channel_name": f"{data_type}_å‡å€¼",
                        "value": round(overall_mean, 2),
                        "type": region_name + "_" + data_type,  # æŒ‰åŒºåŸŸåŒºåˆ†
                        "sheet_name": sheet_name,
                        "created_at": pd.Timestamp.now(),
                    }
                    records.append(record)

        print(f"âœ… {data_type} ç”Ÿæˆ {len(records)} æ¡è®°å½•")
        return records

    def query_daily_averages(self, date_list, data_type_keyword="æ—¥å‰èŠ‚ç‚¹ç”µä»·"):
        """
        æŸ¥è¯¢å¤šå¤©çš„å‡å€¼æ•°æ®ï¼ˆé€‚ç”¨äºå·²è®¡ç®—å¥½çš„å‡å€¼è®°å½•ï¼‰
        
        Args:
            date_list (list): æ—¥æœŸåˆ—è¡¨ï¼Œæ ¼å¼ä¸º "YYYY-MM-DD"
            data_type_keyword (str): æ•°æ®ç±»å‹å…³é”®å­—ï¼Œç”¨äºç­›é€‰ç‰¹å®šç±»å‹çš„æ•°æ®
            
        Returns:
            dict: åŒ…å«æŸ¥è¯¢ç»“æœçš„å­—å…¸
        """
        try:
            # æ„é€ è¡¨ååˆ—è¡¨
            table_names = []
            for date_str in date_list:
                # å°†æ—¥æœŸæ ¼å¼è½¬æ¢ä¸ºè¡¨åæ ¼å¼ (YYYY-MM-DD -> YYYYMMDD)
                date_obj = datetime.datetime.strptime(date_str, "%Y-%m-%d")
                table_name = f"power_data_{date_obj.strftime('%Y%m%d')}"
                print(f"ğŸ” æŸ¥è¯¢è¡¨: {table_name}")
                table_names.append(table_name)
                
            
            # éªŒè¯è¡¨æ˜¯å¦å­˜åœ¨
            existing_tables = self.db_manager.get_tables()
            valid_tables = [table for table in table_names if table in existing_tables]
            
            if not valid_tables:
                return {"data": [], "total": 0, "message": "æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ•°æ®è¡¨"}
            
            # æ„é€ UNIONæŸ¥è¯¢è¯­å¥ï¼šæŸ¥æ‰¾åŒ…å«æŒ‡å®šå…³é”®å­—å’Œ"å‡å€¼"çš„è®°å½•
            union_parts = []
            for table in valid_tables:
                union_parts.append(f""" SELECT * FROM {table} WHERE channel_name LIKE '%å‡å€¼%' AND type LIKE '%{data_type_keyword}%'""")
            if not union_parts:
                return {"data": [], "total": 0, "message": "æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„æ•°æ®"}
                
            union_query = " UNION ALL ".join(union_parts)
            print(f"ğŸš€ æ‰§è¡ŒUNIONæŸ¥è¯¢: {union_query}")
            final_query = f"""
                SELECT * FROM ({union_query}) as combined_data
                ORDER BY record_date, record_time
            """
            
            # æ‰§è¡ŒæŸ¥è¯¢
            result = self.db_manager.complex_query(final_query)
            # print(f"âœ… æŸ¥è¯¢æˆåŠŸï¼Œå…± {len(result)} æ¡è®°å½•")
            # print(result)
            
            # æ„é€ è¿”å›ç»“æœ
            return {
                "data": result.get("data"),
                "total": result.get("total"),
                "message": "æŸ¥è¯¢æˆåŠŸ"
            }
            
        except Exception as e:
            print(f"âŒ æŸ¥è¯¢å¤šå¤©å‡å€¼æ•°æ®å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return {"data": [], "total": 0, "message": f"æŸ¥è¯¢å¤±è´¥: {str(e)}"}

    def query_price_difference(self, date_list, region=""):
        """
        æŸ¥è¯¢ä»·å·®æ•°æ®ï¼ˆæ—¥å‰èŠ‚ç‚¹ç”µä»· - å®æ—¶èŠ‚ç‚¹ç”µä»·ï¼‰
        
        Args:
            date_list (list): æ—¥æœŸåˆ—è¡¨ï¼Œæ ¼å¼ä¸º "YYYY-MM-DD"
            region (str): åœ°åŒºå‰ç¼€ï¼Œå¦‚"äº‘å—_"ï¼Œé»˜è®¤ä¸ºç©º
            
        Returns:
            dict: åŒ…å«ä»·å·®æŸ¥è¯¢ç»“æœçš„å­—å…¸
        """
        try:
            import pandas as pd
            
            # æ„é€ æ•°æ®ç±»å‹å…³é”®è¯
            dayahead_keyword = f"{region}æ—¥å‰èŠ‚ç‚¹ç”µä»·" if region else "æ—¥å‰èŠ‚ç‚¹ç”µä»·"
            realtime_keyword = f"{region}å®æ—¶èŠ‚ç‚¹ç”µä»·" if region else "å®æ—¶èŠ‚ç‚¹ç”µä»·"
            
            print(f"ğŸ” æŸ¥è¯¢ä»·å·®æ•°æ®:")
            print(f"  - æ—¥å‰èŠ‚ç‚¹ç”µä»·å…³é”®è¯: {dayahead_keyword}")
            print(f"  - å®æ—¶èŠ‚ç‚¹ç”µä»·å…³é”®è¯: {realtime_keyword}")
            print(f"  - æ—¥æœŸåˆ—è¡¨: {date_list}")
            
            # æŸ¥è¯¢æ—¥å‰èŠ‚ç‚¹ç”µä»·æ•°æ®
            dayahead_result = self.query_daily_averages(date_list, dayahead_keyword)
            dayahead_data = dayahead_result.get("data", [])
            
            # æŸ¥è¯¢å®æ—¶èŠ‚ç‚¹ç”µä»·æ•°æ®
            realtime_result = self.query_daily_averages(date_list, realtime_keyword)
            realtime_data = realtime_result.get("data", [])
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ä¸¤ä¸ªæ•°æ®
            if not dayahead_data:
                return {
                    "data": [],
                    "total": 0,
                    "message": f"æœªæ‰¾åˆ°æ—¥å‰èŠ‚ç‚¹ç”µä»·æ•°æ®ï¼ˆå…³é”®è¯: {dayahead_keyword}ï¼‰",
                    "has_dayahead": False,
                    "has_realtime": len(realtime_data) > 0
                }
            
            if not realtime_data:
                return {
                    "data": [],
                    "total": 0,
                    "message": f"æœªæ‰¾åˆ°å®æ—¶èŠ‚ç‚¹ç”µä»·æ•°æ®ï¼ˆå…³é”®è¯: {realtime_keyword}ï¼‰",
                    "has_dayahead": True,
                    "has_realtime": False
                }
            
            print(f"âœ… æ‰¾åˆ°æ—¥å‰æ•°æ®: {len(dayahead_data)} æ¡")
            print(f"âœ… æ‰¾åˆ°å®æ—¶æ•°æ®: {len(realtime_data)} æ¡")
            
            # è½¬æ¢ä¸ºDataFrameä»¥ä¾¿å¤„ç†
            dayahead_df = pd.DataFrame(dayahead_data)
            realtime_df = pd.DataFrame(realtime_data)
            
            # ç¡®ä¿å¿…è¦çš„åˆ—å­˜åœ¨
            required_columns = ['channel_name', 'record_date', 'record_time', 'value']
            if not all(col in dayahead_df.columns for col in required_columns):
                return {
                    "data": [],
                    "total": 0,
                    "message": "æ—¥å‰æ•°æ®ç¼ºå°‘å¿…è¦åˆ—",
                    "has_dayahead": True,
                    "has_realtime": True
                }
            
            if not all(col in realtime_df.columns for col in required_columns):
                return {
                    "data": [],
                    "total": 0,
                    "message": "å®æ—¶æ•°æ®ç¼ºå°‘å¿…è¦åˆ—",
                    "has_dayahead": True,
                    "has_realtime": True
                }
            
            # ç»Ÿä¸€æ ¼å¼åŒ–å­—æ®µä»¥ä¾¿åŒ¹é…
            # 1. æ ¼å¼åŒ–channel_nameï¼šå»é™¤ç©ºæ ¼ï¼Œç»Ÿä¸€å¤§å°å†™
            dayahead_df['channel_name_clean'] = dayahead_df['channel_name'].astype(str).str.strip()
            realtime_df['channel_name_clean'] = realtime_df['channel_name'].astype(str).str.strip()
            
            # 2. æ ¼å¼åŒ–record_dateï¼šç»Ÿä¸€ä¸ºå­—ç¬¦ä¸²æ ¼å¼ YYYY-MM-DD
            def format_date(date_val):
                if pd.isna(date_val):
                    return ""
                if isinstance(date_val, str):
                    return date_val.strip()
                if hasattr(date_val, 'strftime'):
                    return date_val.strftime('%Y-%m-%d')
                return str(date_val).strip()
            
            dayahead_df['record_date_clean'] = dayahead_df['record_date'].apply(format_date)
            realtime_df['record_date_clean'] = realtime_df['record_date'].apply(format_date)
            
            # 3. æ ¼å¼åŒ–record_timeï¼šç»Ÿä¸€æ—¶é—´æ ¼å¼
            def format_time(time_val):
                if pd.isna(time_val):
                    return ""
                
                # å¤„ç†timedeltaå¯¹è±¡ï¼ˆå¦‚ '0 days 00:00:00'ï¼‰
                if hasattr(time_val, 'total_seconds'):
                    total_seconds = int(time_val.total_seconds())
                    hour = total_seconds // 3600
                    return f"{hour:02d}:00"
                
                # å¦‚æœæ˜¯å­—ç¬¦ä¸²
                if isinstance(time_val, str):
                    time_str = time_val.strip()
                    # å¦‚æœåŒ…å«"days"ï¼Œè¯´æ˜æ˜¯timedeltaå­—ç¬¦ä¸²æ ¼å¼
                    if 'days' in time_str.lower():
                        # è§£ætimedeltaå­—ç¬¦ä¸²ï¼Œå¦‚ "0 days 01:00:00"
                        import re
                        match = re.search(r'(\d+):(\d+):(\d+)', time_str)
                        if match:
                            hours = int(match.group(1))
                            return f"{hours:02d}:00"
                    # å¦‚æœåŒ…å«å†’å·ï¼Œç›´æ¥è¿”å›
                    if ':' in time_str:
                        return time_str
                
                # å¦‚æœæ˜¯æ•°å­—ï¼Œè½¬æ¢ä¸ºHH:MMæ ¼å¼
                try:
                    if isinstance(time_val, (int, float)):
                        val = int(time_val)
                        # å¦‚æœæ˜¯ç§’æ•°ï¼ˆ>=3600ï¼‰ï¼Œè½¬æ¢ä¸ºå°æ—¶
                        if val >= 3600:
                            hour = val // 3600
                            return f"{hour:02d}:00"
                        # å¦‚æœæ˜¯å°æ—¶ï¼ˆ0-23ï¼‰ï¼Œç›´æ¥ä½¿ç”¨
                        if 0 <= val < 24:
                            return f"{val:02d}:00"
                        # å¦‚æœæ˜¯HHMMæ ¼å¼ï¼ˆ100-2400ï¼‰ï¼Œè½¬æ¢ä¸ºHH:MM
                        if 100 <= val <= 2400:
                            hour = val // 100
                            return f"{hour:02d}:00"
                        # å¦‚æœæ˜¯0ï¼Œè¿”å›00:00
                        if val == 0:
                            return "00:00"
                except:
                    pass
                return str(time_val).strip()
            
            dayahead_df['record_time_clean'] = dayahead_df['record_time'].apply(format_time)
            realtime_df['record_time_clean'] = realtime_df['record_time'].apply(format_time)
            
            # æ‰“å°å‰å‡ æ¡æ•°æ®ç”¨äºè°ƒè¯•
            print(f"ğŸ“Š æ—¥å‰æ•°æ®ç¤ºä¾‹:")
            print(f"  channel_name: {dayahead_df['channel_name_clean'].head(3).tolist()}")
            print(f"  record_date: {dayahead_df['record_date_clean'].head(3).tolist()}")
            print(f"  record_time: {dayahead_df['record_time_clean'].head(3).tolist()}")
            print(f"ğŸ“Š å®æ—¶æ•°æ®ç¤ºä¾‹:")
            print(f"  channel_name: {realtime_df['channel_name_clean'].head(3).tolist()}")
            print(f"  record_date: {realtime_df['record_date_clean'].head(3).tolist()}")
            print(f"  record_time: {realtime_df['record_time_clean'].head(3).tolist()}")
            
            # åˆ›å»ºåˆå¹¶é”®ï¼šä»·å·®æŸ¥è¯¢åªä½¿ç”¨record_dateå’Œrecord_timeåŒ¹é…
            # å› ä¸ºæ—¥å‰å’Œå®æ—¶çš„channel_nameå¯èƒ½ä¸åŒï¼ˆå¦‚"æ—¥å‰èŠ‚ç‚¹ç”µä»·æŸ¥è¯¢_å‡å€¼" vs "å®æ—¶èŠ‚ç‚¹ç”µä»·æŸ¥è¯¢_å‡å€¼"ï¼‰
            # ä½†å¦‚æœæ˜¯ç›¸åŒæ—¶é—´ç‚¹çš„å‡å€¼æ•°æ®ï¼Œåº”è¯¥åŒ¹é…
            # å¦‚æœchannel_nameç›¸åŒï¼Œä¹ŸåŒ…å«åœ¨åˆå¹¶é”®ä¸­ï¼›å¦‚æœä¸åŒï¼Œåªä½¿ç”¨æ—¥æœŸå’Œæ—¶é—´
            dayahead_df['merge_key'] = (
                dayahead_df['record_date_clean'] + '_' +
                dayahead_df['record_time_clean']
            )
            realtime_df['merge_key'] = (
                realtime_df['record_date_clean'] + '_' +
                realtime_df['record_time_clean']
            )
            
            # æ‰“å°åˆå¹¶é”®ç¤ºä¾‹
            print(f"ğŸ“Š åˆå¹¶é”®ç¤ºä¾‹ï¼ˆæ—¥å‰ï¼‰: {dayahead_df['merge_key'].head(3).tolist()}")
            print(f"ğŸ“Š åˆå¹¶é”®ç¤ºä¾‹ï¼ˆå®æ—¶ï¼‰: {realtime_df['merge_key'].head(3).tolist()}")
            print(f"ğŸ“Š åˆå¹¶é”®å”¯ä¸€å€¼æ•°é‡ï¼ˆæ—¥å‰ï¼‰: {dayahead_df['merge_key'].nunique()}")
            print(f"ğŸ“Š åˆå¹¶é”®å”¯ä¸€å€¼æ•°é‡ï¼ˆå®æ—¶ï¼‰: {realtime_df['merge_key'].nunique()}")
            
            # åˆå¹¶æ•°æ®
            merged_df = pd.merge(
                dayahead_df[['merge_key', 'channel_name', 'record_date', 'record_time', 'value', 'sheet_name']],
                realtime_df[['merge_key', 'value']],
                on='merge_key',
                how='inner',
                suffixes=('_dayahead', '_realtime')
            )
            
            print(f"ğŸ“Š åˆå¹¶ç»“æœ: {len(merged_df)} æ¡åŒ¹é…è®°å½•")
            print(f"ğŸ“Š æ—¥å‰æ•°æ®å”¯ä¸€åˆå¹¶é”®æ•°: {dayahead_df['merge_key'].nunique()}")
            print(f"ğŸ“Š å®æ—¶æ•°æ®å”¯ä¸€åˆå¹¶é”®æ•°: {realtime_df['merge_key'].nunique()}")
            
            if len(merged_df) == 0:
                # æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
                dayahead_keys = set(dayahead_df['merge_key'].unique())
                realtime_keys = set(realtime_df['merge_key'].unique())
                missing_in_realtime = dayahead_keys - realtime_keys
                missing_in_dayahead = realtime_keys - dayahead_keys
                
                error_msg = "æ—¥å‰å’Œå®æ—¶æ•°æ®æ— æ³•åŒ¹é…ã€‚"
                if len(missing_in_realtime) > 0:
                    error_msg += f" æ—¥å‰æ•°æ®ä¸­æœ‰ {len(missing_in_realtime)} ä¸ªé”®åœ¨å®æ—¶æ•°æ®ä¸­æ‰¾ä¸åˆ°ï¼ˆç¤ºä¾‹: {list(missing_in_realtime)[:3]}ï¼‰ã€‚"
                if len(missing_in_dayahead) > 0:
                    error_msg += f" å®æ—¶æ•°æ®ä¸­æœ‰ {len(missing_in_dayahead)} ä¸ªé”®åœ¨æ—¥å‰æ•°æ®ä¸­æ‰¾ä¸åˆ°ï¼ˆç¤ºä¾‹: {list(missing_in_dayahead)[:3]}ï¼‰ã€‚"
                
                return {
                    "data": [],
                    "total": 0,
                    "message": error_msg,
                    "has_dayahead": True,
                    "has_realtime": True
                }
            
            # è®¡ç®—ä»·å·®ï¼šä¸¤ä¸ªè¡¨å¯¹åº”çš„valueç›¸å‡ï¼ˆæ—¥å‰èŠ‚ç‚¹ç”µä»· - å®æ—¶èŠ‚ç‚¹ç”µä»·ï¼‰
            # ç¡®ä¿valueåˆ—æ˜¯æ•°å€¼ç±»å‹
            dayahead_values = pd.to_numeric(merged_df['value_dayahead'], errors='coerce')
            realtime_values = pd.to_numeric(merged_df['value_realtime'], errors='coerce')
            # è®¡ç®—ä»·å·®ï¼šæ—¥å‰ - å®æ—¶ï¼Œå¹¶ä¿ç•™ä¸¤ä½å°æ•°
            merged_df['value'] = (dayahead_values - realtime_values).round(2)
            
            # å°†channel_nameæ”¹ä¸º"ä»·å·®"
            merged_df['channel_name'] = 'ä»·å·®'
            
            print(f"ğŸ“Š ä»·å·®è®¡ç®—ç¤ºä¾‹:")
            print(f"  æ—¥å‰å€¼: {dayahead_values.head(3).tolist()}")
            print(f"  å®æ—¶å€¼: {realtime_values.head(3).tolist()}")
            print(f"  ä»·å·®å€¼ï¼ˆä¿ç•™ä¸¤ä½å°æ•°ï¼‰: {merged_df['value'].head(3).tolist()}")
            
            # åˆ é™¤ä¸´æ—¶åˆ—
            merged_df = merged_df.drop(columns=['merge_key', 'value_dayahead', 'value_realtime'])
            
            # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
            difference_data = merged_df.to_dict('records')
            
            print(f"âœ… ä»·å·®è®¡ç®—å®Œæˆï¼Œå…± {len(difference_data)} æ¡è®°å½•")
            
            return {
                "data": difference_data,
                "total": len(difference_data),
                "message": "ä»·å·®æŸ¥è¯¢æˆåŠŸ",
                "has_dayahead": True,
                "has_realtime": True
            }
            
        except Exception as e:
            print(f"âŒ æŸ¥è¯¢ä»·å·®æ•°æ®å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return {
                "data": [],
                "total": 0,
                "message": f"æŸ¥è¯¢å¤±è´¥: {str(e)}",
                "has_dayahead": False,
                "has_realtime": False
            }

    def _process_outage_as_table(self, df, data_date, sheet_name):
        """å°†è¡¨æ ¼æ•°æ®æ˜ å°„ä¸ºåœç”µè®°å½•ï¼Œé€‚é…æ–‡ä»¶æ ¼å¼"""
        records = []
        df = df.dropna(how="all")  # åˆ é™¤ç©ºè¡Œ
        
        # å¤„ç†è¡¨å¤´ï¼ˆç¡®ä¿åˆ—åæ­£ç¡®æ˜ å°„ï¼‰
        df.columns = [str(c).strip() for c in df.iloc[0]]  # ç¬¬ä¸€è¡Œä½œåˆ—å
        df = df[1:]  # å»æ‰æ ‡é¢˜è¡Œ
        # æ¸…æ´—åˆ—åï¼Œå»é™¤ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦
        df.columns = [str(col).strip().replace('\n', '').replace(' ', '') for col in df.columns]
        # éªŒè¯å¿…è¦åˆ—æ˜¯å¦å­˜åœ¨
        required_cols = ["è®¾å¤‡åç§°", "ç”µå‹ç­‰çº§", "è®¾å¤‡ç±»å‹", "è®¾å¤‡ç¼–å·", 
                        "è®¡åˆ’åœç”µæ—¥æœŸ", "å®é™…åœç”µæ—¥æœŸ", "è®¡åˆ’å¤ç”µæ—¶é—´", "å®é™…å¤ç”µæ—¶é—´"]
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            raise ValueError(f"ç¼ºå¤±å¿…è¦åˆ—: {missing_cols}")
        
        # éå†æ¯ä¸€è¡Œæ•°æ®
        for idx, row in df.iterrows():
            # è·³è¿‡ç©ºè¡Œå’Œæ ‡é¢˜è¡Œï¼ˆå¦‚æœæœ‰æ®‹ç•™ï¼‰
            device_name = str(row.get("è®¾å¤‡åç§°", "")).strip()
            if not device_name:
                continue
            
            # æ„å»ºè®°å½•å­—å…¸
            record = {
                "device_name": device_name,
                "record_date": data_date,
                "sheet_name": sheet_name,
                "voltage_level": str(row.get("ç”µå‹ç­‰çº§", "")).strip() or None,  # ç©ºå€¼å¤„ç†ä¸ºNone
                "device_type": str(row.get("è®¾å¤‡ç±»å‹", "")).strip(),
                "device_code": str(row.get("è®¾å¤‡ç¼–å·", "")).strip(),
                # æ—¶é—´å­—æ®µä¿æŒåŸå§‹æ ¼å¼ï¼ˆæ•°æ®åº“æ’å…¥æ—¶ä¼šç”¨STR_TO_DATEè½¬æ¢ï¼‰
                "planned_power_off_time": str(row.get("è®¡åˆ’åœç”µæ—¥æœŸ", "")).strip(),
                "actual_power_off_time": str(row.get("å®é™…åœç”µæ—¥æœŸ", "")).strip(),
                "planned_power_on_time": str(row.get("è®¡åˆ’å¤ç”µæ—¶é—´", "")).strip(),
                "actual_power_on_time": str(row.get("å®é™…å¤ç”µæ—¶é—´", "")).strip(),
            }
            
            # éªŒè¯å…³é”®å­—æ®µ
            if not record["device_code"]:
                print(f"è·³è¿‡æ— æ•ˆè¡Œï¼ˆæ— è®¾å¤‡ç¼–å·ï¼‰ï¼š{idx}è¡Œ")
                continue
            if not all([record["planned_power_off_time"], record["planned_power_on_time"]]):
                print(f"è·³è¿‡æ— æ•ˆè¡Œï¼ˆæ—¶é—´ä¸å®Œæ•´ï¼‰ï¼š{idx}è¡Œ")
                continue
            
            records.append(record)
        
        return records
    
    def _process_internal_as_table(self, df, data_date, sheet_name):
        """å°†è¡¨æ ¼æ•°æ®æ˜ å°„ä¸ºå‘ç”µæœºå¹²é¢„è®°å½•ï¼Œé€‚é…æ–‡ä»¶æ ¼å¼"""
        records = []
        df = df.dropna(how="all")  # åˆ é™¤ç©ºè¡Œ
        
        # å¤„ç†è¡¨å¤´ï¼ˆç¡®ä¿åˆ—åæ­£ç¡®æ˜ å°„ï¼‰
        df.columns = [str(c).strip() for c in df.iloc[0]]  # ç¬¬ä¸€è¡Œä½œåˆ—å
        df = df[1:]  # å»æ‰æ ‡é¢˜è¡Œ
        # æ¸…æ´—åˆ—åï¼Œå»é™¤ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦
        df.columns = [str(col).strip().replace('\n', '').replace(' ', '') for col in df.columns]
        # éªŒè¯å¿…è¦åˆ—æ˜¯å¦å­˜åœ¨
        required_cols = ["å¯¹è±¡åç§°", "å¯¹è±¡id", "å¹²é¢„å¼€å§‹æ—¶é—´", "å¹²é¢„ç»“æŸæ—¶é—´", 
                        "å¹²é¢„å‰æœ€å¤§å€¼", "å¹²é¢„å‰æœ€å°å€¼", "å¹²é¢„åæœ€å¤§å€¼", "å¹²é¢„åæœ€å°å€¼", "å¹²é¢„åŸå› "]
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            raise ValueError(f"ç¼ºå¤±å¿…è¦åˆ—: {missing_cols}")
        
        # éå†æ¯ä¸€è¡Œæ•°æ®
        for idx, row in df.iterrows():
            # è·³è¿‡ç©ºè¡Œå’Œæ ‡é¢˜è¡Œï¼ˆå¦‚æœæœ‰æ®‹ç•™ï¼‰
            object_name = str(row.get("å¯¹è±¡åç§°", "")).strip()
            if not object_name:
                continue
            
            # æ„å»ºè®°å½•å­—å…¸
            record = {
                "record_date": data_date,
                "sheet_name": sheet_name,
                "object_name": object_name,
                "object_id": str(row.get("å¯¹è±¡id", "")).strip(),
                "intervention_start_time": str(row.get("å¹²é¢„å¼€å§‹æ—¶é—´", "")).strip(),
                "intervention_end_time": str(row.get("å¹²é¢„ç»“æŸæ—¶é—´", "")).strip(),
                "pre_intervention_max": row.get("å¹²é¢„å‰æœ€å¤§å€¼"),
                "pre_intervention_min": row.get("å¹²é¢„å‰æœ€å°å€¼"),
                "post_intervention_max": row.get("å¹²é¢„åæœ€å¤§å€¼"),
                "post_intervention_min": row.get("å¹²é¢„åæœ€å°å€¼"),
                "intervention_reason": str(row.get("å¹²é¢„åŸå› ", "")).strip(),
            }
            
            # éªŒè¯å…³é”®å­—æ®µ
            if not record["object_id"]:
                print(f"è·³è¿‡æ— æ•ˆè¡Œï¼ˆæ— å¯¹è±¡IDï¼‰ï¼š{idx}è¡Œ")
                continue
            if not all([record["intervention_start_time"], record["intervention_end_time"]]):
                print(f"è·³è¿‡æ— æ•ˆè¡Œï¼ˆæ—¶é—´ä¸å®Œæ•´ï¼‰ï¼š{idx}è¡Œ")
                continue
            
            # å°è¯•è½¬æ¢æ•°å€¼å­—æ®µ
            try:
                for field in ["pre_intervention_max", "pre_intervention_min", "post_intervention_max", "post_intervention_min"]:
                    if record[field] is not None and str(record[field]).strip() != "":
                        record[field] = float(record[field])
                    else:
                        record[field] = None
            except ValueError as e:
                print(f"è·³è¿‡æ— æ•ˆè¡Œï¼ˆæ•°å€¼è½¬æ¢å¤±è´¥ï¼‰ï¼š{idx}è¡Œ, é”™è¯¯: {e}")
                continue
            
            records.append(record)
        
        return records
    
    def _process_7_channel(self, df, data_date, sheet_name):
        """å°†è¡¨æ ¼æ•°æ®æ˜ å°„ä¸ºæœºç»„ç¾¤æ¯”ä¾‹è®°å½•ï¼Œé€‚é…æ‰€æœ‰å­—æ®µå¯ç©ºçš„è¡¨ç»“æ„"""
        records = []
        df = df.dropna(how="all")  # åˆ é™¤å…¨ç©ºè¡Œ
        # æ¸…æ´—åˆ—åï¼šå»é™¤ç©ºæ ¼ã€æ¢è¡Œç¬¦ï¼Œç¡®ä¿ä¸è¡¨å­—æ®µåŒ¹é…
        df.columns = [str(col).strip().replace('\n', '').replace(' ', '') for col in df.columns]
        
        # ç©ºDataFrameæ ¡éªŒ
        if df.empty:
            print(f"è­¦å‘Šï¼šsheet '{sheet_name}' æ— æœ‰æ•ˆæ•°æ®ï¼ˆæ‰€æœ‰è¡Œéƒ½æ˜¯ç©ºè¡Œï¼‰")
            return records 
        
        # éå†æ¯ä¸€è¡Œæ•°æ®ï¼ˆé€‚é…â€œæœºç»„ç¾¤å~æ‰€å æ¯”ä¾‹â€è¡¨å­—æ®µï¼‰
        for idx, row in df.iterrows():
            # æ„å»ºè®°å½•å­—å…¸ï¼šå¯¹åº”è¡¨ä¸­8ä¸ªä¸šåŠ¡å­—æ®µï¼Œæ‰€æœ‰å­—æ®µå…è®¸ä¸ºç©º
            record = {
                "record_date": data_date,  # å¤–éƒ¨ä¼ å…¥çš„æ—¥æœŸï¼ˆå¦‚æ•°æ®æ‰€å±æ—¥æœŸï¼‰
                "sheet_name": sheet_name,  # æ•°æ®æ¥æºè¡¨å
                "unit_group_name": str(row.get("æœºç»„ç¾¤å", "")).strip() or None,  # æœºç»„ç¾¤åï¼ˆç©ºå­—ç¬¦ä¸²è½¬Noneï¼‰
                "power_plant_id": str(row.get("ç”µå‚ID", "")).strip() or None,    # ç”µå‚ID
                "power_plant_name": str(row.get("ç”µå‚åç§°", "")).strip() or None,  # ç”µå‚åç§°
                "unit_id": str(row.get("æœºç»„ID", "")).strip() or None,            # æœºç»„ID
                "unit_name": str(row.get("æœºç»„åç§°", "")).strip() or None,          # æœºç»„åç§°
                "proportion": row.get("æ‰€å æ¯”ä¾‹"),                                 # æ‰€å æ¯”ä¾‹ï¼ˆæ•°å€¼å‹ï¼‰
                "record_time": str(row.get("è®°å½•æ—¶é—´", "")).strip() or None         # è®°å½•æ—¶é—´ï¼ˆåŸå§‹æ ¼å¼ï¼Œå¦‚20250918_15:45:00ï¼‰
            }
            
            # æ•°å€¼å­—æ®µè½¬æ¢ï¼šä»…å¤„ç†â€œæ‰€å æ¯”ä¾‹â€ï¼Œç©ºå€¼æˆ–éæ•°å€¼ç›´æ¥è®¾ä¸ºNoneï¼ˆä¸å¼ºåˆ¶æ ¡éªŒï¼‰
            try:
                if record["proportion"] is not None and str(record["proportion"]).strip():
                    record["proportion"] = float(record["proportion"])
                else:
                    record["proportion"] = None
            except ValueError as e:
                print(f"è¡Œ{idx}ï¼š'æ‰€å æ¯”ä¾‹'å­—æ®µéæœ‰æ•ˆæ•°å€¼ï¼Œè®¾ä¸ºNoneï¼Œé”™è¯¯ï¼š{e}")
                record["proportion"] = None
            
            # æ— å¼ºåˆ¶å…³é”®å­—æ®µæ ¡éªŒï¼ˆæ‰€æœ‰å­—æ®µå¯ç©ºï¼‰ï¼Œç›´æ¥æ·»åŠ è®°å½•
            records.append(record)
        
        return records
    
    def save_to_shubiandian_database(self, records, data_date):
        """ä¿å­˜è®¾å¤‡ç”µå‹ç­‰çº§æ•°æ®åˆ°å›ºå®šè¡¨ device_voltage_level"""
        if not records:
            print("âŒ æ²¡æœ‰å¯ä¿å­˜çš„è®°å½•")
            return False, None, 0, []

        # ğŸ§© 1. å¦‚æœä¼ å…¥çš„æ˜¯ DataFrameï¼Œè½¬æˆ list[dict]
        if isinstance(records, pd.DataFrame):
            records = records.to_dict(orient="records")

        if not isinstance(records, list):
            print(f"âŒ records ç±»å‹é”™è¯¯: {type(records)}ï¼Œåº”ä¸º list[dict]")
            return False, None, 0, []

        # ğŸ§© 2. è¿‡æ»¤æ— æ•ˆè®°å½•å¹¶é€‚é…è¡¨å­—æ®µ
        valid_records = []
        for i, r in enumerate(records):
            if not isinstance(r, dict):
                continue
            # æ·»åŠ  record_date å­—æ®µ
            r["record_date"] = data_date
            valid_records.append(r)
        for i, r in enumerate(records):
            if not isinstance(r, dict):
                continue


        # --- ä½¿ç”¨è®¾å¤‡ç”µå‹ç­‰çº§è¡¨çš„å›ºå®šè¡¨å ---
        table_name = "power_shubiandian"

        try:
            with self.db_manager.engine.begin() as conn:
                # --- åˆ›å»ºè¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰ï¼Œä¸¥æ ¼åŒ¹é…è®¾å¤‡ç”µå‹ç­‰çº§è¡¨ç»“æ„ ---
                create_table_sql = f"""
                CREATE TABLE IF NOT EXISTS `{table_name}` (
                    `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT 'è‡ªå¢ä¸»é”®ï¼Œå”¯ä¸€æ ‡è¯†ä¸€æ¡è®¾å¤‡ç”µå‹è®°å½•',
                    `record_date` date DEFAULT NULL COMMENT 'æ—¥æœŸï¼ˆå¦‚2025-09-18ï¼‰',
                    `device_name` varchar(300) DEFAULT NULL COMMENT 'è®¾å¤‡åç§°ï¼ˆå¦‚â€œ110kVç™½æ²™ç²¤æºªå…‰ä¼ç”µç«™...å¼€å…³ä½ç½®â€ï¼‰',
                    `voltage_level` varchar(50) DEFAULT NULL COMMENT 'ç”µå‹ç­‰çº§(kV)ï¼ˆå¦‚â€œ37kVâ€â€œ115kVâ€ï¼‰',
                    `sheet_name` varchar(255) DEFAULT NULL COMMENT 'æ•°æ®æ¥æºè¡¨åï¼ˆå¦‚â€œè®¾å¤‡ç”µå‹ç­‰çº§è¡¨20250918â€ï¼‰',
                    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT 'è®°å½•å…¥åº“æ—¶é—´',
                    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT 'è®°å½•æ›´æ–°æ—¶é—´',
                    PRIMARY KEY (`id`),
                    KEY `idx_device_name` (`device_name`) COMMENT 'è®¾å¤‡åç§°ç´¢å¼•',
                    KEY `idx_record_date` (`record_date`) COMMENT 'æ—¥æœŸç´¢å¼•',
                    KEY `idx_sheet_name` (`sheet_name`) COMMENT 'æ•°æ®æ¥æºç´¢å¼•'
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
                """
                conn.execute(text(create_table_sql))

                # --- æ’å…¥æ•°æ®ï¼Œå­—æ®µä¸è¡¨ç»“æ„ä¸¥æ ¼å¯¹åº” ---
                insert_sql = text(f"""
                INSERT INTO `{table_name}` (
                    `record_date`,
                    `device_name`,
                    `voltage_level`,
                    `sheet_name`
                ) VALUES (
                    :record_date,
                    :device_name,
                    :voltage_level,
                    :sheet_name
                )
                """)
                conn.execute(insert_sql, valid_records)

                # --- è·å–æ’å…¥ç»“æœï¼ˆé¢„è§ˆå‰10æ¡ï¼‰---
                preview_sql = text(f"""
                SELECT * FROM `{table_name}`
                WHERE `record_date` = :record_date
                ORDER BY `record_date`
                LIMIT 10;
                """)
                # preview_data = conn.execute(preview_sql, {"record_date": data_date}).fetchall()

            return True, table_name, len(valid_records), []

        except Exception as e:
            print(f"ä¿å­˜æ•°æ®æ—¶å‡ºé”™ï¼š{e}")
            return False, None, 0, []

    def save_to_jizuyueshu_database(self, records, data_date):
        """ä¿å­˜æœºç»„çº¦æŸæ•°æ®åˆ°å›ºå®šè¡¨ unit_group_constraint"""
        if not records:
            print("âŒ æ²¡æœ‰å¯ä¿å­˜çš„è®°å½•")
            return True, None, 0, []

        # ğŸ§© 1. å¦‚æœä¼ å…¥çš„æ˜¯ DataFrameï¼Œè½¬æˆ list[dict]
        if isinstance(records, pd.DataFrame):
            records = records.to_dict(orient="records")

        if not isinstance(records, list):
            print(f"âŒ records ç±»å‹é”™è¯¯: {type(records)}ï¼Œåº”ä¸º list[dict]")
            return False, None, 0, []

        # ğŸ§© 2. è¿‡æ»¤æ— æ•ˆè®°å½•
        valid_records = []
        for i, r in enumerate(records):
            if not isinstance(r, dict):
                continue
            # æ·»åŠ  record_date å­—æ®µ
            r["record_date"] = data_date
            valid_records.append(r)

        if not valid_records:
            print("âŒ æ²¡æœ‰å¯ä¿å­˜çš„æœ‰æ•ˆè®°å½•")
            return False, None, 0, []

        # --- ä½¿ç”¨å›ºå®šè¡¨å ---
        table_name = "power_yueshu"
        preview_data = []

        try:
            with self.db_manager.engine.begin() as conn:
                # --- åˆ›å»ºè¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰---
                create_table_sql = f"""
                CREATE TABLE IF NOT EXISTS `{table_name}` (
                  `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT 'è‡ªå¢ä¸»é”®ï¼Œå”¯ä¸€æ ‡è¯†ä¸€æ¡çº¦æŸè®°å½•',
                  `unit_group_name` varchar(200) DEFAULT NULL COMMENT 'æœºç»„ç¾¤åï¼ˆå¦‚"ä¸œæ–¹ç«™çŸ­è·¯ç”µæµæ§åˆ¶""ä¸­ç ç‰‡å¿…å¼€æœºç»„ç¾¤1"ï¼‰',
                  `effective_time` datetime DEFAULT NULL COMMENT 'ç”Ÿæ•ˆæ—¶é—´ï¼ˆå¦‚2025-07-10 00:00:00ï¼Œçº¦æŸå¼€å§‹ç”Ÿæ•ˆçš„æ—¶é—´ï¼‰',
                  `expire_time` datetime DEFAULT NULL COMMENT 'å¤±æ•ˆæ—¶é—´ï¼ˆå¦‚2038-01-19 11:14:07ï¼Œçº¦æŸå¤±æ•ˆçš„æ—¶é—´ï¼Œé»˜è®¤é•¿æœŸæœ‰æ•ˆï¼‰',
                  `power_constraint` tinyint(1) DEFAULT NULL COMMENT 'ç”µåŠ›çº¦æŸï¼ˆ1=æ˜¯ï¼Œ0=å¦ï¼Œå¯¹åº”æ•°æ®ä¸­çš„"æ˜¯/å¦"ï¼‰',
                  `electricity_constraint` tinyint(1) DEFAULT NULL COMMENT 'ç”µé‡çº¦æŸï¼ˆ1=æ˜¯ï¼Œ0=å¦ï¼Œå¯¹åº”æ•°æ®ä¸­çš„"æ˜¯/å¦"ï¼‰',
                  `max_operation_constraint` tinyint(1) DEFAULT NULL COMMENT 'æœ€å¤§è¿è¡Œæ–¹å¼çº¦æŸï¼ˆ1=æ˜¯ï¼Œ0=å¦ï¼Œå¯¹åº”æ•°æ®ä¸­çš„"æ˜¯/å¦"ï¼‰',
                  `min_operation_constraint` tinyint(1) DEFAULT NULL COMMENT 'æœ€å°è¿è¡Œæ–¹å¼çº¦æŸï¼ˆ1=æ˜¯ï¼Œ0=å¦ï¼Œå¯¹åº”æ•°æ®ä¸­çš„"æ˜¯/å¦"ï¼‰',
                  `max_electricity` decimal(18,2) DEFAULT NULL COMMENT 'æœ€å¤§ç”µé‡ï¼ˆæ•°æ®ä¸­ä¸º0ï¼Œæ”¯æŒå°æ•°ï¼Œå•ä½æ ¹æ®ä¸šåŠ¡å®šä¹‰å¦‚MWhï¼‰',
                  `min_electricity` decimal(18,2) DEFAULT NULL COMMENT 'æœ€å°ç”µé‡ï¼ˆæ•°æ®ä¸­ä¸º0ï¼Œæ”¯æŒå°æ•°ï¼Œå•ä½åŒæœ€å¤§ç”µé‡ï¼‰',
                  `record_date` date DEFAULT NULL COMMENT 'æ•°æ®æ‰€å±æ—¥æœŸï¼ˆå¦‚2025-09-18ï¼Œç»Ÿä¸€æ ‡è¯†è¯¥æ‰¹æ•°æ®çš„æ—¶é—´ç»´åº¦ï¼‰',
                  `sheet_name` varchar(255) DEFAULT NULL COMMENT 'æ•°æ®æ¥æºè¡¨åï¼ˆå¦‚"æœºç»„ç¾¤çº¦æŸé…ç½®è¡¨202509"ï¼Œç”¨äºæ•°æ®æº¯æºï¼‰',
                  `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT 'è®°å½•å…¥åº“æ—¶é—´ï¼ˆè‡ªåŠ¨ç”Ÿæˆï¼Œæ— éœ€æ‰‹åŠ¨æ’å…¥ï¼‰',
                  `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT 'è®°å½•æ›´æ–°æ—¶é—´ï¼ˆè‡ªåŠ¨æ›´æ–°ï¼Œæ— éœ€ç»´æŠ¤ï¼‰',
                  PRIMARY KEY (`id`),
                  KEY `idx_unit_group` (`unit_group_name`) COMMENT 'æœºç»„ç¾¤åç´¢å¼•ï¼Œä¼˜åŒ–"æŒ‰æœºç»„ç¾¤æŸ¥è¯¢çº¦æŸ"åœºæ™¯',
                  KEY `idx_effective_time` (`effective_time`, `expire_time`) COMMENT 'ç”Ÿæ•ˆ-å¤±æ•ˆæ—¶é—´è”åˆç´¢å¼•ï¼Œä¼˜åŒ–"æŸ¥è¯¢å½“å‰æœ‰æ•ˆçº¦æŸ"åœºæ™¯',
                  KEY `idx_record_date` (`record_date`) COMMENT 'æ•°æ®æ—¥æœŸç´¢å¼•ï¼Œä¼˜åŒ–"æŒ‰æ—¥æœŸç­›é€‰æ‰¹æ¬¡æ•°æ®"åœºæ™¯'
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='æœºç»„ç¾¤çº¦æŸé…ç½®è¡¨ï¼ˆå­˜å‚¨æœºç»„ç¾¤çš„ç”µåŠ›/ç”µé‡/è¿è¡Œæ–¹å¼çº¦æŸé…ç½®ï¼‰';
                """
                conn.execute(text(create_table_sql))
                print(f"âœ… è¡¨ {table_name} å·²å­˜åœ¨æˆ–åˆ›å»ºæˆåŠŸ")

                # åˆ é™¤è¯¥æ—¥æœŸçš„æ—§æ•°æ®
                conn.execute(text(f"DELETE FROM {table_name} WHERE record_date = :record_date"), 
                             {"record_date": data_date})
                print(f"ğŸ—‘ï¸ å·²åˆ é™¤ {data_date} çš„æ—§æ•°æ®")

                # --- æ‰¹é‡æ’å…¥ ---
                insert_stmt = text(f"""
                INSERT IGNORE INTO {table_name} 
                (unit_group_name, effective_time, expire_time, power_constraint, electricity_constraint, 
                 max_operation_constraint, min_operation_constraint, max_electricity, min_electricity, 
                 record_date, sheet_name)
                VALUES 
                (:unit_group_name, :effective_time, :expire_time, :power_constraint, :electricity_constraint, 
                 :max_operation_constraint, :min_operation_constraint, :max_electricity, :min_electricity, 
                 :record_date, :sheet_name)
                """)
                
                # æ‰¹é‡æ’å…¥æ•°æ®
                batch_size = 200
                for i in range(0, len(valid_records), batch_size):
                    batch = valid_records[i:i + batch_size]
                    conn.execute(insert_stmt, batch)
                    print(f"ğŸ’¾ å·²æ’å…¥ç¬¬ {i // batch_size + 1} æ‰¹æ•°æ® ({len(batch)} æ¡)")

                # è·å–æ’å…¥çš„æ•°æ®æ€»é‡
                count_stmt = text(f"SELECT COUNT(*) FROM {table_name} WHERE record_date = :record_date")
                count = conn.execute(count_stmt, {"record_date": data_date}).scalar()
                
                # è·å–é¢„è§ˆæ•°æ®
                preview_stmt = text(f"SELECT * FROM {table_name} WHERE record_date = :record_date LIMIT 5")
                preview_result = conn.execute(preview_stmt, {"record_date": data_date})
                for row in preview_result:
                    preview_data.append(dict(row._mapping))

                print(f"âœ… {table_name} æ•°æ®åº“ä¿å­˜æˆåŠŸ: {count} æ¡è®°å½•")
                return True, table_name, count, []
        except Exception as e:
            print(f"âŒ {table_name} æ•°æ®åº“ä¿å­˜å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False, None, 0, []

    def save_to_jizujichu_database(self, records, data_date):
        """ä¿å­˜æœºç»„åŸºç¡€æ•°æ®åˆ°å›ºå®šè¡¨ jizujichu"""
        if not records:
            print("âŒ æ²¡æœ‰å¯ä¿å­˜çš„è®°å½•")
            return True, None, 0, []

        # ğŸ§© 1. å¦‚æœä¼ å…¥çš„æ˜¯ DataFrameï¼Œè½¬æˆ list[dict]
        if isinstance(records, pd.DataFrame):
            records = records.to_dict(orient="records")

        if not isinstance(records, list):
            print(f"âŒ records ç±»å‹é”™è¯¯: {type(records)}ï¼Œåº”ä¸º list[dict]")
            return False, None, 0, []

        # ğŸ§© 2. è¿‡æ»¤æ— æ•ˆè®°å½•
        valid_records = []
        for i, r in enumerate(records):
            if not isinstance(r, dict):
                continue
            # æ·»åŠ  record_date å­—æ®µ
            r["record_date"] = data_date
            valid_records.append(r)

        if not valid_records:
            print("âŒ æ²¡æœ‰å¯ä¿å­˜çš„æœ‰æ•ˆè®°å½•")
            return False, None, 0, []

        # --- ä½¿ç”¨å›ºå®šè¡¨å ---
        table_name = "power_jichu"
        preview_data = []

        try:
            with self.db_manager.engine.begin() as conn:
                # --- åˆ›å»ºè¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰---
                create_table_sql = f"""
                CREATE TABLE IF NOT EXISTS `{table_name}` (
                  `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT 'è‡ªå¢ä¸»é”®ï¼Œå”¯ä¸€æ ‡è¯†ä¸€æ¡è®°å½•',
                  `unit_group_name` varchar(200) DEFAULT NULL COMMENT 'æœºç»„ç¾¤åï¼ˆå¦‚"ä¸œæ–¹ç«™çŸ­è·¯ç”µæµæ§åˆ¶""ä¸­ç ç‰‡å¿…å¼€æœºç»„ç¾¤1"ï¼‰',
                  `power_plant_id` varchar(50) DEFAULT NULL COMMENT 'ç”µå‚IDï¼ˆå”¯ä¸€æ ‡è¯†ï¼Œå¦‚"0300F15000014""0300F13000059"ï¼‰',
                  `power_plant_name` varchar(200) DEFAULT NULL COMMENT 'ç”µå‚åç§°ï¼ˆå¦‚"æ²™è§’Cå‚""ç²¤æµ·å‚"ï¼‰',
                  `unit_id` varchar(100) DEFAULT NULL COMMENT 'æœºç»„IDï¼ˆå”¯ä¸€æ ‡è¯†ï¼Œå¦‚"0300F150000140HNN00FAB001"ï¼‰',
                  `unit_name` varchar(100) DEFAULT NULL COMMENT 'æœºç»„åç§°ï¼ˆå¦‚"C1Få‘ç”µæœº""2G"ï¼‰',
                  `proportion` decimal(5,2) DEFAULT NULL COMMENT 'æ‰€å æ¯”ä¾‹ï¼ˆæ•°æ®ä¸­ä¸ºæ•´æ•°1ï¼Œæ”¯æŒå°æ•°å¦‚0.5è¡¨ç¤º50%ï¼Œç²¾åº¦ä¿ç•™2ä½ï¼‰',
                  `record_date` date DEFAULT NULL COMMENT 'æ•°æ®æ‰€å±æ—¥æœŸï¼ˆå¦‚2025-09-18ï¼Œç»Ÿä¸€æ ‡è¯†æ•°æ®çš„æ—¶é—´ç»´åº¦ï¼‰',
                  `sheet_name` varchar(255) DEFAULT NULL COMMENT 'æ•°æ®æ¥æºè¡¨åï¼ˆå¦‚"ä¸œæ–¹ç«™æœºç»„ç¾¤æ¯”ä¾‹è¡¨20250918"ï¼Œç”¨äºæ•°æ®æº¯æºï¼‰',
                  `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT 'è®°å½•å…¥åº“æ—¶é—´ï¼ˆè‡ªåŠ¨ç”Ÿæˆï¼Œæ— éœ€æ‰‹åŠ¨æ’å…¥ï¼‰',
                  `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT 'è®°å½•æ›´æ–°æ—¶é—´ï¼ˆè‡ªåŠ¨æ›´æ–°ï¼Œæ— éœ€ç»´æŠ¤ï¼‰',
                  PRIMARY KEY (`id`),
                  KEY `idx_unit_group` (`unit_group_name`) COMMENT 'æœºç»„ç¾¤åç´¢å¼•ï¼Œä¼˜åŒ–"æŒ‰æœºç»„ç¾¤æŸ¥è¯¢æ‰€æœ‰æœºç»„"åœºæ™¯',
                  KEY `idx_power_plant` (`power_plant_id`, `power_plant_name`) COMMENT 'ç”µå‚ID+åç§°è”åˆç´¢å¼•ï¼Œä¼˜åŒ–"æŒ‰ç”µå‚ç­›é€‰"åœºæ™¯',
                  KEY `idx_record_date` (`record_date`) COMMENT 'æ•°æ®æ—¥æœŸç´¢å¼•ï¼Œä¼˜åŒ–"æŒ‰æ—¥æœŸèŒƒå›´ç»Ÿè®¡"åœºæ™¯'
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='æœºç»„ç¾¤-æœºç»„åˆ†é…æ¯”ä¾‹è®°å½•è¡¨ï¼ˆå­˜å‚¨æœºç»„ç¾¤ä¸æœºç»„çš„å½’å±æ¯”ä¾‹å…³ç³»ï¼‰';
                """
                conn.execute(text(create_table_sql))
                print(f"âœ… è¡¨ {table_name} å·²å­˜åœ¨æˆ–åˆ›å»ºæˆåŠŸ")

                # åˆ é™¤è¯¥æ—¥æœŸçš„æ—§æ•°æ®
                conn.execute(text(f"DELETE FROM {table_name} WHERE record_date = :record_date"), 
                             {"record_date": data_date})
                print(f"ğŸ—‘ï¸ å·²åˆ é™¤ {data_date} çš„æ—§æ•°æ®")

                # --- æ‰¹é‡æ’å…¥ ---
                insert_stmt = text(f"""
                INSERT IGNORE INTO {table_name} 
                (unit_group_name, power_plant_id, power_plant_name, unit_id, unit_name, proportion, record_date, sheet_name)
                VALUES 
                (:unit_group_name, :power_plant_id, :power_plant_name, :unit_id, :unit_name, :proportion, :record_date, :sheet_name)
                """)
                
                # æ‰¹é‡æ’å…¥æ•°æ®
                batch_size = 200
                for i in range(0, len(valid_records), batch_size):
                    batch = valid_records[i:i + batch_size]
                    conn.execute(insert_stmt, batch)
                    print(f"ğŸ’¾ å·²æ’å…¥ç¬¬ {i // batch_size + 1} æ‰¹æ•°æ® ({len(batch)} æ¡)")

                # è·å–æ’å…¥çš„æ•°æ®æ€»é‡
                count_stmt = text(f"SELECT COUNT(*) FROM {table_name} WHERE record_date = :record_date")
                count = conn.execute(count_stmt, {"record_date": data_date}).scalar()
                
                # è·å–é¢„è§ˆæ•°æ®
                preview_stmt = text(f"SELECT * FROM {table_name} WHERE record_date = :record_date LIMIT 5")
                preview_result = conn.execute(preview_stmt, {"record_date": data_date})
                for row in preview_result:
                    preview_data.append(dict(row._mapping))

                print(f"âœ… {table_name} æ•°æ®åº“ä¿å­˜æˆåŠŸ: {count} æ¡è®°å½•")
                return True, table_name, count, []

        except Exception as e:
            print(f"âŒ {table_name} æ•°æ®åº“ä¿å­˜å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False, None, 0, []
