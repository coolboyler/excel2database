import pandas as pd
import numpy as np
import datetime
import re
import os
from sqlalchemy import text
from database import DatabaseManager

class PowerDataImporter:
    def __init__(self):
        self.db_manager = DatabaseManager()
        self._city_mapping = None
        self._city_mapping_loaded = False
        pass

    # ===============================
    # åŸå¸‚æ˜ å°„ç›¸å…³ (èŠ‚ç‚¹ç”µä»· -> åŸå¸‚)
    # ===============================
    _CITY_LIST_GD = [
        "å¹¿å·", "æ·±åœ³", "ä½›å±±", "ä¸œè", "ä¸­å±±", "ç æµ·", "æ±Ÿé—¨", "æƒ å·", "æ±•å¤´", "æ±•å°¾",
        "æ­é˜³", "æ½®å·", "æ¢…å·", "æ²³æº", "æ¸…è¿œ", "éŸ¶å…³", "æ¹›æ±Ÿ", "èŒ‚å", "é˜³æ±Ÿ", "äº‘æµ®", "è‚‡åº†"
    ]
    _CITY_LIST_YN = [
        "äº‘å—", "æ˜†æ˜", "æ›²é–", "ç‰æºª", "ä¿å±±", "æ˜­é€š", "ä¸½æ±Ÿ", "æ™®æ´±", "ä¸´æ²§", "æ¥šé›„",
        "çº¢æ²³", "æ–‡å±±", "è¥¿åŒç‰ˆçº³", "å¤§ç†", "å¾·å®", "æ€’æ±Ÿ", "è¿ªåº†"
    ]

    def _city_channel_name(self, city: str) -> str:
        return f"{city}_èŠ‚ç‚¹å‡ä»·"

    def _normalize_node_name(self, name: str) -> str:
        if not name:
            return ""
        s = str(name).strip()
        # å»æ‰åŸå¸‚å‰ç¼€
        for c in self._CITY_LIST_GD:
            if s.startswith(c):
                s = s[len(c):]
                break
        # å»æ‰â€œå…¶ä»–â€å‰ç¼€
        if s.startswith("å…¶ä»–"):
            s = s[2:]
        # ç»Ÿä¸€å¤§å°å†™/ç¬¦å·
        s = s.replace("ï¼«ï¼¶", "kV").replace("KV", "kV").replace("kv", "kV")
        s = s.replace("ï¼ƒ", "#")
        # å»æ‰å¸¸è§åˆ†éš”ç¬¦ä¸å•ä½/æ ‡è¯†
        s = re.sub(r"[\\.Â·ã€‚/\\\\\\-\\s_()ï¼ˆï¼‰]+", "", s)
        s = s.replace("kV", "")
        s = s.replace("æ¯çº¿", "")
        s = s.replace("M", "").replace("m", "")
        # ä»…ä¿ç•™æ±‰å­—/æ•°å­—/# æ–¹ä¾¿åŒ¹é…
        s = re.sub(r"[^\u4e00-\u9fff0-9#]", "", s)
        return s

    def _extract_city_prefix(self, name: str):
        if not name:
            return None
        s = str(name).strip()
        for c in self._CITY_LIST_GD:
            if s.startswith(c):
                return c
        return None

    def _load_city_mapping(self):
        if self._city_mapping_loaded:
            return self._city_mapping or {}

        mapping = {}
        # ä¼˜å…ˆè¯»å–ç¼“å­˜
        base_dir = os.path.dirname(__file__)
        cache_path = os.path.join(base_dir, "state", "node_city_mapping.json")
        try:
            if os.path.exists(cache_path):
                import json
                with open(cache_path, "r", encoding="utf-8") as f:
                    mapping = json.load(f)
        except Exception:
            mapping = {}

        # è‹¥ç¼“å­˜ä¸ºç©ºï¼Œå°è¯•ä» 2025-06-28 æ–‡ä»¶æ„å»º
        if not mapping or len(mapping) < 200:
            candidates = [
                os.path.join(base_dir, "å®æ—¶èŠ‚ç‚¹ç”µä»·æŸ¥è¯¢(2025-06-28).xlsx"),
                os.path.join(base_dir, "data", "å®æ—¶èŠ‚ç‚¹ç”µä»·æŸ¥è¯¢(2025-06-28).xlsx"),
            ]
            source_path = next((p for p in candidates if os.path.exists(p)), None)
            if source_path:
                try:
                    xls = pd.ExcelFile(source_path)
                    sheet_name = xls.sheet_names[0]
                    df = pd.read_excel(source_path, sheet_name=sheet_name, usecols=[0])
                    for raw_name in df.iloc[:, 0].dropna().astype(str).tolist():
                        city = self._extract_city_prefix(raw_name)
                        if not city:
                            continue
                        key = self._normalize_node_name(raw_name)
                        if key:
                            mapping.setdefault(key, city)
                    if mapping:
                        try:
                            import json
                            os.makedirs(os.path.dirname(cache_path), exist_ok=True)
                            with open(cache_path, "w", encoding="utf-8") as f:
                                json.dump(mapping, f, ensure_ascii=False)
                        except Exception:
                            pass
                except Exception as e:
                    print(f"âš ï¸ åŸå¸‚æ˜ å°„æ„å»ºå¤±è´¥: {e}")

        self._city_mapping = mapping
        self._city_mapping_loaded = True
        return mapping

    def _get_city_from_node(self, node_name: str):
        if not node_name:
            return None
        node_str = str(node_name).strip()
        # æ˜ç¡®æ’é™¤äº‘å—èŠ‚ç‚¹ï¼Œé¿å…è¯¯æ˜ å°„ä¸ºå¹¿ä¸œåŸå¸‚
        for kw in self._CITY_LIST_YN:
            if kw and kw in node_str:
                return None
        city = self._extract_city_prefix(node_str)
        if city:
            return city
        mapping = self._load_city_mapping()
        key = self._normalize_node_name(node_str)
        return mapping.get(key)

    def _extract_hour(self, time_val):
        if time_val is None or (isinstance(time_val, float) and np.isnan(time_val)):
            return None
        # datetime.time
        if hasattr(time_val, "hour"):
            try:
                return int(time_val.hour)
            except Exception:
                pass
        # timedelta
        if hasattr(time_val, "total_seconds"):
            try:
                return int(time_val.total_seconds() // 3600)
            except Exception:
                pass
        # number
        if isinstance(time_val, (int, float, np.number)) and not isinstance(time_val, bool):
            val = int(time_val)
            if val >= 3600:
                return val // 3600
            if 0 <= val < 24:
                return val
            if 100 <= val <= 2400:
                return val // 100
            if val == 0:
                return 0
        # string
        try:
            s = str(time_val).strip()
            if ":" in s:
                return int(s.split(":")[0])
            val = int(float(s))
            if val >= 3600:
                return val // 3600
            if 0 <= val < 24:
                return val
            if 100 <= val <= 2400:
                return val // 100
        except Exception:
            return None
        return None

    def ensure_city_means_for_date(self, date_str, data_type_keyword, city=None, insert=True):
        """
        ä¸ºæŒ‡å®šæ—¥æœŸç”ŸæˆåŸå¸‚èŠ‚ç‚¹å‡ä»·ï¼ˆå¯é€‰æ’å…¥åˆ° power_data_YYYYMMDDï¼‰
        city=None è¡¨ç¤ºç”Ÿæˆæ‰€æœ‰åŸå¸‚
        """
        try:
            date_obj = datetime.datetime.strptime(date_str, "%Y-%m-%d").date()
        except Exception:
            print(f"âš ï¸ æ—¥æœŸæ ¼å¼é”™è¯¯: {date_str}")
            return []

        table_name = f"power_data_{date_obj.strftime('%Y%m%d')}"
        existing_tables = self.db_manager.get_tables()
        if table_name not in existing_tables:
            return []

        # æŸ¥è¯¢èŠ‚ç‚¹è®°å½•ï¼ˆæ’é™¤å‡å€¼/åŸå¸‚å‡ä»·è¡Œï¼‰
        type_like = f"%{data_type_keyword}%"
        sql = text(f"""
            SELECT record_time, channel_name, value, sheet_name, type
            FROM {table_name}
            WHERE type LIKE :type_like
              AND channel_name NOT LIKE '%å‡å€¼%'
              AND channel_name NOT LIKE '%èŠ‚ç‚¹å‡ä»·%'
        """)
        with self.db_manager.engine.connect() as conn:
            rows = conn.execute(sql, {"type_like": type_like}).fetchall()

        if not rows:
            return []

        # èšåˆ
        city_hour_values = {}
        sheet_name = None
        type_value = None
        for row in rows:
            row_dict = dict(row._mapping)
            sheet_name = sheet_name or row_dict.get("sheet_name")
            type_value = type_value or row_dict.get("type")
            node_city = self._get_city_from_node(row_dict.get("channel_name"))
            if not node_city:
                continue
            if city and node_city != city:
                continue
            hour = self._extract_hour(row_dict.get("record_time"))
            if hour is None or hour < 0 or hour > 23:
                continue
            city_hour_values.setdefault(node_city, {}).setdefault(hour, []).append(row_dict.get("value"))

        records = []
        for city_name, hour_map in city_hour_values.items():
            for hour, vals in hour_map.items():
                vals = [v for v in vals if v is not None]
                if not vals:
                    continue
                mean_val = sum(vals) / len(vals)
                records.append({
                    "record_date": date_obj,
                    "record_time": f"{hour:02d}:00",
                    "channel_name": self._city_channel_name(city_name),
                    "value": round(mean_val, 2),
                    "type": type_value or data_type_keyword,
                    "sheet_name": sheet_name or data_type_keyword,
                })

        if insert and records:
            with self.db_manager.engine.begin() as conn:
                if city:
                    conn.execute(
                        text(f"""
                            DELETE FROM {table_name}
                            WHERE record_date = :d
                              AND channel_name = :cn
                              AND type LIKE :type_like
                        """),
                        {"d": date_obj, "cn": self._city_channel_name(city), "type_like": type_like}
                    )
                else:
                    conn.execute(
                        text(f"""
                            DELETE FROM {table_name}
                            WHERE record_date = :d
                              AND channel_name LIKE '%èŠ‚ç‚¹å‡ä»·%'
                              AND type LIKE :type_like
                        """),
                        {"d": date_obj, "type_like": type_like}
                    )
                insert_stmt = text(f"""
                    INSERT INTO {table_name}
                    (record_date, record_time, type, channel_name, value, sheet_name)
                    VALUES (:record_date, :record_time, :type, :channel_name, :value, :sheet_name)
                """)
                conn.execute(insert_stmt, records)

        return records

    # ===============================
    # ä¸»å…¥å£ï¼šå¯¼å…¥æ‰€æœ‰sheet
    # ===============================
    def import_power_data(self, excel_file):
        """è‡ªåŠ¨å¯¼å…¥Excelä¸­æ‰€æœ‰Sheetçš„æ•°æ®ï¼Œæ—¥æœŸè‡ªåŠ¨è¯†åˆ«"""
        sheet_dict = self.read_excel_data(excel_file)
        if not sheet_dict:
            return False, None, 0, []

        all_records = []
        table_name = None
        data_type = None

        for sheet_name, df in sheet_dict.items():
            # === è‡ªåŠ¨è¯†åˆ«æ—¥æœŸ ===
            match = re.search(r"\((\d{4}-\d{2}-\d{2})\)", sheet_name)
            data_date = datetime.datetime.strptime(match.group(1), "%Y-%m-%d").date()

            # === æ ¹æ®æ–‡ä»¶åè¯†åˆ«ç±»å‹ ===
            file_name = str(excel_file)
            
            chinese_match = re.search(r'([\u4e00-\u9fff]+)', file_name)
            if chinese_match:
                data_type = chinese_match.group(1)
                print(f"ğŸ“ æ–‡ä»¶ç±»å‹è¯†åˆ«: {data_type}")
            else:
                print(f"âš ï¸ æœªèƒ½åœ¨æ–‡ä»¶åä¸­æ‰¾åˆ°æ±‰å­—ï¼š{file_name}ï¼Œè·³è¿‡ã€‚")
                return False, None, 0, []

            print(f"\nğŸ“˜ æ­£åœ¨å¤„ç† {sheet_name} | æ—¥æœŸ: {data_date} | ç±»å‹: {data_type}")

            records = self.process_24h_data(df, data_date, sheet_name, data_type)
            all_records.extend(records)

        if not all_records:
            print("âŒ æ²¡æœ‰ä»»ä½•æœ‰æ•ˆæ•°æ®è¢«å¯¼å…¥")
            return False, None, 0, []

        # === ä¿å­˜æ•°æ®åº“ ===
        success, table_name, record_count, preview_data = self.save_to_database(all_records, data_date)
        return success, table_name, record_count, preview_data

    # ===============================
    # è¯»å–æ‰€æœ‰sheet
    # ===============================
    def read_excel_data(self, excel_file):
        """è¯»å–Excelä¸­æ‰€æœ‰Sheet"""
        try:
            sheet_dict = pd.read_excel(excel_file, sheet_name=None, header=0)
            print(f"âœ… æˆåŠŸè¯»å–Excelï¼Œå…± {len(sheet_dict)} ä¸ªSheet: {list(sheet_dict.keys())}")
            return sheet_dict
        except Exception as e:
            print(f"âŒ è¯»å–Excelå¤±è´¥: {e}")
            return None

    # ===============================
    # å¤„ç†å•ä¸ªsheetçš„24å°æ—¶æ•°æ®
    # ===============================
    def process_24h_data(self, df, data_date, sheet_name, data_type):
        """å¤„ç†å•ä¸ªSheetï¼ˆè¡Œå¼ç»“æ„ï¼‰çš„24å°æ—¶æ•°æ®"""
        records = []

        # æ ‡å‡†åŒ–åˆ—å
        df.columns = [str(c).strip() for c in df.columns]

        # æ£€æŸ¥æ•°æ®æ ¼å¼ï¼šæœ‰"é€šé“åç§°"åˆ—è¿˜æ˜¯æœ‰"ç±»å‹"åˆ—
        if "é€šé“åç§°" in df.columns:
            records = self._process_channel_format(df, data_date, sheet_name, data_type)
        elif "ç±»å‹" in df.columns:
            records = self._process_type_format(df, data_date, sheet_name, data_type)
        else:
            print(f"âš ï¸ æœªæ‰¾åˆ° 'é€šé“åç§°' æˆ– 'ç±»å‹' åˆ—ï¼Œè·³è¿‡ã€‚å¯ç”¨åˆ—: {list(df.columns)}")
            return records

    def save_to_imformation_pred_database(self, records, data_date):
        """ä¿å­˜ä¿¡æ¯æŠ«éœ²é¢„æµ‹æ•°æ®åˆ°è‡ªå®šä¹‰è¡¨ (åŠ¨æ€åˆ†è¡¨)"""
        if not records:
            print("âŒ æ²¡æœ‰å¯ä¿å­˜çš„è®°å½•")
            return True, None, 0, []

        # 1. è¿‡æ»¤æ— æ•ˆè®°å½•
        valid_records = []
        for r in records:
            if isinstance(r, dict):
                r['record_date'] = data_date
                valid_records.append(r)

        if not valid_records:
            return False, None, 0, []
        
        # ç¿»è¯‘æ˜ å°„
        translation_map = {
            "ç”µå‚åç§°": "power_plant_name", "æœºç»„åç§°": "generator_name",
            "æœ€å°æŠ€æœ¯å‡ºåŠ›": "min_technical_output", "æœ€å°æŠ€æœ¯å‡ºåŠ›(MW)": "min_technical_output",
            "é¢å®šå‡ºåŠ›": "rated_output", "é¢å®šå‡ºåŠ›(MW)": "rated_output",
            "æ—¥æœŸ": "maintenance_date", "æ—¶é—´": "record_time",
            # é¿å…ä¸ç³»ç»Ÿå­—æ®µ `type` å†²çªï¼šExcel è¡¨å¤´â€œç±»å‹â€æ˜ å°„ä¸ºå…¶å®ƒåˆ—å
            "ç±»å‹": "category",
            "å¤‡æ³¨": "remarks", "åºå·": "seq_no", "å…ƒä»¶åç§°": "component_name",
            "è®¾å¤‡åç§°": "device_name", "ç”µå‹ç­‰çº§": "voltage_level", "ç”µå‹ç­‰çº§(Kv)": "voltage_level",
            "åœç”µèŒƒå›´": "outage_scope", "åœç”µæ—¶é—´": "outage_time", "é€ç”µæ—¶é—´": "restore_time",
            "å·¥ä½œå†…å®¹": "work_content", "æ£€ä¿®æ€§è´¨": "maintenance_type", "ç”³è¯·å•ä½": "applicant",
            "æ•°æ®é¡¹": "data_item", "æ–­é¢åç§°": "section_name", "æœºç»„ç¾¤å": "unit_group_name",
            "å¼€å§‹æ—¶é—´": "start_time", "ç»“æŸæ—¶é—´": "end_time", "çŠ¶æ€ç±»å‹": "status_type",
            "è®¾å¤‡æ”¹å˜åŸå› ": "equipment_change_reason",
            "æœºç»„æ£€ä¿®é¢„æµ‹ä¿¡æ¯": "unit_maintenance_prediction", "æœºç»„æŠ€æœ¯å‚æ•°": "unit_technical_parameters",
            "æ£€ä¿®è®¡åˆ’": "maintenance_plan", "è¾“å˜ç”µæ£€ä¿®é¢„æµ‹ä¿¡æ¯": "transmission_maintenance",
            "æœºç»„æ£€ä¿®å®¹é‡é¢„æµ‹ä¿¡æ¯": "unit_maintenance_capacity_prediction", "å¤‡ç”¨é¢„æµ‹ä¿¡æ¯": "reserve_prediction",
            "é˜»å¡é¢„æµ‹ä¿¡æ¯": "congestion_prediction", "æ—¥å‰é˜»å¡æ–­é¢ä¿¡æ¯": "day_ahead_congestion_section",
            "å¿…å¼€å¿…åœæœºç»„ï¼ˆç¾¤ï¼‰çº¦æŸé¢„æµ‹ä¿¡æ¯": "must_run_stop_unit_constraint",
            "å¿…å¼€å¿…åœæœºç»„ä¿¡æ¯é¢„æµ‹ä¿¡æ¯": "must_run_stop_unit_info",
            "å¼€åœæœºä¸æ»¡è¶³æœ€å°çº¦æŸæ—¶é—´æœºç»„ä¿¡æ¯": "unit_constraint_violation_info",
            "å¿…å¼€å¿…åœå®¹é‡é¢„æµ‹ä¿¡æ¯": "must_run_stop_capacity",
            "å¸‚åœºæœºç»„æ€»å®¹é‡ï¼ˆMWï¼‰": "market_unit_total_capacity", "æ€»å®¹é‡ï¼ˆMWï¼‰": "total_capacity",
            "é˜»å¡ä¿¡æ¯": "congestion_info", "æŠ¥ä»·æ¨¡å¼": "quotation_mode", "è¿è¡Œæ—¥": "operation_date",
            # New mappings from user request
            "æ¸©åº¦": "temperature", "å¤©æ°”": "weather", "é£å‘": "wind_direction", "é£é€Ÿ": "wind_speed",
            "é™é›¨æ¦‚ç‡": "precipitation_probability", "ä½“æ„Ÿæ¸©åº¦": "apparent_temperature",
            "æ¹¿åº¦": "humidity", "ç´«å¤–çº¿": "uv_index", "äº‘é‡": "cloud_cover", "é™é›¨é‡": "rainfall",
            "æ˜ŸæœŸ": "week_day", "å¤©": "day",
            "ç»Ÿè°ƒé¢„æµ‹": "dispatch_forecast", "Aç±»ç”µæºé¢„æµ‹": "class_a_power_forecast",
            "Bç±»ç”µæºé¢„æµ‹": "class_b_power_forecast", "åœ°æ–¹ç”µæºé¢„æµ‹": "local_power_forecast",
            "è¥¿ç”µä¸œé€ç”µæºé¢„æµ‹": "west_to_east_power_forecast", "ç²¤æ¸¯æ¾³é¢„æµ‹": "guangdong_hongkong_macau_forecast",
            "å‘ç”µæ€»é¢„æµ‹": "total_generation_forecast", "ç°è´§æ–°èƒ½æºDæ—¥é¢„æµ‹": "spot_new_energy_day_ahead_forecast",
            "ç»Ÿè°ƒæ–°èƒ½æºå…‰ä¼é¢„æµ‹": "dispatch_new_energy_pv_forecast", "ç»Ÿè°ƒæ–°èƒ½æºé£ç”µé¢„æµ‹": "dispatch_new_energy_wind_forecast",
            "æ°´ç”µï¼ˆå«æŠ½è“„ï¼‰é¢„æµ‹": "hydro_power_forecast_incl_pumped", "æŠ½è“„å‡ºåŠ›é¢„æµ‹": "pumped_storage_output_forecast",
            "å®é™…ç»Ÿè°ƒè´Ÿè·": "actual_dispatch_load", "Aç±»ç”µæºå®é™…": "actual_class_a_power",
            "Bç±»ç”µæºå®é™…": "actual_class_b_power", "åœ°æ–¹ç”µæºå®é™…": "actual_local_power",
            "è¥¿ç”µä¸œé€å®é™…": "actual_west_to_east_power", "ç²¤æ¸¯è”ç»œå®é™…": "actual_guangdong_hongkong_link",
            "æ–°èƒ½æºæ€»å®é™…": "actual_total_new_energy", "æ°´ç”µå«æŠ½è“„å®é™…": "actual_hydro_power_incl_pumped",
            "ç»Ÿè°ƒè´Ÿè·åå·®": "dispatch_load_deviation",
        }

        reserved_cols = {"id", "record_date", "sheet_name", "type", "created_at"}

        def _sanitize_identifier(name):
            # SQLAlchemy çš„å‘½åå‚æ•°éœ€è¦â€œå®‰å…¨â€çš„ keyï¼ˆä¸èƒ½æœ‰ `:` ç­‰å­—ç¬¦ï¼‰ï¼ŒåŒæ—¶è¦é¿å…åˆ—åè¿‡é•¿ã€‚
            s = str(name).strip().lower()
            s = re.sub(r"[^0-9a-zA-Z_]+", "_", s)
            s = re.sub(r"_+", "_", s).strip("_")
            if not s:
                s = "col"
            if s[0].isdigit():
                s = f"c_{s}"
            # MySQL åˆ—åæœ€å¤§ 64 å­—ç¬¦
            return s[:64]

        def translate(name, used=None):
            clean = str(name).strip()
            mapped = translation_map.get(clean)
            if mapped is None:
                for k, v in translation_map.items():
                    if k in clean:
                        mapped = v
                        break
            if mapped is None:
                mapped = clean

            safe = _sanitize_identifier(mapped)
            if safe in reserved_cols:
                safe = f"col_{safe}"

            if used is not None:
                base = safe
                n = 1
                while safe in used or safe in reserved_cols:
                    suffix = f"_{n}"
                    safe = (base[: (64 - len(suffix))] + suffix) if len(base) + len(suffix) > 64 else base + suffix
                    n += 1
                used.add(safe)

            return safe

        # æŒ‰ sheet åˆ†ç»„
        sheet_groups = {}
        for r in valid_records:
            s_name = r.get('sheet_name', 'Unknown')
            if s_name not in sheet_groups:
                sheet_groups[s_name] = []
            sheet_groups[s_name].append(r)

        preview_data = []
        
        try:
            with self.db_manager.engine.begin() as conn:
                for sheet_name, sheet_records in sheet_groups.items():
                    # ç¡®å®šè¡¨å
                    base_sheet = re.sub(r'\d{4}[-/]?\d{1,2}[-/]?\d{1,2}', '', sheet_name).replace('()', '').strip()
                    table_suffix = translate(base_sheet) or "unknown"
                    table_name = f"imformation_pred_{table_suffix}".lower()
                    
                    # ç¡®å®šæ‰€æœ‰åˆ—
                    all_keys = set()
                    for r in sheet_records:
                        all_keys.update(r.keys())
                    
                    # ç§»é™¤ç³»ç»Ÿå­—æ®µä»¥é‡æ–°æ’åº
                    if 'record_date' in all_keys: all_keys.remove('record_date')
                    if 'sheet_name' in all_keys: all_keys.remove('sheet_name')
                    if 'type' in all_keys: all_keys.remove('type')
                    if 'data_type' in all_keys: all_keys.remove('data_type')
                    if 'created_at' in all_keys: all_keys.remove('created_at')
                    
                    # æ„å»ºåˆ—å®šä¹‰
                    col_defs = []
                    col_map = {} # åŸå§‹åˆ— -> å®‰å…¨åˆ—
                    used_cols = set()
                    dynamic_cols = []
                    
                    for k in sorted(list(all_keys)):
                        safe_col = translate(k, used=used_cols)
                        col_map[k] = safe_col
                        comment = str(k).replace("'", "''")
                        col_defs.append(f"`{safe_col}` text COMMENT '{comment}'")
                        dynamic_cols.append(safe_col)
                        
                    # åˆ›å»ºè¡¨ SQL
                    dynamic_section = (",".join(col_defs) + ",") if col_defs else ""
                    create_sql = f"""
                    CREATE TABLE IF NOT EXISTS `{table_name}` (
                        `id` bigint(20) NOT NULL AUTO_INCREMENT,
                        `record_date` date DEFAULT NULL,
                        `sheet_name` varchar(255) DEFAULT NULL,
                        `type` varchar(100) DEFAULT NULL,
                        {dynamic_section}
                        `created_at` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
                        PRIMARY KEY (`id`),
                        KEY `idx_record_date` (`record_date`)
                    ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
                    """
                    conn.execute(text(create_sql))
                    
                    # æ¸…ç†æ•°æ®
                    conn.execute(text(f"DELETE FROM `{table_name}` WHERE record_date = :date"), {'date': data_date})
                    
                    # å‡†å¤‡æ’å…¥æ•°æ®
                    insert_records = []
                    insert_cols = ['record_date', 'sheet_name', 'type'] + dynamic_cols
                    for r in sheet_records:
                        new_r = {c: None for c in insert_cols}
                        new_r['record_date'] = r.get('record_date', data_date)
                        new_r['sheet_name'] = r.get('sheet_name', sheet_name)
                        new_r['type'] = r.get('type') or r.get('data_type')
                        for orig_key, safe_key in col_map.items():
                            if orig_key in r:
                                new_r[safe_key] = r.get(orig_key)
                        insert_records.append(new_r)
                        
                    # æ’å…¥
                    if insert_records:
                        keys = insert_cols
                        values_clause = ", ".join([f":{k}" for k in keys])
                        columns_clause = ", ".join([f"`{k}`" for k in keys])
                        
                        stmt = text(f"INSERT INTO `{table_name}` ({columns_clause}) VALUES ({values_clause})")
                        conn.execute(stmt, insert_records)
                        
                        print(f"âœ… å·²ä¿å­˜ {len(insert_records)} æ¡è®°å½•åˆ° {table_name}")
                        if not preview_data:
                            preview_data = insert_records[:10]

            return True, None, len(valid_records), preview_data

        except Exception as e:
            print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False, None, 0, []

        

    def _process_channel_format(self, df, data_date, sheet_name, data_type):
        """å¤„ç†æœ‰'é€šé“åç§°'åˆ—çš„æ•°æ®æ ¼å¼"""
        records = []

        # ç›´æ¥ä½¿ç”¨æ‰€æœ‰æœ‰é€šé“åç§°çš„è¡Œ
        valid_rows = df[df["é€šé“åç§°"].notna()]
        if valid_rows.empty:
            print(f"âš ï¸ Sheetä¸­æ— æœ‰æ•ˆé€šé“ï¼Œé€šé“åˆ—å€¼ä¸º: {df['é€šé“åç§°'].unique().tolist()}")
            return records

        # æå–æ‰€æœ‰æ—¶é—´åˆ—ï¼ˆä¸€èˆ¬ä»00:00åˆ°23:45ï¼‰
        time_cols = [c for c in df.columns if re.match(r"\d{2}:\d{2}", c)]
        if not time_cols:
            print(f"âš ï¸ æ²¡æœ‰å‘ç°æ—¶é—´åˆ—: {list(df.columns)}")
            return records

        # éå†æ¯ä¸€è¡Œï¼ˆä¸€ä¸ªé€šé“ï¼‰
        for _, row in valid_rows.iterrows():
            channel_name = row["é€šé“åç§°"]

            for t in time_cols:
                # å¤„ç†NaNå€¼ï¼Œè·³è¿‡NULLå€¼
                value = row[t]
                if pd.isna(value):
                    continue  # è·³è¿‡è¿™ä¸ªè®°å½•
                
                record = {
                    "record_date": data_date,
                    "record_time": t,
                    "channel_name": channel_name,
                    "value": value,
                    "type": data_type,
                    "sheet_name": sheet_name,
                    "created_at": datetime.datetime.now(),
                }
                records.append(record)

        return records

    def _process_type_format(self, df, data_date, sheet_name, data_type):
        """å¤„ç†æœ‰'ç±»å‹'åˆ—çš„æ•°æ®æ ¼å¼"""
        records = []

        # ç›´æ¥ä½¿ç”¨æ‰€æœ‰æœ‰ç±»å‹åç§°çš„è¡Œ
        valid_rows = df[df["ç±»å‹"].notna()]
        if valid_rows.empty:
            print(f"âš ï¸ Sheetä¸­æ— æœ‰æ•ˆç±»å‹ï¼Œç±»å‹åˆ—å€¼ä¸º: {df['ç±»å‹'].unique().tolist()}")
            return records

        # æå–æ‰€æœ‰æ—¶é—´åˆ—ï¼ˆä¸€èˆ¬ä»00:00åˆ°23:45ï¼‰
        time_cols = [c for c in df.columns if re.match(r"\d{2}:\d{2}", c)]
        if not time_cols:
            print(f"âš ï¸ æ²¡æœ‰å‘ç°æ—¶é—´åˆ—: {list(df.columns)}")
            return records

        # éå†æ¯ä¸€è¡Œï¼ˆä¸€ä¸ªç±»å‹ï¼‰
        for _, row in valid_rows.iterrows():
            channel_name = row["ç±»å‹"]  # å°†"ç±»å‹"åˆ—çš„å€¼ä½œä¸ºchannel_name

            for t in time_cols:
                # å¤„ç†NaNå€¼ï¼Œè·³è¿‡NULLå€¼
                value = row[t]
                if pd.isna(value):
                    continue  # è·³è¿‡è¿™ä¸ªè®°å½•
                
                record = {
                    "record_date": data_date,
                    "record_time": t,
                    "channel_name": channel_name,
                    "value": value,
                    "type": data_type,
                    "sheet_name": sheet_name,
                    "created_at": datetime.datetime.now(),
                }
                records.append(record)

        return records
    # ä¿å­˜æ•°æ®åˆ°æ•°æ®åº“
    def save_to_database(self, records, data_date):
        """æŒ‰æ—¥æœŸè‡ªåŠ¨åˆ›å»ºè¡¨å¹¶ä¿å­˜æ•°æ®"""
        if not records:
            print("âŒ æ²¡æœ‰å¯ä¿å­˜çš„è®°å½•")
            return False, None, 0, []

        # ğŸ§© 1. å¦‚æœä¼ å…¥çš„æ˜¯ DataFrameï¼Œè½¬æˆ list[dict]
        if isinstance(records, pd.DataFrame):
            records = records.to_dict(orient="records")

        if not isinstance(records, list):
            print(f"âŒ records ç±»å‹é”™è¯¯: {type(records)}ï¼Œåº”ä¸º list[dict]")
            return False, None, 0, []

        def _coerce_numeric(v):
            if v is None or (isinstance(v, float) and np.isnan(v)):
                return None
            if isinstance(v, (int, float, np.number)) and not isinstance(v, bool):
                return float(v)
            if isinstance(v, str):
                s = v.strip().replace(",", "")
                if not s:
                    return None
                try:
                    return float(s)
                except Exception:
                    return None
            return None

        # ğŸ§© 2. è¿‡æ»¤æ— æ•ˆè®°å½•ï¼ˆå¹¶ä¿è¯ value å¯å†™å…¥ DECIMALï¼‰
        valid_records = []
        dropped_non_numeric = 0
        for i, r in enumerate(records):
            if not isinstance(r, dict):
                continue
            required_fields = ["record_date", "record_time", "channel_name", "value", "type", "sheet_name"]
            if not all(k in r for k in required_fields):
                continue
            # è½¬ record_date
            if isinstance(r["record_date"], str):
                r["record_date"] = pd.to_datetime(r["record_date"]).date()
            coerced = _coerce_numeric(r.get("value"))
            if coerced is None:
                dropped_non_numeric += 1
                continue
            r["value"] = coerced
            valid_records.append(r)

        if not valid_records:
            print("âŒ æ²¡æœ‰å¯ä¿å­˜çš„æœ‰æ•ˆè®°å½•")
            return False, None, 0, []
        if dropped_non_numeric:
            print(f"âš ï¸ å·²è·³è¿‡ {dropped_non_numeric} æ¡éæ•°å€¼ value è®°å½•ï¼ˆé¿å…å†™å…¥ power_data å¤±è´¥ï¼‰")

        # --- ç”ŸæˆæŒ‰å¤©è¡¨å ---
        table_name = f"power_data_{data_date.strftime('%Y%m%d')}"
        preview_data = []

        try:
            with self.db_manager.engine.begin() as conn:
                # --- åˆ›å»ºè¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰ ---
                create_table_sql = f"""
                CREATE TABLE IF NOT EXISTS {table_name} (
                    id BIGINT AUTO_INCREMENT PRIMARY KEY,
                    record_date DATE NOT NULL,
                    record_time TIME,
                    type VARCHAR(255),
                    channel_name VARCHAR(255),
                    value DECIMAL(10,2),
                    sheet_name VARCHAR(255)
                );
                """
                conn.execute(text(create_table_sql))
                print(f"âœ… è¡¨ {table_name} å·²å­˜åœ¨æˆ–åˆ›å»ºæˆåŠŸ")

                # --- æ‰¹é‡æ’å…¥ ---
                insert_stmt = text(f"""
                INSERT INTO {table_name} 
                (record_date, record_time, type, channel_name, value, sheet_name)
                VALUES (:record_date, :record_time, :type, :channel_name, :value, :sheet_name)
                """)

                batch_size = 200
                for i in range(0, len(valid_records), batch_size):
                    batch = valid_records[i:i + batch_size]
                    conn.execute(insert_stmt, batch)
                    print(f"ğŸ’¾ å·²æ’å…¥ç¬¬ {i // batch_size + 1} æ‰¹æ•°æ® ({len(batch)} æ¡)")

                count_stmt = text(f"SELECT COUNT(*) FROM {table_name} WHERE record_date = :record_date")
                count = conn.execute(count_stmt, {"record_date": data_date}).scalar()
                
                # è·å–å‰5è¡Œæ•°æ®é¢„è§ˆ
                preview_stmt = text(f"SELECT * FROM {table_name} WHERE record_date = :record_date ORDER BY id DESC LIMIT 5")
                result = conn.execute(preview_stmt, {"record_date": data_date})
                # ä¿®å¤ï¼šæ­£ç¡®å¤„ç†SQLAlchemyè¡Œå¯¹è±¡
                preview_data = []
                for row in result:
                    # å°†è¡Œå¯¹è±¡è½¬æ¢ä¸ºå­—å…¸
                    preview_data.append(dict(row._mapping))
                
                print(f"âœ… æ•°æ®åº“ä¿å­˜æˆåŠŸ: {count} æ¡è®°å½•")
                return True, table_name, count, preview_data

        except Exception as e:
            print(f"âŒ æ•°æ®åº“ä¿å­˜å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False, None, 0, []
    def save_to_outage_database(self, records, data_date):
        """ä¿å­˜åœç”µæ•°æ®åˆ°å›ºå®šè¡¨ power_outage"""
        if not records:
            print("âŒ æ²¡æœ‰å¯ä¿å­˜çš„è®°å½•")
            return False, None, 0, []

        # ğŸ§© 1. å¦‚æœä¼ å…¥çš„æ˜¯ DataFrameï¼Œè½¬æˆ list[dict]
        if isinstance(records, pd.DataFrame):
            records = records.to_dict(orient="records")

        if not isinstance(records, list):
            print(f"âŒ records ç±»å‹é”™è¯¯: {type(records)}ï¼Œåº”ä¸º list[dict]")
            return False, None, 0, []

        # ğŸ§© 2. è¿‡æ»¤æ— æ•ˆè®°å½•
        valid_records = []
        for i, r in enumerate(records):
            if not isinstance(r, dict):
                continue
            required_fields = ["device_name", "voltage_level", "device_type", "device_code", 
                        "planned_power_off_time", "actual_power_off_time", "planned_power_on_time","actual_power_on_time"]
            if not all(k in r for k in required_fields):
                continue
            # æ·»åŠ  record_date å­—æ®µ
            r["record_date"] = data_date
            valid_records.append(r)

        if not valid_records:
            print("âŒ æ²¡æœ‰å¯ä¿å­˜çš„æœ‰æ•ˆè®°å½•")
            return False, None, 0, []

        # --- ä½¿ç”¨å›ºå®šè¡¨å ---
        table_name = "power_outage"
        preview_data = []

        try:
            with self.db_manager.engine.begin() as conn:
                # --- åˆ›å»ºè¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰ ---
                create_table_sql = f"""
                CREATE TABLE IF NOT EXISTS `{table_name}` (
                    `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT 'è‡ªå¢ä¸»é”®',
                    `record_date` date NOT NULL COMMENT 'è®°å½•æ—¥æœŸ',
                    `device_name` varchar(200) NOT NULL COMMENT 'è®¾å¤‡åç§°ï¼ˆå¦‚101å˜å‹å™¨å¼€å…³ã€220kV#1ä¸»å˜ï¼‰',
                    `voltage_level` varchar(50) DEFAULT NULL COMMENT 'ç”µå‹ç­‰çº§ï¼ˆå…è®¸ä¸ºç©ºï¼Œéƒ¨åˆ†è®¾å¤‡å¯èƒ½æœªè®°å½•ï¼‰',
                    `device_type` varchar(100) NOT NULL COMMENT 'è®¾å¤‡ç±»å‹ï¼ˆå¦‚å¼€å…³ã€ä¸»å˜ã€æ¯çº¿ï¼‰',
                    `device_code` varchar(50) NOT NULL COMMENT 'è®¾å¤‡ç¼–å·ï¼ˆå”¯ä¸€æ ‡è¯†ï¼‰',
                    `planned_power_off_time` datetime DEFAULT NULL COMMENT 'è®¡åˆ’åœç”µæ—¥æœŸæ—¶é—´ï¼ˆæ ¼å¼ï¼šYYYY-MM-DD HH:MM:SSï¼‰',
                    `actual_power_off_time` datetime DEFAULT NULL COMMENT 'å®é™…åœç”µæ—¥æœŸæ—¶é—´ï¼ˆæ ¼å¼ï¼šYYYY-MM-DD HH:MM:SSï¼‰',
                    `planned_power_on_time` datetime DEFAULT NULL COMMENT 'è®¡åˆ’å¤ç”µæ—¥æœŸæ—¶é—´ï¼ˆæ ¼å¼ï¼šYYYY-MM-DD HH:MM:SSï¼‰',
                    `actual_power_on_time` datetime DEFAULT NULL COMMENT 'å®é™…å¤ç”µæ—¥æœŸæ—¶é—´ï¼ˆæ ¼å¼ï¼šYYYY-MM-DD HH:MM:SSï¼‰',
                    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT 'è®°å½•åˆ›å»ºæ—¶é—´',
                    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT 'è®°å½•æ›´æ–°æ—¶é—´',
                    `sheet_name` varchar(255) DEFAULT NULL COMMENT 'æ•°æ®æ¥æºè¡¨å',
                    PRIMARY KEY (`id`),
                    UNIQUE KEY `uk_device_code` (`device_code`) COMMENT 'è®¾å¤‡ç¼–å·å”¯ä¸€çº¦æŸ'
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='è®¾å¤‡åœç”µè®°å½•ä¿¡æ¯è¡¨';
                """
                conn.execute(text(create_table_sql))
                print(f"âœ… è¡¨ {table_name} å·²å­˜åœ¨æˆ–åˆ›å»ºæˆåŠŸ")

                # --- æ‰¹é‡æ’å…¥ ---
                insert_stmt = text(f"""
                INSERT IGNORE INTO {table_name} 
                (device_name, record_date, voltage_level, device_type, device_code, planned_power_off_time, actual_power_off_time, planned_power_on_time, actual_power_on_time, sheet_name)
                VALUES (:device_name, :record_date, :voltage_level, :device_type, :device_code, STR_TO_DATE(:planned_power_off_time, '%Y%m%d_%H:%i:%s'), STR_TO_DATE(:actual_power_off_time, '%Y%m%d_%H:%i:%s'), STR_TO_DATE(:planned_power_on_time, '%Y%m%d_%H:%i:%s'), STR_TO_DATE(:actual_power_on_time, '%Y%m%d_%H:%i:%s'), :sheet_name)
                """)

                batch_size = 200
                for i in range(0, len(valid_records), batch_size):
                    batch = valid_records[i:i + batch_size]
                    conn.execute(insert_stmt, batch)
                    print(f"ğŸ’¾ å·²æ’å…¥ç¬¬ {i // batch_size + 1} æ‰¹æ•°æ® ({len(batch)} æ¡)")
                # è·å–æ’å…¥çš„æ•°æ®æ€»é‡
                count_stmt = text(f"SELECT COUNT(*) FROM {table_name} WHERE record_date = :record_date")
                count = conn.execute(count_stmt, {"record_date": data_date}).scalar()

                print(f"âœ… {table_name} æ•°æ®åº“ä¿å­˜æˆåŠŸ: {count} æ¡è®°å½•")
                return True, table_name, count, []
        
        except Exception as e:
            print(f"âŒ æ•°æ®åº“ä¿å­˜å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False, None, 0, []    
    def save_to_ynjichu_database(self, records, data_date):
        """ä¿å­˜åœç”µæ•°æ®åˆ°å›ºå®šè¡¨ power_ynjichu"""
        if not records:
            print("âŒ æ²¡æœ‰å¯ä¿å­˜çš„è®°å½•")
            return True, None, 0, []

        # ğŸ§© 1. å¦‚æœä¼ å…¥çš„æ˜¯ DataFrameï¼Œè½¬æˆ list[dict]
        if isinstance(records, pd.DataFrame):
            records = records.to_dict(orient="records")

        if not isinstance(records, list):
            print(f"âŒ records ç±»å‹é”™è¯¯: {type(records)}ï¼Œåº”ä¸º list[dict]")
            return False, None, 0, []

        # ğŸ§© 2. è¿‡æ»¤æ— æ•ˆè®°å½•
        valid_records = []
        for i, r in enumerate(records):
            if not isinstance(r, dict):
                continue
            # æ·»åŠ  record_date å­—æ®µ
            r["record_date"] = data_date
            valid_records.append(r)

        if not valid_records:
            print("âŒ æ²¡æœ‰å¯ä¿å­˜çš„æœ‰æ•ˆè®°å½•")
            return True, None, 0, []

        # --- ä½¿ç”¨å›ºå®šè¡¨å ---
        table_name = "power_jizujichu"
        preview_data = []

        try:
            with self.db_manager.engine.begin() as conn:
                # --- åˆ›å»ºè¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰---
                create_table_sql = f"""
                CREATE TABLE IF NOT EXISTS `power_ynjichu` (
                `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT 'è‡ªå¢ä¸»é”®ï¼Œå”¯ä¸€æ ‡è¯†ä¸€æ¡è®°å½•',
                `record_date` date NOT NULL COMMENT 'è®°å½•æ—¥æœŸ',
                `unit_group_name` varchar(200) DEFAULT NULL COMMENT 'æœºç»„ç¾¤åï¼ˆå…è®¸ä¸ºç©ºï¼‰',
                `power_plant_id` varchar(50) DEFAULT NULL COMMENT 'ç”µå‚IDï¼ˆå…è®¸ä¸ºç©ºï¼‰',
                `power_plant_name` varchar(200) DEFAULT NULL COMMENT 'ç”µå‚åç§°ï¼ˆå…è®¸ä¸ºç©ºï¼‰',
                `unit_id` varchar(50) DEFAULT NULL COMMENT 'æœºç»„IDï¼ˆå…è®¸ä¸ºç©ºï¼‰',
                `unit_name` varchar(200) DEFAULT NULL COMMENT 'æœºç»„åç§°ï¼ˆå…è®¸ä¸ºç©ºï¼‰',
                `proportion` decimal(10,4) DEFAULT NULL COMMENT 'æ‰€å æ¯”ä¾‹ï¼ˆå…è®¸ä¸ºç©ºï¼Œå¦‚0.35è¡¨ç¤º35%ï¼‰',
                `sheet_name` varchar(255) DEFAULT NULL COMMENT 'æ•°æ®æ¥æºè¡¨åï¼ˆå…è®¸ä¸ºç©ºï¼‰',
                `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT 'è®°å½•å…¥åº“æ—¶é—´ï¼ˆè‡ªåŠ¨ç”Ÿæˆï¼‰',
                `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT 'è®°å½•æ›´æ–°æ—¶é—´ï¼ˆè‡ªåŠ¨æ›´æ–°ï¼‰',
                PRIMARY KEY (`id`),
                KEY `idx_unit_group` (`unit_group_name`) COMMENT 'æœºç»„ç¾¤åç´¢å¼•'
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='æœºç»„ç¾¤-æœºç»„åˆ†é…æ¯”ä¾‹è®°å½•è¡¨ï¼ˆæ‰€æœ‰å­—æ®µå…è®¸ä¸ºç©ºï¼‰';
                """
                conn.execute(text(create_table_sql))
                print(f"âœ… è¡¨ {table_name} å·²å­˜åœ¨æˆ–åˆ›å»ºæˆåŠŸ")

                # åˆ é™¤è¯¥æ—¥æœŸçš„æ—§æ•°æ®
                conn.execute(text(f"DELETE FROM {table_name} WHERE record_date = :record_date"), 
                             {"record_date": data_date})
                print(f"ğŸ—‘ï¸ å·²åˆ é™¤ {data_date} çš„æ—§æ•°æ®")

                # --- æ‰¹é‡æ’å…¥ ---
                insert_stmt = text(f"""
                INSERT IGNORE INTO {table_name} 
                (record_date, unit_group_name, power_plant_id, power_plant_name, unit_id, unit_name, proportion, sheet_name)
                VALUES 
                (:record_date, :unit_group_name, :power_plant_id, :power_plant_name, :unit_id, :unit_name, :proportion, :sheet_name)
                """)
                
                # æ‰¹é‡æ’å…¥æ•°æ®
                batch_size = 200
                for i in range(0, len(valid_records), batch_size):
                    batch = valid_records[i:i + batch_size]
                    conn.execute(insert_stmt, batch)
                    print(f"ğŸ’¾ å·²æ’å…¥ç¬¬ {i // batch_size + 1} æ‰¹æ•°æ® ({len(batch)} æ¡)")

                # è·å–æ’å…¥çš„æ•°æ®æ€»é‡
                count_stmt = text(f"SELECT COUNT(*) FROM {table_name} WHERE record_date = :record_date")
                count = conn.execute(count_stmt, {"record_date": data_date}).scalar()
                
                # è·å–é¢„è§ˆæ•°æ®
                preview_stmt = text(f"SELECT * FROM {table_name} WHERE record_date = :record_date LIMIT 5")
                preview_result = conn.execute(preview_stmt, {"record_date": data_date})
                for row in preview_result:
                    preview_data.append(dict(row._mapping))

                print(f"âœ… {table_name} æ•°æ®åº“ä¿å­˜æˆåŠŸ: {count} æ¡è®°å½•")
                return True, table_name, count, []

        except Exception as e:
            print(f"âŒ {table_name} æ•°æ®åº“ä¿å­˜å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False, None, 0, []
    
    def save_to_internal_database(self, records, data_date):
        """ä¿å­˜å‘ç”µæœºå¹²é¢„è®°å½•åˆ°å›ºå®šè¡¨ generator_intervention_records"""
        if not records:
            print("âŒ æ²¡æœ‰å¯ä¿å­˜çš„è®°å½•")
            return False, None, 0, []

        # ğŸ§© 1. å¦‚æœä¼ å…¥çš„æ˜¯ DataFrameï¼Œè½¬æˆ list[dict]
        if isinstance(records, pd.DataFrame):
            records = records.to_dict(orient="records")

        if not isinstance(records, list):
            print(f"âŒ records ç±»å‹é”™è¯¯: {type(records)}ï¼Œåº”ä¸º list[dict]")
            return False, None, 0, []

        # ğŸ§© 2. è¿‡æ»¤æ— æ•ˆè®°å½•
        valid_records = []
        for i, r in enumerate(records):
            if not isinstance(r, dict):
                continue
            required_fields = ["object_name", "object_id", "intervention_start_time", "intervention_end_time",
                               "pre_intervention_max", "pre_intervention_min", "post_intervention_max", "post_intervention_min",
                               "intervention_reason"]
            if not all(k in r for k in required_fields):
                continue
            r["record_date"] = data_date
            valid_records.append(r)

        if not valid_records:
            print("âŒ æ²¡æœ‰å¯ä¿å­˜çš„æœ‰æ•ˆè®°å½•")
            return False, None, 0, []

        # --- ä½¿ç”¨å›ºå®šè¡¨å ---
        table_name = "power_intervention"
        preview_data = []

        try:
            with self.db_manager.engine.begin() as conn:
                # --- åˆ›å»ºè¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰ ---
                create_table_sql = f"""
                CREATE TABLE IF NOT EXISTS `{table_name}` (
                  `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT 'è‡ªå¢ä¸»é”®',
                  `record_date` date NOT NULL COMMENT 'è®°å½•æ—¥æœŸ',
                  `sheet_name` varchar(255) DEFAULT NULL COMMENT 'æ•°æ®æ¥æºè¡¨å',
                  `object_name` varchar(200) NOT NULL COMMENT 'å¯¹è±¡åç§°ï¼ˆå¦‚ç‰›è¿œå‚#2å‘ç”µæœºï¼‰',
                  `object_id` varchar(50) NOT NULL COMMENT 'å¯¹è±¡IDï¼ˆå”¯ä¸€æ ‡è¯†ï¼Œå¦‚40813871689367554ï¼‰',
                  `intervention_start_time` datetime DEFAULT NULL COMMENT 'å¹²é¢„å¼€å§‹æ—¶é—´ï¼ˆæ ¼å¼ï¼šYYYY-MM-DD HH:MM:SSï¼‰',
                  `intervention_end_time` datetime DEFAULT NULL COMMENT 'å¹²é¢„ç»“æŸæ—¶é—´ï¼ˆæ ¼å¼ï¼šYYYY-MM-DD HH:MM:SSï¼‰',
                  `pre_intervention_max` decimal(10,3) DEFAULT NULL COMMENT 'å¹²é¢„å‰æœ€å¤§å€¼',
                  `pre_intervention_min` decimal(10,3) DEFAULT NULL COMMENT 'å¹²é¢„å‰æœ€å°å€¼',
                  `post_intervention_max` decimal(10,3) DEFAULT NULL COMMENT 'å¹²é¢„åæœ€å¤§å€¼',
                  `post_intervention_min` decimal(10,3) DEFAULT NULL COMMENT 'å¹²é¢„åæœ€å°å€¼',
                  `intervention_reason` varchar(500) DEFAULT NULL COMMENT 'å¹²é¢„åŸå› ï¼ˆå¦‚é…åˆç”µå‚å·¥ä½œ:ä¼˜åŒ–å¼€æœºæ›²çº¿ï¼‰',
                  `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT 'è®°å½•åˆ›å»ºæ—¶é—´',
                  `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT 'è®°å½•æ›´æ–°æ—¶é—´',
                  PRIMARY KEY (`id`),
                  KEY `idx_object_id` (`object_id`) COMMENT 'å¯¹è±¡IDç´¢å¼•ï¼Œç”¨äºå…³è”æŸ¥è¯¢',
                  KEY `idx_intervention_time` (`intervention_start_time`, `intervention_end_time`) COMMENT 'å¹²é¢„æ—¶é—´ç´¢å¼•ï¼Œç”¨äºæ—¶é—´èŒƒå›´æŸ¥è¯¢'
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='å‘ç”µæœºå¹²é¢„è®°å½•ä¿¡æ¯è¡¨';
                """
                conn.execute(text(create_table_sql))
                print(f"âœ… è¡¨ {table_name} å·²å­˜åœ¨æˆ–åˆ›å»ºæˆåŠŸ")

                # --- æ‰¹é‡æ’å…¥ ---
                insert_stmt = text(f"""
                INSERT IGNORE INTO {table_name} 
                (record_date, sheet_name, object_name, object_id, intervention_start_time, intervention_end_time, 
                 pre_intervention_max, pre_intervention_min, post_intervention_max, post_intervention_min, 
                 intervention_reason)
                VALUES (:record_date, :sheet_name, :object_name, :object_id, :intervention_start_time, :intervention_end_time,
                        :pre_intervention_max, :pre_intervention_min, :post_intervention_max, :post_intervention_min,
                        :intervention_reason)
                """)

                batch_size = 200
                for i in range(0, len(valid_records), batch_size):
                    batch = valid_records[i:i + batch_size]
                    conn.execute(insert_stmt, batch)
                    print(f"ğŸ’¾ å·²æ’å…¥ç¬¬ {i // batch_size + 1} æ‰¹æ•°æ® ({len(batch)} æ¡)")

                count_stmt = text(f"SELECT COUNT(*) FROM {table_name}")
                count = conn.execute(count_stmt).scalar()
                
                # è·å–å‰5è¡Œæ•°æ®é¢„è§ˆ
                preview_stmt = text(f"SELECT * FROM {table_name} ORDER BY id DESC LIMIT 5")
                result = conn.execute(preview_stmt)
                # ä¿®å¤ï¼šæ­£ç¡®å¤„ç†SQLAlchemyè¡Œå¯¹è±¡
                preview_data = []
                for row in result:
                    # å°†è¡Œå¯¹è±¡è½¬æ¢ä¸ºå­—å…¸
                    preview_data.append(dict(row._mapping))
                
                print(f"âœ… æ•°æ®åº“ä¿å­˜æˆåŠŸ: {count} æ¡è®°å½•")
                return True, table_name, count, []

        except Exception as e:
            print(f"âŒ æ•°æ®åº“ä¿å­˜å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False, None, 0, []
    
    def import_custom_excel(self, excel_file):
        """å¯¼å…¥æŒ‡å®šçš„5ä¸ªsheetï¼Œå¹¶æŒ‰å›ºå®šè§„åˆ™æ˜ å°„"""
        try:
            # è¯»å–æ‰€æœ‰sheet
            sheet_dict = pd.read_excel(excel_file, sheet_name=None, header=None)
        except Exception as e:
            print(f"âŒ æ— æ³•è¯»å–Excel: {e}")
            return False
        file_name = str(excel_file)
        
        chinese_match = re.search(r'([\u4e00-\u9fff]+)', file_name)
        if chinese_match:
            data_type = chinese_match.group(1) + "å®é™…ä¿¡æ¯"
            print(f"ğŸ“ æ–‡ä»¶ç±»å‹è¯†åˆ«: {data_type}")
        else:
            print(f"âš ï¸ æœªèƒ½åœ¨æ–‡ä»¶åä¸­æ‰¾åˆ°æ±‰å­—ï¼š{file_name}ï¼Œè·³è¿‡ã€‚")
            return False
        sheet_names = list(sheet_dict.keys())
        print(f"ğŸ“˜ æ£€æµ‹åˆ° {len(sheet_names)} ä¸ªSheet: {sheet_names}")

        # è¦å¤„ç†çš„sheetç¼–å·ï¼ˆ1-basedï¼‰
        target_indexes = [0, 1, 3, 4, 5,6,-2,-1]  # å¯¹åº”ç¬¬1,2,4,5,6ä¸ªsheet

        all_records = []
        outage_records = []
        ineternal_records = []

        for i in target_indexes:
            if i >= len(sheet_names):
                print(f"âš ï¸ Excelä¸­ä¸å­˜åœ¨ç¬¬{i+1}ä¸ªsheetï¼Œè·³è¿‡")
                continue

            sheet_name = sheet_names[i]
            df = sheet_dict[sheet_name]
            print(f"\nğŸ”¹ æ­£åœ¨å¤„ç† Sheet {i+1}: {sheet_name}")

            # ç»Ÿä¸€è¯†åˆ«æ—¥æœŸ
            data_date = self._extract_date_from_text(sheet_name) or self._extract_date_from_text(file_name)
            if not data_date:
                print(f"âš ï¸ æœªè¯†åˆ«åˆ°æ—¥æœŸ: {sheet_name}ï¼Œè·³è¿‡")
                continue
            # æ ¹æ®sheetåºå·è°ƒç”¨ä¸åŒæ˜ å°„å‡½æ•°
            if i in [0, 3, 4]:  # ç¬¬1,4,5ä¸ªsheetï¼šæ—¶åˆ»â†’channel_name
                records = self._process_time_as_channel(df, data_date, sheet_name, data_type)
            elif i in [1, 5]:  # ç¬¬2,6ä¸ªsheetï¼šç¬¬ä¸€è¡Œâ†’channel_name
                records = self._process_first_row_as_channel(df, data_date, sheet_name, data_type)
            elif i in [6]:
                records = self._process_fsc_as_channel(df, data_date, sheet_name, data_type)
            elif i in [-2]:
                try:
                    outage_records = self._process_outage_as_table(df, data_date, sheet_name)
                except Exception as e:
                    print(f"âš ï¸ åœç”µä¿¡æ¯è§£æå¤±è´¥ï¼Œå·²è·³è¿‡: {e}")
                    outage_records = []
            elif i in [-1]:
                try:
                    ineternal_records = self._process_internal_as_table(df, data_date, sheet_name)
                except Exception as e:
                    print(f"âš ï¸ æœºç»„å†…éƒ¨ä¿¡æ¯è§£æå¤±è´¥ï¼Œå·²è·³è¿‡: {e}")
                    ineternal_records = []
            else:
                print(f"âš ï¸ ç¬¬{i+1}ä¸ªsheetæœªå®šä¹‰å¤„ç†è§„åˆ™ï¼Œè·³è¿‡")
                continue

            print(f"âœ… Sheet{i+1} å¤„ç†å®Œæˆï¼Œå…± {len(records)} æ¡è®°å½•")
            all_records.extend(records)
        
        if not all_records:
            print("âŒ æ²¡æœ‰ç”Ÿæˆä»»ä½•æœ‰æ•ˆè®°å½•")
            return False

        success1, table_name1, count1, preview_data1 = self.save_to_database(all_records, data_date)
        if outage_records:
            success2, table_name2, count2, preview_data2 = self.save_to_outage_database(outage_records, data_date)
        else:
            success2, table_name2, count2, preview_data2 = True, None, 0, []
        if ineternal_records:
            success3, table_name3, count3, preview_data3 = self.save_to_internal_database(ineternal_records, data_date)
        else:
            success3, table_name3, count3, preview_data3 = True, None, 0, []
        
        # è¿”å›ä¸¤ä¸ªæ“ä½œçš„ç»“æœ
        return (success1, table_name1, count1, preview_data1), (success2, table_name2, count2, preview_data2),(success3, table_name3, count3, preview_data3)

    def _extract_date_from_text(self, text_value):
        """
        ä»æ–‡æœ¬ä¸­æå–æ—¥æœŸï¼ˆæ”¯æŒæ‹¬å·/ä¸­æ–‡æ‹¬å·/æ— æ‹¬å·çš„ YYYY-MM-DD æˆ– YYYYMMDDï¼‰
        è¿”å› date æˆ– None
        """
        if not text_value:
            return None
        text = str(text_value)
        patterns = [
            r"[ï¼ˆ(]\s*(\d{4}-\d{1,2}-\d{1,2})\s*[)ï¼‰]",
            r"(\d{4}-\d{1,2}-\d{1,2})",
            r"(\d{8})",
        ]
        for p in patterns:
            m = re.search(p, text)
            if not m:
                continue
            s = m.group(1)
            if len(s) == 8 and s.isdigit():
                s = f"{s[0:4]}-{s[4:6]}-{s[6:8]}"
            try:
                return datetime.datetime.strptime(s, "%Y-%m-%d").date()
            except Exception:
                continue
        return None
    def _process_time_as_channel(self, df, data_date, sheet_name, data_type):
        """å°†æ—¶åˆ»åˆ—åæ˜ å°„ä¸ºchannel_name"""
        records = []
        df = df.dropna(how="all")  # åˆ é™¤ç©ºè¡Œ

        # å¦‚æœç¬¬ä¸€åˆ—æ˜¯ â€œæ—¶åˆ»â€ å­—æ ·
        if str(df.iloc[0, 0]).strip() == "æ—¶åˆ»":
            df.columns = [str(c).strip() for c in df.iloc[0]]  # ç¬¬ä¸€è¡Œä½œåˆ—å
            df = df[1:]  # å»æ‰æ ‡é¢˜è¡Œ
        else:
            df.columns = [str(c).strip() for c in df.columns]

        # æŸ¥æ‰¾æ—¶é—´åˆ—ï¼ˆå½¢å¦‚ 00:00ã€01:15ï¼‰
        time_cols = [c for c in df.columns if re.match(r"\d{2}:\d{2}", c)]
        if not time_cols:
            print(f"âš ï¸ æœªæ‰¾åˆ°æ—¶é—´åˆ—: {df.columns.tolist()}")
            return []
        # éå†æ¯ä¸€è¡Œï¼ˆæ¯ä¸€ç±»æŒ‡æ ‡ï¼‰
        for _, row in df.iterrows():
            # è·³è¿‡æ— æ•ˆè¡Œæˆ–æ ‡é¢˜è¡Œ
            if not isinstance(row[time_cols[0]], (int, float)) and not str(row[time_cols[0]]).replace('.', '', 1).isdigit():
                continue

            # æŒ‡æ ‡åï¼ˆæ¯”å¦‚ â€œç»Ÿè°ƒè´Ÿè·(MW)â€ï¼‰
            indicator_name = str(row.get("æ—¶åˆ»") or row.index[0]).strip()

            for t in time_cols:
                value = row[t]
                if pd.isna(value):
                    continue
                try:
                    value = float(value)
                except:
                    continue  # è·³è¿‡éæ•°å€¼çš„å•å…ƒæ ¼
                record = {
                    "record_date": data_date,
                    "record_time": t,
                    "channel_name": indicator_name,  # ç”¨æŒ‡æ ‡åä½œé€šé“å
                    "value": value,
                    "type": data_type,
                    "sheet_name": sheet_name,
                    "created_at": datetime.datetime.now(),
                }
                records.append(record)
        return records
    def _process_fsc_as_channel(self, df, data_date, sheet_name, data_type):
        """å°†æ—¶åˆ»åˆ—åæ˜ å°„ä¸ºchannel_name"""
        records = []
        df = df.dropna(how="all")  # åˆ é™¤ç©ºè¡Œ
        if df.empty:
            print(f"è­¦å‘Šï¼šsheet '{sheet_name}' æ— æœ‰æ•ˆæ•°æ®ï¼ˆæ‰€æœ‰è¡Œéƒ½æ˜¯ç©ºè¡Œï¼‰")
            return records  # è¿”å›ç©ºåˆ—è¡¨ï¼Œé¿å…åç»­æŠ¥é”™
        df.columns = [str(c).strip() for c in df.iloc[0]]  # ç¬¬ä¸€è¡Œä½œåˆ—å
        df = df[1:]  # å»æ‰æ ‡é¢˜è¡Œ
        
        first_col = df.columns[0]
        second_col = df.columns[1]
       
        # æŸ¥æ‰¾æ—¶é—´åˆ—ï¼ˆå½¢å¦‚ 00:00ã€01:15 æˆ–æ•°å­—æ ¼å¼ 0, 1, 2...ï¼‰
        time_cols = [c for c in df.columns if re.match(r"\d{2}:\d{2}", c)]
        if not time_cols:
            print(f"âš ï¸ æœªæ‰¾åˆ°æ—¶é—´åˆ—: {df.columns.tolist()}")
            return []

        # éå†æ¯ä¸€è¡Œï¼ˆæ¯ä¸€ç±»æŒ‡æ ‡ï¼‰
        for _, row in df.iterrows():
            # è·³è¿‡æ— æ•ˆè¡Œæˆ–æ ‡é¢˜è¡Œ
            if not isinstance(row[time_cols[0]], (int, float)) and not str(row[time_cols[0]]).replace('.', '', 1).isdigit():
                continue
            
            # ç”Ÿæˆ channel_nameï¼šç¬¬ä¸€åˆ—å’Œç¬¬äºŒåˆ—ç”¨ä¸‹åˆ’çº¿è¿æ¥
            channel_name = f"{row[first_col]}_{row[second_col]}"

            for t in time_cols:
                value = row[t]
                if pd.isna(value):
                    continue
                try:
                    value = float(value)
                except:
                    continue  # è·³è¿‡éæ•°å€¼çš„å•å…ƒæ ¼

                record = {
                    "record_date": data_date,
                    "record_time": t,
                    "channel_name": channel_name,  # ç”¨æŒ‡æ ‡åä½œé€šé“å
                    "value": value,
                    "type": data_type,
                    "sheet_name": sheet_name,
                    "created_at": datetime.datetime.now(),
                }
                records.append(record)
        return records
    
    def _process_3_as_channel(self, df, data_date, sheet_name):
        """
        å¤„ç†è®¾å¤‡ç”µå‹ç­‰çº§ä¿¡æ¯sheetï¼Œæå–è®¾å¤‡ç”µå‹ç­‰çº§æ•°æ®
        """
        records = []
        df = df.dropna(how="all")  # åˆ é™¤ç©ºè¡Œ
        
        if df.empty:
            print(f"è­¦å‘Šï¼šsheet '{sheet_name}' æ— æœ‰æ•ˆæ•°æ®ï¼ˆæ‰€æœ‰è¡Œéƒ½æ˜¯ç©ºè¡Œï¼‰")
            return records  # è¿”å›ç©ºåˆ—è¡¨ï¼Œé¿å…åç»­æŠ¥é”™

        # ç¡®ä¿åˆ—åæ­£ç¡®
        df.columns = [str(c).strip() for c in df.columns]
        
        # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
        required_columns = ["åºå·", "æ—¥æœŸ", "è®¾å¤‡åç§°", "ç”µå‹ç­‰çº§(kV)"]
        if not all(col in df.columns for col in required_columns):
            print(f"âš ï¸  sheet '{sheet_name}' ç¼ºå°‘å¿…è¦çš„åˆ—: {required_columns}")
            return records

        # éå†æ¯ä¸€è¡Œæ•°æ®
        for _, row in df.iterrows():
            # è·³è¿‡ç©ºè¡Œ
            if pd.isna(row["åºå·"]) and pd.isna(row["æ—¥æœŸ"]) and pd.isna(row["è®¾å¤‡åç§°"]):
                continue
                
            # å¤„ç†åºå·å­—æ®µ
            def convert_serial_number(value):
                if pd.isna(value):
                    return None
                try:
                    return int(value)
                except:
                    return None

            record = {
                "serial_number": convert_serial_number(row["åºå·"]),
                "record_date": data_date,  # ä½¿ç”¨ç»Ÿä¸€çš„æ—¥æœŸ
                "device_name": str(row["è®¾å¤‡åç§°"]) if not pd.isna(row["è®¾å¤‡åç§°"]) else None,
                "voltage_level": str(row["ç”µå‹ç­‰çº§(kV)"]) if not pd.isna(row["ç”µå‹ç­‰çº§(kV)"]) else None,
                "sheet_name": sheet_name
            }
            records.append(record)
            
        print(f"âœ… Sheet '{sheet_name}' è§£æå®Œæˆï¼Œå…± {len(records)} æ¡è®°å½•")
        return records

    def _process_4_as_channel(self, df, data_date, sheet_name):
        """
        å¤„ç†æœºç»„åŸºç¡€ä¿¡æ¯sheetï¼Œæå–æœºç»„ç¾¤ã€ç”µå‚å’Œæœºç»„ä¿¡æ¯
        """
        records = []
        df = df.dropna(how="all")  # åˆ é™¤ç©ºè¡Œ
        
        if df.empty:
            print(f"è­¦å‘Šï¼šsheet '{sheet_name}' æ— æœ‰æ•ˆæ•°æ®ï¼ˆæ‰€æœ‰è¡Œéƒ½æ˜¯ç©ºè¡Œï¼‰")
            return records  # è¿”å›ç©ºåˆ—è¡¨ï¼Œé¿å…åç»­æŠ¥é”™

        # ç¡®ä¿åˆ—åæ­£ç¡®
        df.columns = [str(c).strip() for c in df.columns]
        
        # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
        required_columns = ["æœºç»„ç¾¤å", "ç”µå‚ID", "ç”µå‚åç§°", "æœºç»„ID", "æœºç»„åç§°", "æ‰€å æ¯”ä¾‹"]
        if not all(col in df.columns for col in required_columns):
            print(f"âš ï¸  sheet '{sheet_name}' ç¼ºå°‘å¿…è¦çš„åˆ—: {required_columns}")
            return records

        # éå†æ¯ä¸€è¡Œæ•°æ®
        for _, row in df.iterrows():
            # è·³è¿‡ç©ºè¡Œ
            if pd.isna(row["æœºç»„ç¾¤å"]) and pd.isna(row["ç”µå‚ID"]) and pd.isna(row["æœºç»„ID"]):
                continue
                
            record = {
                "record_date": data_date,
                "unit_group_name": str(row["æœºç»„ç¾¤å"]) if not pd.isna(row["æœºç»„ç¾¤å"]) else None,
                "power_plant_id": str(row["ç”µå‚ID"]) if not pd.isna(row["ç”µå‚ID"]) else None,
                "power_plant_name": str(row["ç”µå‚åç§°"]) if not pd.isna(row["ç”µå‚åç§°"]) else None,
                "unit_id": str(row["æœºç»„ID"]) if not pd.isna(row["æœºç»„ID"]) else None,
                "unit_name": str(row["æœºç»„åç§°"]) if not pd.isna(row["æœºç»„åç§°"]) else None,
                "proportion": float(row["æ‰€å æ¯”ä¾‹"]) if not pd.isna(row["æ‰€å æ¯”ä¾‹"]) else None,
                "sheet_name": sheet_name
            }
            records.append(record)
            
        print(f"âœ… Sheet '{sheet_name}' è§£æå®Œæˆï¼Œå…± {len(records)} æ¡è®°å½•")
        return records

    def _process_5_channel(self, df, data_date, sheet_name):
        """
        å¤„ç†æœºç»„çº¦æŸä¿¡æ¯sheetï¼Œæå–æœºç»„ç¾¤çº¦æŸé…ç½®
        """
        records = []
        df = df.dropna(how="all")  # åˆ é™¤ç©ºè¡Œ
        
        if df.empty:
            print(f"è­¦å‘Šï¼šsheet '{sheet_name}' æ— æœ‰æ•ˆæ•°æ®ï¼ˆæ‰€æœ‰è¡Œéƒ½æ˜¯ç©ºè¡Œï¼‰")
            return records  # è¿”å›ç©ºåˆ—è¡¨ï¼Œé¿å…åç»­æŠ¥é”™

        # ç¡®ä¿åˆ—åæ­£ç¡®
        df.columns = [str(c).strip() for c in df.columns]
        
        # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
        required_columns = ["æœºç»„ç¾¤å", "ç”Ÿæ•ˆæ—¶é—´", "å¤±æ•ˆæ—¶é—´", "ç”µåŠ›çº¦æŸ", "ç”µé‡çº¦æŸ", "æœ€å¤§è¿è¡Œæ–¹å¼çº¦æŸ", "æœ€å°è¿è¡Œæ–¹å¼çº¦æŸ", "æœ€å¤§ç”µé‡", "æœ€å°ç”µé‡"]
        if not all(col in df.columns for col in required_columns):
            print(f"âš ï¸  sheet '{sheet_name}' ç¼ºå°‘å¿…è¦çš„åˆ—: {required_columns}")
            return records

        # éå†æ¯ä¸€è¡Œæ•°æ®
        for _, row in df.iterrows():
            # è·³è¿‡ç©ºè¡Œ
            if pd.isna(row["æœºç»„ç¾¤å"]) and pd.isna(row["ç”Ÿæ•ˆæ—¶é—´"]) and pd.isna(row["å¤±æ•ˆæ—¶é—´"]):
                continue
                
            # å¤„ç†çº¦æŸå­—æ®µï¼Œå°†"æ˜¯"/"å¦"è½¬æ¢ä¸º1/0
            def convert_constraint(value):
                if pd.isna(value):
                    return None
                if str(value).strip() == "æ˜¯":
                    return 1
                elif str(value).strip() == "å¦":
                    return 0
                else:
                    return None
                    
            # å¤„ç†æ•°å€¼å­—æ®µ
            def convert_numeric(value):
                if pd.isna(value):
                    return None
                try:
                    return float(value)
                except:
                    return None

            record = {
                "record_date": data_date,
                "unit_group_name": str(row["æœºç»„ç¾¤å"]) if not pd.isna(row["æœºç»„ç¾¤å"]) else None,
                "effective_time": str(row["ç”Ÿæ•ˆæ—¶é—´"]) if not pd.isna(row["ç”Ÿæ•ˆæ—¶é—´"]) else None,
                "expire_time": str(row["å¤±æ•ˆæ—¶é—´"]) if not pd.isna(row["å¤±æ•ˆæ—¶é—´"]) else None,
                "power_constraint": convert_constraint(row["ç”µåŠ›çº¦æŸ"]),
                "electricity_constraint": convert_constraint(row["ç”µé‡çº¦æŸ"]),
                "max_operation_constraint": convert_constraint(row["æœ€å¤§è¿è¡Œæ–¹å¼çº¦æŸ"]),
                "min_operation_constraint": convert_constraint(row["æœ€å°è¿è¡Œæ–¹å¼çº¦æŸ"]),
                "max_electricity": convert_numeric(row["æœ€å¤§ç”µé‡"]),
                "min_electricity": convert_numeric(row["æœ€å°ç”µé‡"]),
                "sheet_name": sheet_name
            }
            records.append(record)
            
        print(f"âœ… Sheet '{sheet_name}' è§£æå®Œæˆï¼Œå…± {len(records)} æ¡è®°å½•")
        return records

    def _process_5_as_channel(self, df, data_date, sheet_name, data_type):
        """å°†æ—¶åˆ»åˆ—åæ˜ å°„ä¸ºchannel_name"""
        records = []
        df = df.dropna(how="all")  # åˆ é™¤ç©ºè¡Œ
        if df.empty:
            print(f"è­¦å‘Šï¼šsheet '{sheet_name}' æ— æœ‰æ•ˆæ•°æ®ï¼ˆæ‰€æœ‰è¡Œéƒ½æ˜¯ç©ºè¡Œï¼‰")
            return records  # è¿”å›ç©ºåˆ—è¡¨ï¼Œé¿å…åç»­æŠ¥é”™
        
        first_col = df.columns[0]
        second_col = df.columns[1]
       
        # æŸ¥æ‰¾æ—¶é—´åˆ—ï¼ˆå½¢å¦‚ 00:00ã€01:15 æˆ–æ•°å­—æ ¼å¼ 0, 1, 2...ï¼‰
        time_cols = [c for c in df.columns if re.match(r"\d{2}:\d{2}", c)]
        if not time_cols:
            print(f"âš ï¸ æœªæ‰¾åˆ°æ—¶é—´åˆ—: {df.columns.tolist()}")
            return []

        # éå†æ¯ä¸€è¡Œï¼ˆæ¯ä¸€ç±»æŒ‡æ ‡ï¼‰
        for _, row in df.iterrows():
            # è·³è¿‡æ— æ•ˆè¡Œæˆ–æ ‡é¢˜è¡Œ
            if not isinstance(row[time_cols[0]], (int, float)) and not str(row[time_cols[0]]).replace('.', '', 1).isdigit():
                continue
            
            # ç”Ÿæˆ channel_nameï¼šç¬¬ä¸€åˆ—å’Œç¬¬äºŒåˆ—ç”¨ä¸‹åˆ’çº¿è¿æ¥
            channel_name = f"{row[first_col]}_{row[second_col]}"

            for t in time_cols:
                value = row[t]
                if pd.isna(value):
                    continue
                try:
                    value = float(value)
                except:
                    continue  # è·³è¿‡éæ•°å€¼çš„å•å…ƒæ ¼

                record = {
                    "record_date": data_date,
                    "record_time": t,
                    "channel_name": channel_name,  # ç”¨æŒ‡æ ‡åä½œé€šé“å
                    "value": value,
                    "type": data_type,
                    "sheet_name": sheet_name,
                    "created_at": datetime.datetime.now(),
                }
                records.append(record)
        return records

    def _process_first_row_as_channel(self, df, data_date, sheet_name, data_type):
        """
        å¤„ç†æ ¼å¼ï¼š
        æœ€é«˜è´Ÿè·(MW) | æœ€ä½è´Ÿè·(MW) | å¹³å‡è´Ÿè·(MW)
        243330.375    | 182924.0156  | 212967.9509
        """
        records = []
        # åˆ é™¤ç©ºè¡Œä¸ç©ºåˆ—
        df = df.dropna(how="all").dropna(axis=1, how="all")
        if df.empty:
            print(f"âš ï¸ Sheet {sheet_name} ä¸ºç©ºï¼Œè·³è¿‡ã€‚")
            return records

        # ç¬¬ä¸€è¡Œä½œä¸º channel_name
        channel_names = [str(c).strip() for c in df.iloc[0].tolist()]
        df = df.iloc[1:]  # ä»ç¬¬äºŒè¡Œå¼€å§‹ä¸ºæ•°æ®
        if df.empty:
            print(f"âš ï¸ Sheet {sheet_name} ä»…æœ‰è¡¨å¤´ï¼Œæ— æ•°æ®ã€‚")
            return records

        for _, row in df.iterrows():
            for col_idx, value in enumerate(row):
                if col_idx >= len(channel_names):
                    continue
                if pd.isna(value):
                    continue
                record = {
                    "record_date": data_date,
                    "record_time": None,  # æ²¡æœ‰æ—¶é—´åˆ—
                    "channel_name": channel_names[col_idx],
                    "value": value,
                    "type": data_type,
                    "sheet_name": sheet_name,
                    "created_at": datetime.datetime.now(),
                }
                records.append(record)

        print(f"âœ… Sheet {sheet_name} è§£æå®Œæˆï¼Œå…± {len(records)} æ¡è®°å½•ã€‚")
        return records

    def import_custom_excel_pred(self, excel_file):
            """å¯¼å…¥æŒ‡å®šçš„5ä¸ªsheetï¼Œå¹¶æŒ‰å›ºå®šè§„åˆ™æ˜ å°„"""
            try:
                # è¯»å–æ‰€æœ‰sheet
                sheet_dict = pd.read_excel(excel_file, sheet_name=None, header=0)
            except Exception as e:
                print(f"âŒ æ— æ³•è¯»å–Excel: {e}")
                return False
            file_name = str(excel_file)
            single_data_date = self._extract_date_from_text(file_name)
            if not single_data_date:
                print(f"âš ï¸ æœªè¯†åˆ«åˆ°æ—¥æœŸï¼š{file_name}ï¼Œè·³è¿‡")
                return False
            print("è¯†åˆ«åˆ°çš„æ—¥æœŸï¼š", single_data_date)
            chinese_match = re.search(r'([\u4e00-\u9fff]+)', file_name)
            if chinese_match:
                data_type = chinese_match.group(1) + "é¢„æµ‹ä¿¡æ¯"
                print(f"ğŸ“ æ–‡ä»¶ç±»å‹è¯†åˆ«: {data_type}")
            else:
                print(f"âš ï¸ æœªèƒ½åœ¨æ–‡ä»¶åä¸­æ‰¾åˆ°æ±‰å­—ï¼š{file_name}ï¼Œè·³è¿‡ã€‚")
                return False
            sheet_names = list(sheet_dict.keys())
            print(f"ğŸ“˜ æ£€æµ‹åˆ° {len(sheet_names)} ä¸ªSheet: {sheet_names}")

            # è¦å¤„ç†çš„sheetç¼–å·ï¼ˆ1-basedï¼‰
            target_indexes = [0, 1, 2,3,4, 5,6,7,-5,-4,-3, -2, -1]  # å¯¹åº”ç¬¬1,2,4,5,6ä¸ªsheet

            all_records = []
            jichu_records = []
            yueshu_records = []
            ynjichu_records = []
            jizujichu_records = []
            jizuyueshu_records = []
            ynyueshu_records = []
            shubiandian_records = []

            for i in target_indexes:
                if i >= len(sheet_names):
                    print(f"âš ï¸ Excelä¸­ä¸å­˜åœ¨ç¬¬{i+1}ä¸ªsheetï¼Œè·³è¿‡")
                    continue

                sheet_name = sheet_names[i]
                df = sheet_dict[sheet_name]
                print(f"\nğŸ”¹ æ­£åœ¨å¤„ç† Sheet {i+1}: {sheet_name}")

                # ç»Ÿä¸€è¯†åˆ«æ—¥æœŸ
                data_date = self._extract_date_from_text(sheet_name) or single_data_date
                if data_date:
                    print("è¯†åˆ«åˆ°çš„æ—¥æœŸï¼š", data_date)  # è¾“å‡ºï¼šè¯†åˆ«åˆ°çš„æ—¥æœŸï¼š2025-09-01ï¼ˆè‹¥è¾“å…¥æ˜¯2025-09-1ï¼Œä¼šè‡ªåŠ¨è¡¥0ä¸º2025-09-01ï¼‰
                else:
                    print("æœªè¯†åˆ«åˆ°æ—¥æœŸæ ¼å¼")

                # æ ¹æ®sheetåºå·è°ƒç”¨ä¸åŒæ˜ å°„å‡½æ•°
                if i in [0]:  # ç¬¬1ä¸ªsheetï¼šæ—¶åˆ»â†’channel_name
                    records = self._process_time_as_channel(df, data_date, sheet_name, data_type)
                elif i in [1]: 
                    records = self._process_1_channel(df, data_date, sheet_name, data_type)
                elif i in [2]:  # ç¬¬3ä¸ªsheetï¼šæ—¶åˆ»â†’channel_name
                    records = self._process_type_date_value(df, data_date, sheet_name, data_type)
                elif i in [3]: 
                    shubiandian_records = self._process_3_as_channel(df, data_date, sheet_name)
                elif i in [4]:  # ç¬¬4ä¸ªsheetï¼šç¬¬ä¸€è¡Œâ†’channel_name
                    jizujichu_records = self._process_4_as_channel(df, data_date, sheet_name)
                elif i in [5]:  # ç¬¬5ä¸ªsheetï¼šæ—¶åˆ»â†’channel_name
                    jizuyueshu_records = self._process_5_channel(df, data_date, sheet_name)
                elif i in [-5]:  # ç¬¬6ä¸ªsheetï¼šæ—¶åˆ»â†’channel_name
                    ynyueshu_records = self._process_5_channel(df, single_data_date, sheet_name)
                elif i in [-3]:  # ç¬¬4,5ä¸ªsheetï¼šæ—¶åˆ»â†’channel_name
                    records = self._process_3_channel(df, data_date, sheet_name, data_type)
                elif i in [-2, -1]:  # ç¬¬7,8ä¸ªsheetï¼šç¬¬ä¸€è¡Œâ†’channel_name
                    records = self._process_2_channel(df, data_date, sheet_name, data_type)
                elif i in [-4,6]:  # ç¬¬9ä¸ªsheet
                    records = self._process_5_as_channel(df, single_data_date, sheet_name, data_type)
                elif i in [7]:
                    ynjichu_records = self._process_4_as_channel(df, single_data_date, sheet_name)
                
                else:
                    print(f"âš ï¸ ç¬¬{i+1}ä¸ªsheetæœªå®šä¹‰å¤„ç†è§„åˆ™ï¼Œè·³è¿‡")
                    continue

                print(f"âœ… Sheet{i+1} å¤„ç†å®Œæˆï¼Œå…± {len(records)} æ¡è®°å½•")
                all_records.extend(records)
               
               
            jichu_records.extend(ynjichu_records)
            jichu_records.extend(jizujichu_records)
            yueshu_records.extend(jizuyueshu_records)
            yueshu_records.extend(ynyueshu_records)
                
            if not all_records:
                print("âŒ æ²¡æœ‰ç”Ÿæˆä»»ä½•æœ‰æ•ˆè®°å½•")
                return False
           
            success1, table_name1, count1, preview_data1 = self.save_to_database(all_records, data_date)
            success2, table_name2, count2, preview_data2 = self.save_to_jizujichu_database(jichu_records, data_date)
            success4, table_name4, count4, preview_data4 = self.save_to_jizuyueshu_database(yueshu_records, data_date)
            success5, table_name5, count5, preview_data5 = self.save_to_shubiandian_database(shubiandian_records, data_date)

            # è¿”å›ä¸¤ä¸ªæ“ä½œçš„ç»“æœ
            return (success1, table_name1, count1, preview_data1), (success2, table_name2, count2, preview_data2), (success4, table_name4, count4, preview_data4), (success5, table_name5, count5, preview_data5)

            # return self.save_to_database(all_records, data_date)
    
    def _process_1_channel(self, df, data_date, sheet_name, data_type):
        """
        å¤šæŒ‡æ ‡æ—¶åˆ»å‹sheetå¤„ç†ï¼š
        - è¯†åˆ«â€œç±»å‹ + ç”µæºç±»å‹â€ä¸º channel_name
        - ä½¿ç”¨â€œæ—¥æœŸâ€åˆ—ä½œä¸º record_date
        - æ—¶é—´åˆ—ä¸º 00:00ã€00:15 ç­‰å¸¸è§„æ ¼å¼
        """
        import datetime
        import pandas as pd
        import re

        records = []
        df = df.dropna(how="all")
        df.columns = [str(c).strip() for c in df.columns]

        # 1ï¸âƒ£ æ‰¾æ—¶é—´åˆ—ï¼ˆå¦‚ 00:00ã€01:15 ç­‰ï¼‰
        time_cols = [c for c in df.columns if re.match(r"^\d{1,2}:\d{2}$", c)]
        if not time_cols:
            print(f"âš ï¸ æœªæ‰¾åˆ°æ—¶é—´åˆ—: {df.columns.tolist()}")
            return []

        # 2ï¸âƒ£ è¯†åˆ«è¾…åŠ©åˆ—
        col_type = "ç±»å‹" if "ç±»å‹" in df.columns else None
        col_date = "æ—¥æœŸ" if "æ—¥æœŸ" in df.columns else None
        col_power = "ç”µæºç±»å‹" if "ç”µæºç±»å‹" in df.columns else None

        # 3ï¸âƒ£ éå†æ¯ä¸€è¡Œï¼ˆæ¯ä¸ªé€šé“ï¼‰
        for _, row in df.iterrows():
            # --- æ—¥æœŸåˆ— ---
            record_date = data_date
            if col_date and pd.notna(row[col_date]):
                try:
                    # è‡ªåŠ¨è¯†åˆ«æ—¥æœŸæ ¼å¼
                    record_date = pd.to_datetime(str(row[col_date]), errors="coerce").date()
                except:
                    record_date = data_date

            # --- é€šé“åï¼šç±»å‹ + ç”µæºç±»å‹ ---
            parts = []
            if col_type and pd.notna(row[col_type]):
                parts.append(str(row[col_type]).strip())
            if col_power and pd.notna(row[col_power]):
                parts.append(str(row[col_power]).strip())
            if not parts:
                continue
            channel_name = "-".join(parts)

            # --- éå†æ—¶é—´åˆ— ---
            for t in time_cols:
                value = row[t]
                if pd.isna(value):
                    continue
                try:
                    value = float(value)
                except:
                    continue

                records.append({
                    "record_date": record_date,        # ç¡®ä¿æ˜¯ date ç±»å‹
                    "record_time": t,                  # å¦‚ 00:00
                    "channel_name": channel_name,      # å¦‚ "ç°è´§æ–°èƒ½æºæ€»å‡ºåŠ›(MW)-é£ç”µ"
                    "value": value,
                    "type": data_type,
                    "sheet_name": sheet_name,
                    "created_at": datetime.datetime.now(),
                })

        print(f"âœ… {sheet_name} è§£æå®Œæˆï¼Œå…± {len(records)} æ¡è®°å½•")
        return records

    def _process_3_channel(self, df, data_date, sheet_name, data_type):
        """
        å°†å¤šåˆ—é€šé“å‹Sheetå¤„ç†æˆè®°å½•åˆ—è¡¨ï¼Œæ¯åˆ—è§†ä¸ºä¸€ä¸ªé€šé“ã€‚
        ç»“æ„ç¤ºä¾‹ï¼š
        åºå· | æ—¥æœŸ | å¿…å¼€æœºç»„å®¹é‡(MW) | å¿…åœæœºç»„å®¹é‡(MW)
        """
        import datetime
        import pandas as pd

        print(f"ğŸ”¹ æ­£åœ¨å¤„ç† Sheet: {sheet_name}")

        records = []

        # 1ï¸âƒ£ åˆ é™¤æ— ç”¨åˆ—
        if "åºå·" in df.columns:
            df = df.drop(columns=["åºå·"])

        # 2ï¸âƒ£ ç¡®ä¿æ—¥æœŸå­—æ®µå­˜åœ¨
        if "æ—¥æœŸ" not in df.columns:
            print(f"âš ï¸ æœªæ‰¾åˆ°æ—¥æœŸåˆ—ï¼Œè·³è¿‡ {sheet_name}")
            return []

        df = df.dropna(how="all")
        df.columns = [str(c).strip() for c in df.columns]

        # 3ï¸âƒ£ éå†æ¯ä¸€è¡Œ
        for _, row in df.iterrows():
            # æ—¥æœŸ
            record_date = data_date
            if pd.notna(row["æ—¥æœŸ"]):
                try:
                    record_date = pd.to_datetime(str(row["æ—¥æœŸ"]), errors="coerce").date()
                except:
                    record_date = data_date

            # 4ï¸âƒ£ éå†é€šé“åˆ—ï¼ˆé™¤â€œæ—¥æœŸâ€å¤–ï¼‰
            for col in df.columns:
                if col in ["æ—¥æœŸ"]:
                    continue
                value = row[col]
                if pd.isna(value):
                    continue
                try:
                    value = float(value)
                except:
                    continue

                records.append({
                    "record_date": record_date,
                    "record_time": None,
                    "channel_name": col,
                    "value": value,
                    "type": data_type,
                    "sheet_name": sheet_name,
                    "created_at": datetime.datetime.now(),
                })

        print(f"âœ… {sheet_name} å¤„ç†å®Œæˆï¼Œå…± {len(records)} æ¡è®°å½•")
        return records
    
    def _process_2_channel(self, df, data_date, sheet_name, data_type):
        """
        å¤„ç†æœºç»„åå•è¡¨ï¼š
        - channel_name = ç”µå‚åç§°-æœºç»„åç§°-ç±»å‹
        - value é»˜è®¤ä¸º 1
        """
        import datetime
        import pandas as pd

        records = []

        if df.empty:
            print(f"âš ï¸ {sheet_name} è¡¨ä¸ºç©ºï¼Œè·³è¿‡")
            return []

        df = df.dropna(how="all")
        df.columns = [str(c).strip() for c in df.columns]

        # å¿…è¦åˆ—
        col_date = "æ—¥æœŸ" if "æ—¥æœŸ" in df.columns else None
        col_plant = "ç”µå‚åç§°" if "ç”µå‚åç§°" in df.columns else None
        col_unit = "æœºç»„åç§°" if "æœºç»„åç§°" in df.columns else None
        col_type = "ç±»å‹" if "ç±»å‹" in df.columns else None

        for _, row in df.iterrows():
            # æ—¥æœŸ
            record_date = data_date
            if col_date and pd.notna(row[col_date]):
                try:
                    record_date = pd.to_datetime(str(row[col_date]), errors="coerce").date()
                except:
                    record_date = data_date

            # channel_name æ‹¼æ¥
            parts = []
            for col in [col_plant, col_unit, col_type]:
                if col and pd.notna(row[col]):
                    parts.append(str(row[col]).strip())
            if not parts:
                continue
            channel_name = "-".join(parts)

            # æ·»åŠ è®°å½•
            records.append({
                "record_date": record_date,
                "channel_name": channel_name,
                "record_time": None,
                "value": None,
                "type": data_type,
                "sheet_name": sheet_name,
                "created_at": datetime.datetime.now(),
            })

        print(f"âœ… {sheet_name} å¤„ç†å®Œæˆï¼Œå…± {len(records)} æ¡è®°å½•")
        return records

    def _process_type_date_value(self, df, data_date, sheet_name, data_type):
        """å¤„ç†ç±»ä¼¼ 'ç±»å‹ æ—¥æœŸ æ•°å€¼' çš„ç»“æ„ï¼ˆæ— æ—¶é—´åˆ—ï¼Œrecord_dateä¸ºdateç±»å‹ï¼‰"""
        records = []
        df = df.dropna(how="all")
        df.columns = [str(c).strip() for c in df.columns]

        # æŸ¥æ‰¾åˆ—
        col_type = "ç±»å‹" if "ç±»å‹" in df.columns else None
        col_date = "æ—¥æœŸ" if "æ—¥æœŸ" in df.columns else None

        # æŸ¥æ‰¾æ•°å€¼åˆ—ï¼ˆæ’é™¤æ‰å·²çŸ¥åˆ—ï¼‰
        value_cols = [c for c in df.columns if c not in [col_type, col_date]]
        if not value_cols:
            print(f"âš ï¸ æœªæ‰¾åˆ°æ•°å€¼åˆ—: {df.columns.tolist()}")
            return []

        value_col = value_cols[0]  # é»˜è®¤åªå–ç¬¬ä¸€åˆ—æ•°å€¼

        for _, row in df.iterrows():
            channel_name = str(row[col_type]).strip() if col_type else "æœªçŸ¥ç±»å‹"
            raw_date = str(row[col_date]).strip() if col_date and pd.notna(row[col_date]) else None

            # === æ—¥æœŸè§£æé€»è¾‘ ===
            parsed_date = None
            if raw_date:
                # 1. å¦‚æœæ˜¯æ ‡å‡†æ—¥æœŸæ ¼å¼
                try:
                    parsed_date = pd.to_datetime(raw_date).date()
                except Exception:
                    pass

                # 2. å¦‚æœæ˜¯å½¢å¦‚ â€œ2025å¹´ç¬¬38å‘¨(09.15~09.21)â€
                if parsed_date is None:
                    match = re.search(r"\((\d{2})\.(\d{2})", raw_date)
                    year_match = re.search(r"(\d{4})å¹´", raw_date)
                    if match and year_match:
                        year = int(year_match.group(1))
                        month = int(match.group(1))
                        day = int(match.group(2))
                        parsed_date = datetime.date(year, month, day)

            # å¦‚æœéƒ½è§£æå¤±è´¥ï¼Œåˆ™ç”¨ data_date å…œåº•
            if parsed_date is None:
                parsed_date = pd.to_datetime(data_date).date()

            # æ•°å€¼
            try:
                value = float(row[value_col])
            except:
                continue

            record = {
                "record_date": parsed_date,
                "record_time": datetime.datetime.now().time(),
                "channel_name": channel_name,
                "value": value,
                "type": data_type,
                "sheet_name": sheet_name,
                "created_at": datetime.datetime.now(),
            }
            records.append(record)

        return records
    
    def save_to_shubiandian_database(self, records, data_date):
        """ä¿å­˜è¾“å˜ç”µä¿¡æ¯åˆ°æ•°æ®åº“"""
        if not records:
            return True, None, 0, []
            
        table_suffix = data_date.strftime("%Y%m%d")
        table_name = f"power_substation_device_{table_suffix}"
        
        try:
            with self.db_manager.engine.begin() as conn:
                # åˆ›å»ºè¡¨
                create_sql = f"""
                CREATE TABLE IF NOT EXISTS {table_name} (
                    id BIGINT AUTO_INCREMENT PRIMARY KEY,
                    serial_number INT,
                    record_date DATE,
                    device_name VARCHAR(255),
                    voltage_level VARCHAR(50),
                    sheet_name VARCHAR(255),
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
                """
                conn.execute(text(create_sql))
                
                # åˆ é™¤æ—§æ•°æ®
                conn.execute(text(f"DELETE FROM {table_name} WHERE record_date = :d"), {"d": data_date})
                
                # æ’å…¥æ•°æ®
                insert_sql = text(f"""
                INSERT INTO {table_name} (serial_number, record_date, device_name, voltage_level, sheet_name)
                VALUES (:serial_number, :record_date, :device_name, :voltage_level, :sheet_name)
                """)
                
                conn.execute(insert_sql, records)
                
                count = len(records)
                print(f"âœ… {table_name} ä¿å­˜æˆåŠŸ: {count} æ¡")
                return True, table_name, count, records[:5]
                
        except Exception as e:
            print(f"âŒ {table_name} ä¿å­˜å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False, None, 0, []

    def save_to_jizujichu_database(self, records, data_date):
        """ä¿å­˜æœºç»„åŸºç¡€ä¿¡æ¯åˆ°æ•°æ®åº“"""
        if not records:
            return True, None, 0, []
            
        table_suffix = data_date.strftime("%Y%m%d")
        table_name = f"power_unit_basic_{table_suffix}"
        
        try:
            with self.db_manager.engine.begin() as conn:
                # åˆ›å»ºè¡¨
                create_sql = f"""
                CREATE TABLE IF NOT EXISTS {table_name} (
                    id BIGINT AUTO_INCREMENT PRIMARY KEY,
                    record_date DATE,
                    unit_group_name VARCHAR(255),
                    power_plant_id VARCHAR(100),
                    power_plant_name VARCHAR(255),
                    unit_id VARCHAR(100),
                    unit_name VARCHAR(255),
                    proportion FLOAT,
                    sheet_name VARCHAR(255),
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
                """
                conn.execute(text(create_sql))
                
                # åˆ é™¤æ—§æ•°æ®
                conn.execute(text(f"DELETE FROM {table_name} WHERE record_date = :d"), {"d": data_date})
                
                # æ’å…¥æ•°æ®
                insert_sql = text(f"""
                INSERT INTO {table_name} (record_date, unit_group_name, power_plant_id, power_plant_name, unit_id, unit_name, proportion, sheet_name)
                VALUES (:record_date, :unit_group_name, :power_plant_id, :power_plant_name, :unit_id, :unit_name, :proportion, :sheet_name)
                """)
                
                conn.execute(insert_sql, records)
                
                count = len(records)
                print(f"âœ… {table_name} ä¿å­˜æˆåŠŸ: {count} æ¡")
                return True, table_name, count, records[:5]
                
        except Exception as e:
            print(f"âŒ {table_name} ä¿å­˜å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False, None, 0, []

    def save_to_jizuyueshu_database(self, records, data_date):
        """ä¿å­˜æœºç»„çº¦æŸä¿¡æ¯åˆ°æ•°æ®åº“"""
        if not records:
            return True, None, 0, []
            
        table_suffix = data_date.strftime("%Y%m%d")
        table_name = f"power_unit_constraint_{table_suffix}"
        
        try:
            with self.db_manager.engine.begin() as conn:
                # åˆ›å»ºè¡¨
                create_sql = f"""
                CREATE TABLE IF NOT EXISTS {table_name} (
                    id BIGINT AUTO_INCREMENT PRIMARY KEY,
                    record_date DATE,
                    unit_group_name VARCHAR(255),
                    effective_time VARCHAR(50),
                    expire_time VARCHAR(50),
                    power_constraint INT COMMENT '1=æ˜¯, 0=å¦',
                    electricity_constraint INT COMMENT '1=æ˜¯, 0=å¦',
                    max_operation_constraint INT COMMENT '1=æ˜¯, 0=å¦',
                    min_operation_constraint INT COMMENT '1=æ˜¯, 0=å¦',
                    max_electricity FLOAT,
                    min_electricity FLOAT,
                    sheet_name VARCHAR(255),
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
                """
                conn.execute(text(create_sql))
                
                # åˆ é™¤æ—§æ•°æ®
                conn.execute(text(f"DELETE FROM {table_name} WHERE record_date = :d"), {"d": data_date})
                
                # æ’å…¥æ•°æ®
                insert_sql = text(f"""
                INSERT INTO {table_name} (record_date, unit_group_name, effective_time, expire_time, 
                    power_constraint, electricity_constraint, max_operation_constraint, min_operation_constraint, 
                    max_electricity, min_electricity, sheet_name)
                VALUES (:record_date, :unit_group_name, :effective_time, :expire_time, 
                    :power_constraint, :electricity_constraint, :max_operation_constraint, :min_operation_constraint, 
                    :max_electricity, :min_electricity, :sheet_name)
                """)
                
                conn.execute(insert_sql, records)
                
                count = len(records)
                print(f"âœ… {table_name} ä¿å­˜æˆåŠŸ: {count} æ¡")
                return True, table_name, count, records[:5]
                
        except Exception as e:
            print(f"âŒ {table_name} ä¿å­˜å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False, None, 0, []

    def import_point_data(self, excel_file):
        """è‡ªåŠ¨å¯¼å…¥Excelç¬¬ä¸€ä¸ªSheetçš„æ•°æ®ï¼Œå¹¶æŒ‰åˆ—æ±‚å‡å€¼"""
        import re
        import datetime
        import pandas as pd

        try:
            xls = pd.ExcelFile(excel_file)
            first_sheet_name = xls.sheet_names[0]  # âœ… è·å–ç¬¬ä¸€ä¸ª sheet å
            df = pd.read_excel(excel_file, sheet_name=first_sheet_name, header=0)
            print(f"âœ… æˆåŠŸè¯»å– Excel: {excel_file}, sheet: {first_sheet_name}")
        except Exception as e:
            print(f"âŒ è¯»å– Excel å¤±è´¥: {e}")
            return False, None, 0, []

        # è‡ªåŠ¨è¯†åˆ«æ—¥æœŸ
        # é¦–å…ˆå°è¯•åŒ¹é…æ‹¬å·ä¸­çš„æ—¥æœŸæ ¼å¼ "(2025-09-29)"
        match = re.search(r"\((\d{4}-\d{2}-\d{2})\)", first_sheet_name)
        if match:
            data_date = datetime.datetime.strptime(match.group(1), "%Y-%m-%d").date()
        else:
            # å¦‚æœæ²¡æœ‰æ‹¬å·ï¼Œåˆ™å°è¯•ç›´æ¥åŒ¹é…æ—¥æœŸæ ¼å¼ "2025-09-29"
            match = re.search(r"(\d{4}-\d{2}-\d{2})", first_sheet_name)
            if match:
                data_date = datetime.datetime.strptime(match.group(1), "%Y-%m-%d").date()
            else:
                print(f"âŒ æ— æ³•ä» sheet åç§° '{first_sheet_name}' ä¸­æå–æ—¥æœŸ")
                return False, None, 0, []

        # æ ¹æ®æ–‡ä»¶åè¯†åˆ«ç±»å‹
        file_name = os.path.basename(str(excel_file)) # ç¡®ä¿åªå–æ–‡ä»¶å
        chinese_match = re.search(r'([\u4e00-\u9fff]+)', file_name)
        if chinese_match:
            data_type = chinese_match.group(1)
            # ä¿®æ­£: å¦‚æœè¯†åˆ«å‡ºçš„ data_type åŒ…å« "æŸ¥è¯¢" å­—æ ·ï¼Œå»æ‰å®ƒï¼Œä¿æŒç®€æ´
            data_type = data_type.replace("æŸ¥è¯¢", "")
            print(f"ğŸ“ æ–‡ä»¶ç±»å‹è¯†åˆ«: {data_type}")
        else:
            print(f"âš ï¸ æœªèƒ½åœ¨æ–‡ä»¶åä¸­æ‰¾åˆ°æ±‰å­—ï¼š{file_name}ï¼Œè·³è¿‡ã€‚")
            return False, None, 0, []
        data_type = "å¹¿ä¸œ_" + data_type
        print(f"\nğŸ“˜ æ­£åœ¨å¤„ç† {first_sheet_name} | æ—¥æœŸ: {data_date} | ç±»å‹: {data_type}")
    
        # æŒ‰åˆ—æ±‚å‡å€¼å¹¶ç”Ÿæˆ records
        records = self.process_mean_by_column(df, data_date, first_sheet_name, data_type)

        if not records:
            print("âŒ æ²¡æœ‰ä»»ä½•æœ‰æ•ˆæ•°æ®è¢«å¯¼å…¥")
            return False, None, 0, []

        # ä¿å­˜åˆ°æ•°æ®åº“
        success, table_name, record_count, preview_data = self.save_to_database(records, data_date)
        print(f"âœ… æ•°æ®ä¿å­˜æˆåŠŸï¼Œè¡¨å: {table_name}ï¼Œè®°å½•æ•°: {record_count}")
        return success, table_name, record_count, preview_data
    def process_mean_by_column(self, df, data_date, sheet_name, data_type):
        """
        é’ˆå¯¹èŠ‚ç‚¹ç”µä»·ç­‰è¡¨æ ¼ï¼šå¯¹æ¯ä¸€åˆ—ï¼ˆä»ç¬¬3åˆ—å¼€å§‹ï¼‰æ±‚å‡å€¼ï¼Œå¹¶ç”Ÿæˆè®°å½•
        æ¯ä¸€åˆ—ã®å‡å€¤ãƒ‡ãƒ¼ã‚¿æ”¾åœ¨æœ€å¾Œï¼Œå…¶ä»–ãƒ‡ãƒ¼ã‚¿æŒ‰é †åºéƒ½å­˜ä¸€ä¸‹
        """
        records = []

        # æ ‡å‡†åŒ–åˆ—å
        df.columns = [str(c).strip() for c in df.columns]
        # print(f"COLUMNS: {df.columns.tolist()}")

        # è·å–æ—¶é—´åˆ—ï¼ˆç¬¬3åˆ—åŠä¹‹åï¼‰
        time_cols = df.columns[2:]
        if time_cols.empty or len(time_cols) == 0:
            print(f"âš ï¸ Sheet {sheet_name} æ²¡æœ‰å‘ç°æ—¶é—´åˆ—")
            return records

        # å°†æ—¶é—´åˆ—æŒ‰æ¯4ä¸ªåˆ†ç»„ï¼ˆæ¯å°æ—¶4ä¸ª15åˆ†é’Ÿé—´éš”ï¼‰
        time_groups = {}
        for t in time_cols:
            # ä» "HH:MM" æ ¼å¼ä¸­æå–å°æ—¶
            hour = t.split(':')[0]
            if hour not in time_groups:
                time_groups[hour] = []
            time_groups[hour].append(t)

        # å…ˆä¿å­˜åŸæœ‰çš„æ•°æ®ï¼ˆæŒ‰å°æ—¶åˆ†ç»„ï¼‰
        # é¢„å…ˆè®¡ç®—æ¯è¡Œæ¯å°æ—¶çš„å‡å€¼
        hourly_means = {}  # {(row_index, hour): mean_value}
        city_hour_values = {}  # {city: {hour: [values]}}
        
        for _, row in df.iterrows():
            # æ£€æŸ¥ç¬¬ä¸€åˆ—æ˜¯å¦æœ‰æœ‰æ•ˆæ•°æ®ï¼Œå¦‚æœæ²¡æœ‰åˆ™è·³è¿‡ï¼ˆå¤„ç†æ ‡é¢˜è¡Œï¼‰
            channel_name = row.iloc[0]  # ç¬¬ä¸€åˆ—ä½œä¸ºé€šé“åç§°
            if pd.isna(channel_name) or channel_name == "":
                continue
            city_name = self._get_city_from_node(channel_name)
                
            # ä¸ºæ¯è¡Œæ¯å°æ—¶è®¡ç®—å‡å€¼
            for hour, times in time_groups.items():
                # è®¡ç®—è¯¥å°æ—¶å†…å››ä¸ªæ—¶é—´ç‚¹çš„å‡å€¼
                values = []
                for t in times:
                    value = row[t]
                    if not pd.isna(value):
                        values.append(value)
                
                # å¦‚æœæœ‰æœ‰æ•ˆå€¼ï¼Œåˆ™è®¡ç®—å‡å€¼
                if values:
                    hourly_mean = sum(values) / len(values)
                    hourly_means[(_, hour)] = hourly_mean
                    
                    record = {
                        "record_date": pd.to_datetime(data_date).date(),
                        "record_time": f"{hour}:00",  # æŒ‰å°æ—¶å­˜å‚¨
                        "channel_name": channel_name,
                        "value": round(hourly_mean, 2),  # ä½¿ç”¨è¯¥å°æ—¶å†…å››ä¸ªæ—¶é—´ç‚¹çš„å‡å€¤
                        "type": data_type,
                        "sheet_name": sheet_name,
                        "created_at": pd.Timestamp.now(),
                    }
                    records.append(record)
                    if city_name:
                        city_hour_values.setdefault(city_name, {}).setdefault(hour, []).append(hourly_mean)

        for hour, times in time_groups.items():
            values = []
            for t in times:
                mean_value = df[t].mean()
                values.append(mean_value)
            if values:
                overall_mean = sum(values) / len(values)
                record = {
                    "record_date": pd.to_datetime(data_date).date(),
                    "record_time": f"{hour}:00",
                    "channel_name": f"{data_type}_å‡å€¼",
                    "value": round(overall_mean, 2),
                    "type": str(data_type),
                    "sheet_name": sheet_name,
                    "created_at": pd.Timestamp.now(),
                }
                records.append(record)

        # ç”ŸæˆåŸå¸‚èŠ‚ç‚¹å‡ä»·
        if city_hour_values:
            for city_name, hour_map in city_hour_values.items():
                for hour, vals in hour_map.items():
                    vals = [v for v in vals if v is not None]
                    if not vals:
                        continue
                    city_mean = sum(vals) / len(vals)
                    records.append({
                        "record_date": pd.to_datetime(data_date).date(),
                        "record_time": f"{hour}:00",
                        "channel_name": self._city_channel_name(city_name),
                        "value": round(city_mean, 2),
                        "type": data_type,
                        "sheet_name": sheet_name,
                        "created_at": pd.Timestamp.now(),
                    })

        print(f"âœ… {data_type} å‡å€¼ç”Ÿæˆ {len(records)} æ¡è®°å½•")
        return records

    def import_point_data_new(self, excel_file):
        """è‡ªåŠ¨å¯¼å…¥Excelç¬¬ä¸€ä¸ªSheetçš„æ•°æ®ï¼Œå¹¶æŒ‰åˆ—æ±‚å‡å€¼"""
        import re
        import datetime
        import pandas as pd

        try:
            xls = pd.ExcelFile(excel_file)
            first_sheet_name = xls.sheet_names[0]  # âœ… è·å–ç¬¬ä¸€ä¸ª sheet å
            df = pd.read_excel(excel_file, sheet_name=first_sheet_name, header=1)
            print(f"âœ… æˆåŠŸè¯»å– Excel: {excel_file}, sheet: {first_sheet_name}")
        except Exception as e:
            print(f"âŒ è¯»å– Excel å¤±è´¥: {e}")
            return False, None, 0, []
        
        # è‡ªåŠ¨è¯†åˆ«æ—¥æœŸ
        # é¦–å…ˆå°è¯•åŒ¹é…æ‹¬å·ä¸­çš„æ—¥æœŸæ ¼å¼ "(2025-09-29)"
        match = re.search(r"\((\d{4}-\d{2}-\d{2})\)", first_sheet_name)
        if match:
            data_date = datetime.datetime.strptime(match.group(1), "%Y-%m-%d").date()
        else:
            # å¦‚æœæ²¡æœ‰æ‹¬å·ï¼Œåˆ™å°è¯•ç›´æ¥åŒ¹é…æ—¥æœŸæ ¼å¼ "2025-09-29"
            match = re.search(r"(\d{4}-\d{2}-\d{2})", first_sheet_name)
            if match:
                data_date = datetime.datetime.strptime(match.group(1), "%Y-%m-%d").date()
            else:
                print(f"âŒ æ— æ³•ä» sheet åç§° '{first_sheet_name}' ä¸­æå–æ—¥æœŸ")
                return False, None, 0, []

        # æ ¹æ®æ–‡ä»¶åè¯†åˆ«ç±»å‹
        file_name = str(excel_file)
        file_name = os.path.basename(file_name)
        chinese_match = re.search(r'([\u4e00-\u9fff]+)', file_name)
        if chinese_match:
            data_type = chinese_match.group(1)
            print(f"ğŸ“ æ–‡ä»¶ç±»å‹è¯†åˆ«: {data_type}")
        else:
            print(f"âš ï¸ æœªèƒ½åœ¨æ–‡ä»¶åä¸­æ‰¾åˆ°æ±‰å­—ï¼š{file_name}ï¼Œè·³è¿‡ã€‚")
            return False, None, 0, []

        print(f"\nğŸ“˜ æ­£åœ¨å¤„ç† {first_sheet_name} | æ—¥æœŸ: {data_date} | ç±»å‹: {data_type}")

        # æŒ‰åˆ—æ±‚å‡å€¼å¹¶ç”Ÿæˆ records
        records = self.process_point_new(df, data_date, first_sheet_name, data_type)

        if not records:
            print("âŒ æ²¡æœ‰ä»»ä½•æœ‰æ•ˆæ•°æ®è¢«å¯¼å…¥")
            return False, None, 0, []

        # ä¿å­˜åˆ°æ•°æ®åº“
        success, table_name, record_count, preview_data = self.save_to_database(records, data_date)
        print(f"âœ… æ•°æ®ä¿å­˜æˆåŠŸï¼Œè¡¨å: {table_name}ï¼Œè®°å½•æ•°: {record_count}")
        return success, table_name, record_count, preview_data
    
    def process_point_new(self, df, data_date, sheet_name, data_type):
        """
        é’ˆå¯¹èŠ‚ç‚¹ç”µä»·ç­‰è¡¨æ ¼ï¼šæŒ‰åŒºåŸŸåˆ’åˆ†è®¡ç®—æ¯å°æ—¶å‡å€¼
        """
        records = []

        # æ ‡å‡†åŒ–åˆ—å
        df.columns = [str(c).strip() for c in df.columns]

        # è·å–æ—¶é—´åˆ—ï¼ˆç¬¬3åˆ—åŠä¹‹åï¼‰
        time_cols = df.columns[2:]
        if time_cols.empty or len(time_cols) == 0:
            print(f"âš ï¸ Sheet {sheet_name} æ²¡æœ‰å‘ç°æ—¶é—´åˆ—")
            return records

        # å°†æ—¶é—´åˆ—æŒ‰æ¯4ä¸ªåˆ†ç»„ï¼ˆæ¯å°æ—¶4ä¸ª15åˆ†é’Ÿé—´éš”ï¼‰
        time_groups = {}
        for t in time_cols:
            hour = t.split(':')[0]
            if hour not in time_groups:
                time_groups[hour] = []
            time_groups[hour].append(t)

        # å…ˆä¿å­˜åŸæœ‰çš„æ•°æ®ï¼ˆæŒ‰å°æ—¶åˆ†ç»„ï¼‰
        for _, row in df.iterrows():
            region_name = row.iloc[0]
            region_name_clean = str(region_name).strip()

            # åªå¤„ç†å¹¿ä¸œå’Œäº‘å—ï¼Œæ’é™¤å…¶ä»–åœ°åŒº
            if "å¹¿ä¸œ" not in region_name_clean and "äº‘å—" not in region_name_clean:
                continue
                
            # æ£€æŸ¥ç¬¬ä¸€åˆ—æ˜¯å¦æœ‰æœ‰æ•ˆæ•°æ®
            channel_name = row.iloc[1]
            if pd.isna(channel_name) or channel_name == "":
                continue
                
            # ä¸ºæ¯è¡Œæ¯å°æ—¶è®¡ç®—å‡å€¼
            for hour, times in time_groups.items():
                values = []
                for t in times:
                    value = row[t]
                    if not pd.isna(value):
                        values.append(value)
                
                if values:
                    hourly_mean = sum(values) / len(values)
                    record = {
                        "record_date": pd.to_datetime(data_date).date(),
                        "record_time": f"{hour}:00",
                        "channel_name": channel_name,
                        "value": round(hourly_mean, 2),
                        "type": region_name + "_" + data_type,
                        "sheet_name": sheet_name,
                        "created_at": pd.Timestamp.now(),
                    }
                    records.append(record)

        # æŒ‰åŒºåŸŸåˆ†ç»„è®¡ç®—æ¯å°æ—¶çš„å‡å€¼
        region_groups = {}

        # å…ˆæŒ‰åŒºåŸŸåˆ†ç»„æ•°æ®
        for _, row in df.iterrows():
            region_name = row.iloc[0]
            region_name_clean = str(region_name).strip()

            # åªå¤„ç†å¹¿ä¸œå’Œäº‘å—ï¼Œæ’é™¤å…¶ä»–åœ°åŒº
            if "å¹¿ä¸œ" not in region_name_clean and "äº‘å—" not in region_name_clean:
                continue
                
            # æ£€æŸ¥ç¬¬ä¸€åˆ—æ˜¯å¦æœ‰æœ‰æ•ˆæ•°æ®
            channel_name = row.iloc[1]
            if pd.isna(channel_name) or channel_name == "":
                continue
            
            # åˆå§‹åŒ–åœ°åŒºåˆ†ç»„
            if region_name not in region_groups:
                region_groups[region_name] = []
            
            region_groups[region_name].append(row)

        # ä¸ºæ¯ä¸ªåŒºåŸŸè®¡ç®—æ¯å°æ—¶çš„å‡å€¼
        for region_name, region_rows in region_groups.items():
            for hour, times in time_groups.items():
                # è·å–è¯¥åŒºåŸŸæ‰€æœ‰è¡Œåœ¨è¿™äº›æ—¶é—´ç‚¹çš„å€¼å¹¶è®¡ç®—å‡å€¼
                values = []
                for t in times:
                    # è®¡ç®—è¯¥æ—¶é—´ç‚¹åœ¨è¯¥åŒºåŸŸæ‰€æœ‰è¡Œä¸­çš„å‡å€¼
                    region_values = [row[t] for row in region_rows if not pd.isna(row[t])]
                    if region_values:
                        mean_value = sum(region_values) / len(region_values)
                        values.append(mean_value)
                
                # è®¡ç®—4ä¸ªæ—¶é—´ç‚¹çš„æ€»å‡å€¼
                if values:
                    overall_mean = sum(values) / len(values)
                    record = {
                        "record_date": pd.to_datetime(data_date).date(),
                        "record_time": f"{hour}:00",
                        "channel_name": f"{data_type}_å‡å€¼",
                        "value": round(overall_mean, 2),
                        "type": region_name + "_" + data_type,  # æŒ‰åŒºåŸŸåŒºåˆ†
                        "sheet_name": sheet_name,
                        "created_at": pd.Timestamp.now(),
                    }
                    records.append(record)

        print(f"âœ… {data_type} ç”Ÿæˆ {len(records)} æ¡è®°å½•")
        return records
    def import_imformation_true(self, excel_file):
        """è‡ªåŠ¨ç”Ÿæˆçš„å¯¼å…¥å‡½æ•°: ä¿¡æ¯æŠ«éœ²æŸ¥è¯¢å®é™…ä¿¡æ¯(2025-12-23).xlsx (ç±»)"""
        try:
            sheet_dict = pd.read_excel(excel_file, sheet_name=None, header=0)
        except Exception as e:
            print(f"âŒ æ— æ³•è¯»å–Excel: {e}")
            return False, None, 0, []
        file_name = str(excel_file)
        chinese_match = re.search(r'([\u4e00-\u9fff]+)', file_name)
        if chinese_match:
                data_type = chinese_match.group(1)
                print(f"ğŸ“ æ–‡ä»¶ç±»å‹è¯†åˆ«: {data_type}")
        else:
            print(f"âš ï¸ æœªèƒ½åœ¨æ–‡ä»¶åä¸­æ‰¾åˆ°æ±‰å­—ï¼š{file_name}ï¼Œè·³è¿‡ã€‚")
            return False
        all_records = []
        jizuchuli_records = []
        data_date = None
        
        # å°è¯•ä»æ–‡ä»¶åæå–æ—¥æœŸ
        match = re.search(r'(\d{4}-\d{1,2}-\d{1,2})', str(excel_file))
        if match:
            data_date = datetime.datetime.strptime(match.group(1), '%Y-%m-%d').date()
        else:
            # å¦‚æœæ–‡ä»¶åæ²¡æ—¥æœŸï¼Œå°è¯•ç”¨å½“å¤©æˆ–æŠ›å‡ºè­¦å‘Š
            print(f"âš ï¸ æœªèƒ½åœ¨æ–‡ä»¶åä¸­è¯†åˆ«æ—¥æœŸï¼Œé»˜è®¤ä½¿ç”¨ä»Šæ—¥")
            data_date = datetime.date.today()

        sheet_names = list(sheet_dict.keys())

        # å¤„ç†ç¬¬ 1 ä¸ª Sheet (åŸå: è´Ÿè·å®é™…ä¿¡æ¯(2025-12-23))
        if len(sheet_names) > 0:
            current_sheet_name = sheet_names[0]
            records = self._process_imformation_true_sheet_1(sheet_dict[current_sheet_name], data_date, current_sheet_name,data_type)
            all_records.extend(records)

        # å¤„ç†ç¬¬ 2 ä¸ª Sheet (åŸå: åœ°æ–¹ç”µå®é™…ä¿¡æ¯(2025-12-23))
        if len(sheet_names) > 1:
            current_sheet_name = sheet_names[1]
            records = self._process_imformation_true_sheet_2(sheet_dict[current_sheet_name], data_date, current_sheet_name,data_type)
            all_records.extend(records)

        # å¤„ç†ç¬¬ 3 ä¸ª Sheet (åŸå: è¥¿ç”µä¸œé€å„é€šé“å®é™…ä¿¡æ¯(2025-12-23))
        if len(sheet_names) > 2:
            current_sheet_name = sheet_names[2]
            records = self._process_imformation_true_sheet_3(sheet_dict[current_sheet_name], data_date, current_sheet_name,data_type)
            all_records.extend(records)


        # å¤„ç†ç¬¬ 5 ä¸ª Sheet (åŸå: å¤‡ç”¨å®é™…ä¿¡æ¯(2025-12-23))
        if len(sheet_names) > 4:
            current_sheet_name = sheet_names[4]
            records = self._process_imformation_true_sheet_5(sheet_dict[current_sheet_name], data_date, current_sheet_name, data_type)
            all_records.extend(records)

        # å¤„ç†ç¬¬ 6 ä¸ª Sheet (åŸå: å®æ—¶å‡ºæ¸…æ–­é¢(2025-12-23))
        if len(sheet_names) > 5:
            current_sheet_name = sheet_names[5]
            records = self._process_imformation_true_sheet_6(sheet_dict[current_sheet_name], data_date, current_sheet_name,data_type)
            all_records.extend(records)

        # å¤„ç†ç¬¬ 7 ä¸ª Sheet (åŸå: å®é™…æ–­é¢(2025-12-23))
        if len(sheet_names) > 6:
            current_sheet_name = sheet_names[6]
            records = self._process_imformation_true_sheet_7(sheet_dict[current_sheet_name], data_date, current_sheet_name,data_type)
            all_records.extend(records)

        # å¤„ç†ç¬¬ 9 ä¸ª Sheet (åŸå: æœºç»„å‡ºåŠ›å—é™æƒ…å†µ(2025-12-23))
        if len(sheet_names) > 8:
            current_sheet_name = sheet_names[8]
            records = self._process_imformation_true_sheet_9(sheet_dict[current_sheet_name], data_date, current_sheet_name)
            jizuchuli_records.extend(records)


        # å¤„ç†ç¬¬ 13 ä¸ª Sheet (åŸå: è¾“å˜ç”µè®¾å¤‡æ£€ä¿®è®¡åˆ’æ‰§è¡Œæƒ…å†µ(2025-12-23))
        # if len(sheet_names) > 12:
        #     current_sheet_name = sheet_names[12]
        #     records = self._process_imformation_true_sheet_13(sheet_dict[current_sheet_name], data_date, current_sheet_name)
        #     all_records.extend(records)

        # å¤„ç†ç¬¬ 15 ä¸ª Sheet (åŸå: çº¿è·¯åœè¿æƒ…å†µ(2025-12-23))
        if len(sheet_names) > 14:
            current_sheet_name = sheet_names[14]
            records = self._process_imformation_true_sheet_15(sheet_dict[current_sheet_name], data_date, current_sheet_name, data_type)
            all_records.extend(records)

        # å¤„ç†ç¬¬ 16 ä¸ª Sheet (åŸå: æœºç»„å‡ºåŠ›æƒ…å†µ(2025-12-23))
        if len(sheet_names) > 15:
            current_sheet_name = sheet_names[15]
            records = self._process_imformation_true_sheet_16(sheet_dict[current_sheet_name], data_date, current_sheet_name, data_type)
            all_records.extend(records)
            
        if not all_records:
            print("âŒ æ²¡æœ‰ç”Ÿæˆä»»ä½•æœ‰æ•ˆè®°å½•")
            return False, None, 0, []

        # ä¿å­˜åˆ°æ•°æ®åº“ (é»˜è®¤ä½¿ç”¨é€šç”¨ä¿å­˜æ–¹æ³•ï¼Œå¯æ ¹æ®éœ€è¦ä¿®æ”¹)
        return (self.save_to_database(all_records, data_date)),(self.save_to_generator_tech_database(jizuchuli_records, data_date))

    def _process_imformation_true_sheet_1(self, df, data_date, sheet_name,data_type):
        """è‡ªåŠ¨ç”Ÿæˆçš„å¤„ç†å‡½æ•°: è´Ÿè·å®é™…ä¿¡æ¯(2025-12-23) (æ¨¡å¼: time_series_matrix)"""
        records = []
        df = df.dropna(how='all')
        df.columns = [str(c).strip() for c in df.columns]
        
        # è¯†åˆ«æ—¶é—´åˆ—
        time_cols = [c for c in df.columns if re.match(r'^\d{1,2}:\d{2}$', c)]

        # è¿‡æ»¤æ‰â€œç±»å‹â€ç­‰éæŒ‡æ ‡è¡Œï¼Œé¿å…é€šé“åç§°è¢«æ±¡æŸ“
        if "ç±»å‹" in df.columns and "é€šé“åç§°" in df.columns:
            df = df[df["ç±»å‹"].astype(str).str.strip().isin(["å®é™…"])]
        
        for _, row in df.iterrows():
            # ç»Ÿä¸€ä½¿ç”¨â€œé€šé“åç§°â€ä½œä¸ºæŒ‡æ ‡
            channel_name = str(row.get('é€šé“åç§°', 'Unknown')).strip()
            
            for t in time_cols:
                val = row[t]
                if pd.isna(val): continue
                
                records.append({
                    'record_date': data_date,
                    'record_time': t,
                    'channel_name': channel_name,
                    'value': val,
                    'sheet_name': sheet_name,
                    'type': data_type,
                    'created_at': datetime.datetime.now()
                })
        return records

    def _process_imformation_true_sheet_2(self, df, data_date, sheet_name,data_type):
        """è‡ªåŠ¨ç”Ÿæˆçš„å¤„ç†å‡½æ•°: åœ°æ–¹ç”µå®é™…ä¿¡æ¯(2025-12-23) (æ¨¡å¼: time_series_matrix)"""
        records = []
        df = df.dropna(how='all')
        df.columns = [str(c).strip() for c in df.columns]
        
        # è¯†åˆ«æ—¶é—´åˆ—
        time_cols = [c for c in df.columns if re.match(r'^\d{1,2}:\d{2}$', c)]

        # è¿‡æ»¤æ‰â€œç±»å‹â€ç­‰éæŒ‡æ ‡è¡Œï¼Œé¿å…é€šé“åç§°è¢«æ±¡æŸ“
        if "ç±»å‹" in df.columns and "é€šé“åç§°" in df.columns:
            df = df[df["ç±»å‹"].astype(str).str.strip().isin(["å®é™…"])]
        
        for _, row in df.iterrows():
            # å‡è®¾ 'ç±»å‹' åˆ—æ˜¯æŒ‡æ ‡åç§°
            channel_name = str(row.get('é€šé“åç§°', 'Unknown')).strip()
            
            for t in time_cols:
                val = row[t]
                if pd.isna(val): continue
                
                records.append({
                    'record_date': data_date,
                    'record_time': t,
                    'channel_name': channel_name,
                    'value': val,
                    'sheet_name': sheet_name,
                    'type': data_type,
                    'created_at': datetime.datetime.now()
                })
        return records

    def _process_imformation_true_sheet_3(self, df, data_date, sheet_name, data_type):
        """è‡ªåŠ¨ç”Ÿæˆçš„å¤„ç†å‡½æ•°: è¥¿ç”µä¸œé€å„é€šé“å®é™…ä¿¡æ¯(2025-12-23) (æ¨¡å¼: time_series_matrix)"""
        records = []
        df = df.dropna(how='all')
        df.columns = [str(c).strip() for c in df.columns]
        
        # è¯†åˆ«æ—¶é—´åˆ—
        time_cols = [c for c in df.columns if re.match(r'^\d{1,2}:\d{2}$', c)]

        # è¿‡æ»¤æ‰â€œç±»å‹â€ç­‰éæŒ‡æ ‡è¡Œï¼Œé¿å…é€šé“åç§°è¢«æ±¡æŸ“
        if "ç±»å‹" in df.columns and "é€šé“åç§°" in df.columns:
            df = df[df["ç±»å‹"].astype(str).str.strip().isin(["å®é™…"])]
        
        for _, row in df.iterrows():
            # å‡è®¾ 'ç±»å‹' åˆ—æ˜¯æŒ‡æ ‡åç§°
            channel_name = str(row.get('é€šé“åç§°', 'Unknown')).strip()
            
            for t in time_cols:
                val = row[t]
                if pd.isna(val): continue
                
                records.append({
                    'record_date': data_date,
                    'record_time': t,
                    'channel_name': channel_name,
                    'value': val,
                    'sheet_name': sheet_name,
                    'type': data_type,
                    'created_at': datetime.datetime.now()
                })
        return records
    
    def _process_imformation_true_sheet_5(self, df, data_date, sheet_name, data_type):
        """è‡ªåŠ¨ç”Ÿæˆçš„å¤„ç†å‡½æ•°: å¤‡ç”¨å®é™…ä¿¡æ¯(2025-12-23) (æ¨¡å¼: time_series_matrix)"""
        records = []
        df = df.dropna(how='all')
        df.columns = [str(c).strip() for c in df.columns]
        
        # è¯†åˆ«æ—¶é—´åˆ—
        time_cols = [c for c in df.columns if re.match(r'^\d{1,2}:\d{2}$', c)]
        
        for _, row in df.iterrows():
            # å‡è®¾ 'æ•°æ®é¡¹' åˆ—æ˜¯æŒ‡æ ‡åç§°
            channel_name = str(row.get('æ•°æ®é¡¹', 'Unknown')).strip()
            
            for t in time_cols:
                val = row[t]
                if pd.isna(val): continue
                
                records.append({
                    'record_date': data_date,
                    'record_time': t,
                    'channel_name': channel_name,
                    'value': val,
                    'sheet_name': sheet_name,
                    'type': data_type,
                    'created_at': datetime.datetime.now()
                })
        return records

    def _process_imformation_true_sheet_6(self, df, data_date, sheet_name, data_type):
        """è‡ªåŠ¨ç”Ÿæˆçš„å¤„ç†å‡½æ•°: å®æ—¶å‡ºæ¸…æ–­é¢(2025-12-23) (æ¨¡å¼: time_series_matrix)"""
        records = []
        df = df.dropna(how='all')
        df.columns = [str(c).strip() for c in df.columns]
        
        # è¯†åˆ«æ—¶é—´åˆ—
        time_cols = [c for c in df.columns if re.match(r'^\d{1,2}:\d{2}$', c)]
        
        for _, row in df.iterrows():
            # å‡è®¾ 'æ–­é¢åç§°' åˆ—æ˜¯æŒ‡æ ‡åç§°
            channel_name = str(row.get('æ–­é¢åç§°', 'Unknown')).strip()
            
            for t in time_cols:
                val = row[t]
                if pd.isna(val): continue
                
                records.append({
                    'record_date': data_date,
                    'record_time': t,
                    'channel_name': channel_name,
                    'value': val,
                    'sheet_name': sheet_name,
                    'type': data_type,
                    'created_at': datetime.datetime.now()
                })
        return records

    def _process_imformation_true_sheet_7(self, df, data_date, sheet_name, data_type):
        """è‡ªåŠ¨ç”Ÿæˆçš„å¤„ç†å‡½æ•°: å®é™…æ–­é¢(2025-12-23) (æ¨¡å¼: time_series_matrix)"""
        records = []
        df = df.dropna(how='all')
        df.columns = [str(c).strip() for c in df.columns]
        
        # è¯†åˆ«æ—¶é—´åˆ—
        time_cols = [c for c in df.columns if re.match(r'^\d{1,2}:\d{2}$', c)]
        
        for _, row in df.iterrows():
            # å‡è®¾ 'æ–­é¢åç§°' åˆ—æ˜¯æŒ‡æ ‡åç§°
            channel_name = str(row.get('æ–­é¢åç§°', 'Unknown')).strip()
            
            for t in time_cols:
                val = row[t]
                if pd.isna(val): continue
                
                records.append({
                    'record_date': data_date,
                    'record_time': t,
                    'channel_name': channel_name,
                    'value': val,
                    'sheet_name': sheet_name,
                    'type': data_type,
                    'created_at': datetime.datetime.now()
                })
        return records
    
    def _process_imformation_true_sheet_9(self, df, data_date, sheet_name):
        """
        å¤„ç†ç”µå‚æœºç»„æŠ€æœ¯å‚æ•°sheetï¼Œæå–æœºç»„æŠ€æœ¯å‚æ•°æ•°æ®ï¼Œæœºç»„å‡ºåŠ›
        """
        records = []
        df = df.dropna(how="all")  # åˆ é™¤ç©ºè¡Œ
        
        if df.empty:
            print(f"è­¦å‘Šï¼šsheet '{sheet_name}' æ— æœ‰æ•ˆæ•°æ®")
            return records

        # ç¡®ä¿åˆ—åæ­£ç¡®
        df.columns = [str(c).strip().replace('ï¼ˆ', '(').replace('ï¼‰', ')') for c in df.columns]
        print(f"DEBUG: Sheet '{sheet_name}' columns: {df.columns.tolist()}")
        
        # ===== æ·»åŠ è°ƒè¯•ï¼šæ‰“å°å‰å‡ è¡ŒåŸå§‹æ•°æ® =====
        print("DEBUG: åŸå§‹æ•°æ®å‰5è¡Œ:")
        for i in range(min(5, len(df))):
            row = df.iloc[i]
            print(f"  è¡Œ{i}: ç”µå‚='{row.get('ç”µå‚åç§°')}' (ç±»å‹: {type(row.get('ç”µå‚åç§°'))}), "
                f"æœºç»„='{row.get('æœºç»„åç§°')}', "
                f"æœ€å°å‡ºåŠ›='{row.get('æœ€å°æŠ€æœ¯å‡ºåŠ›(MW)')}', "
                f"é¢å®šå‡ºåŠ›='{row.get('é¢å®šå‡ºåŠ›(MW)')}'")
        
        # æ¿€è¿›çš„æ¸…ç†ç­–ç•¥ï¼šå¤„ç†ç”µå‚åç§°
        if "ç”µå‚åç§°" in df.columns:
            # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
            df["ç”µå‚åç§°"] = df["ç”µå‚åç§°"].astype(str)
            print("DEBUG: è½¬æ¢ä¸ºå­—ç¬¦ä¸²åå‰5è¡Œç”µå‚åç§°:")
            print(df["ç”µå‚åç§°"].head(5).tolist())
            
            # å°† 'nan', 'None', ç©ºç™½å­—ç¬¦ æ›¿æ¢ä¸º NaN
            df["ç”µå‚åç§°"] = df["ç”µå‚åç§°"].replace(r'^\s*$', np.nan, regex=True)
            df["ç”µå‚åç§°"] = df["ç”µå‚åç§°"].replace(['nan', 'None'], np.nan)
            
            print("DEBUG: æ›¿æ¢ç©ºå€¼åå‰5è¡Œç”µå‚åç§°:")
            print(df["ç”µå‚åç§°"].head(5).tolist())
            
            # å‰å‘å¡«å……
            df["ç”µå‚åç§°"] = df["ç”µå‚åç§°"].ffill()
            print("DEBUG: å‰å‘å¡«å……åå‰5è¡Œç”µå‚åç§°:")
            print(df["ç”µå‚åç§°"].head(5).tolist())
            
        # éå†æ¯ä¸€è¡Œæ•°æ®
        for idx, row in df.iterrows():
            # è·³è¿‡ç©ºè¡Œ
            if pd.isna(row["æœºç»„åç§°"]) or str(row["æœºç»„åç§°"]).strip() == "":
                continue
            
            # ===== æ·»åŠ è°ƒè¯•ï¼šæ‰“å°å½“å‰è¡Œå¤„ç†è¿‡ç¨‹ =====
            print(f"\nDEBUG å¤„ç†ç¬¬{idx+1}è¡Œ:")
            print(f"  åŸå§‹ç”µå‚åç§°: '{row['ç”µå‚åç§°']}' (ç±»å‹: {type(row['ç”µå‚åç§°'])})")
            print(f"  åŸå§‹æœºç»„åç§°: '{row['æœºç»„åç§°']}'")
            print(f"  åŸå§‹æœ€å°å‡ºåŠ›: '{row['æœ€å°æŠ€æœ¯å‡ºåŠ›(MW)']}'")
            
            # å¤„ç†ç”µå‚åç§°
            plant_name_raw = row["ç”µå‚åç§°"]
            is_plant_na = pd.isna(plant_name_raw)
            plant_name_str = str(plant_name_raw).strip() if not is_plant_na else ""
            
            print(f"  pd.isna(ç”µå‚åç§°) = {is_plant_na}")
            print(f"  str(ç”µå‚åç§°) = '{plant_name_str}'")
            
            if is_plant_na or plant_name_str in ["nan", "None", ""]:
                plant_name = str(row["æœºç»„åç§°"]).strip()
                print(f"  â†’ ä½¿ç”¨æœºç»„åç§°ä½œä¸ºç”µå‚åç§°: '{plant_name}'")
            else:
                plant_name = plant_name_str
                print(f"  â†’ ä½¿ç”¨åŸå§‹ç”µå‚åç§°: '{plant_name}'")
            
            # å¤„ç†æœ€å°å‡ºåŠ›
            min_output_raw = row["æœ€å°æŠ€æœ¯å‡ºåŠ›(MW)"]
            min_output = min_output_raw
            print(f"  åŸå§‹æœ€å°å‡ºåŠ›å€¼: {min_output_raw} (ç±»å‹: {type(min_output_raw)})")
            
            if pd.isna(min_output_raw):
                min_output = 0.0
                print(f"  â†’ æœ€å°å‡ºåŠ›ä¸ºNaNï¼Œå¡«å……ä¸º: {min_output}")
            elif str(min_output_raw) == 'None':
                min_output = 0.0
                print(f"  â†’ æœ€å°å‡ºåŠ›ä¸º'None'å­—ç¬¦ä¸²ï¼Œå¡«å……ä¸º: {min_output}")
            else:
                try:
                    min_output = float(min_output_raw)
                    print(f"  â†’ æœ€å°å‡ºåŠ›è½¬æ¢ä¸ºæµ®ç‚¹æ•°: {min_output}")
                except Exception as e:
                    min_output = 0.0
                    print(f"  â†’ æœ€å°å‡ºåŠ›è½¬æ¢å¤±è´¥ï¼Œå¡«å……ä¸º: {min_output}, é”™è¯¯: {e}")
            
            record = {
                "record_date": data_date,
                "power_plant_name": plant_name,
                "generator_name": str(row["æœºç»„åç§°"]).strip(),
                "min_technical_output": min_output,
                "rated_output": float(row["é¢å®šå‡ºåŠ›(MW)"]) if not pd.isna(row["é¢å®šå‡ºåŠ›(MW)"]) else None,
                "sheet_name": sheet_name
            }
            
            print(f"  â†’ æœ€ç»ˆè®°å½•: {record}")
            records.append(record)
                
        print(f"âœ… Sheet '{sheet_name}' è§£æå®Œæˆï¼Œå…± {len(records)} æ¡è®°å½•")
        
        # ===== æ·»åŠ è°ƒè¯•ï¼šæ‰“å°æœ€ç»ˆè®°å½• =====
        if records and len(records) > 0:
            print("ğŸ” è§£æå‡½æ•°å®é™…è¿”å›çš„å­—æ®µåæ£€æŸ¥:")
            first_record = records[0]
            print(f"   ç¬¬ä¸€æ¡è®°å½•å­—æ®µå: {list(first_record.keys())}")
            print(f"   ç¬¬ä¸€æ¡è®°å½•å†…å®¹:")
            for key, value in first_record.items():
                print(f"     {key}: {repr(value)}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æˆ‘ä»¬æœŸæœ›çš„ä¸­æ–‡å­—æ®µ
            expected_fields = ["ç”µå‚åç§°", "æœºç»„åç§°", "æœ€å°æŠ€æœ¯å‡ºåŠ›(MW)", "é¢å®šå‡ºåŠ›(MW)"]
            missing_fields = [field for field in expected_fields if field not in first_record]
            if missing_fields:
                print(f"   â— ç¼ºå°‘ä¸­æ–‡å­—æ®µ: {missing_fields}")

        return records

    # def _process_imformation_true_sheet_13(self, df, data_date, sheet_name):
    #     """è‡ªåŠ¨ç”Ÿæˆçš„å¤„ç†å‡½æ•°: è¾“å˜ç”µè®¾å¤‡æ£€ä¿®è®¡åˆ’æ‰§è¡Œæƒ…å†µ(2025-12-23) (æ¨¡å¼: generic_table)"""
    #     records = []
    #     df = df.dropna(how='all')
    #     df.columns = [str(c).strip() for c in df.columns]
        
    #     # é€šç”¨è¡¨æ ¼å¤„ç† (ç›´æ¥æ˜ å°„æ‰€æœ‰åˆ—)
    #     for _, row in df.iterrows():
    #         record = {
    #             'record_date': data_date,
    #             'sheet_name': sheet_name,
    #             'created_at': datetime.datetime.now()
    #         }
    #         # åŠ¨æ€æ˜ å°„æ‰€æœ‰åˆ—
    #         for col in df.columns:
    #             val = row[col]
    #             if pd.notna(val):
    #                 record[col] = val
    #         records.append(record)
    #     return records

    def _process_imformation_true_sheet_15(self, df, data_date, sheet_name,data_type):
        """è‡ªåŠ¨ç”Ÿæˆçš„å¤„ç†å‡½æ•°: çº¿è·¯åœè¿æƒ…å†µ(2025-12-23) (æ¨¡å¼: generic_table)"""
        records = []
        df = df.dropna(how='all')
        df.columns = [str(c).strip() for c in df.columns]
        
        # é€šç”¨è¡¨æ ¼å¤„ç† (ç›´æ¥æ˜ å°„æ‰€æœ‰åˆ—)
        for _, row in df.iterrows():
            channel_name = str(row.get('å†…å®¹', 'Unknown')).strip()
            record = {
                'record_date': data_date,
                'sheet_name': sheet_name,
                'channel_name': channel_name,
                'value': None,
                'type': data_type,
                'created_at': datetime.datetime.now()
            }
            # åŠ¨æ€æ˜ å°„æ‰€æœ‰åˆ—
            for col in df.columns:
                val = row[col]
                if pd.notna(val):
                    record[col] = val
            records.append(record)
        return records

    def _process_imformation_true_sheet_16(self, df, data_date, sheet_name, data_type):
        """è‡ªåŠ¨ç”Ÿæˆçš„å¤„ç†å‡½æ•°: æœºç»„å‡ºåŠ›æƒ…å†µ(2025-12-23) (æ¨¡å¼: time_series_matrix)"""
        records = []
        df = df.dropna(how='all')
        df.columns = [str(c).strip() for c in df.columns]
        
        # è¯†åˆ«æ—¶é—´åˆ—
        time_cols = [c for c in df.columns if re.match(r'^\d{1,2}:\d{2}$', c)]
        
        for _, row in df.iterrows():
            # å‡è®¾ 'ç±»å‹' åˆ—æ˜¯æŒ‡æ ‡åç§°
            # ä¼˜å…ˆæŸ¥æ‰¾ 'ç±»å‹'ï¼Œå¦‚æœæ²¡æœ‰åˆ™å°è¯• 'æ•°æ®é¡¹' (å…¼å®¹æ€§)
            channel_name = str(row.get('ç±»å‹', row.get('æ•°æ®é¡¹', 'Unknown'))).strip()
            
            for t in time_cols:
                val = row[t]
                if pd.isna(val): continue
                
                records.append({
                    'record_date': data_date,
                    'record_time': t,
                    'channel_name': channel_name,
                    'value': val,
                    'sheet_name': sheet_name,
                    'type': data_type,
                    'created_at': datetime.datetime.now()
                })
        return records
    
    def query_daily_averages(self, date_list, data_type_keyword="æ—¥å‰èŠ‚ç‚¹ç”µä»·", station_name=None, city=None):
        """
        æŸ¥è¯¢å¤šå¤©çš„å‡å€¼æ•°æ®ï¼ˆé€‚ç”¨äºå·²è®¡ç®—å¥½çš„å‡å€¼è®°å½•ï¼‰
        
        Args:
            date_list (list): æ—¥æœŸåˆ—è¡¨ï¼Œæ ¼å¼ä¸º "YYYY-MM-DD"
            data_type_keyword (str): æ•°æ®ç±»å‹å…³é”®å­—ï¼Œç”¨äºç­›é€‰ç‰¹å®šç±»å‹çš„æ•°æ®
            station_name (str): ç«™ç‚¹åç§°ï¼Œå¦‚æœæä¾›åˆ™æŒ‰ç…§ç«™ç‚¹åç§°æ¨¡ç³ŠåŒ¹é…ï¼Œå¦åˆ™é»˜è®¤åŒ¹é…'å‡å€¼'
            city (str): åŸå¸‚åç§°ï¼ˆå¯é€‰ï¼‰ï¼Œä¼˜å…ˆäºç«™ç‚¹åç§°
            
        Returns:
            dict: åŒ…å«æŸ¥è¯¢ç»“æœçš„å­—å…¸
        """
        try:
            # å¦‚æœæŒ‡å®šåŸå¸‚ï¼Œèµ°åŸå¸‚å‡ä»·æŸ¥è¯¢
            if city and str(city).strip():
                return self.query_city_daily_averages(date_list, data_type_keyword, city)

            # æ„é€ è¡¨ååˆ—è¡¨
            table_names = []
            for date_str in date_list:
                # å°†æ—¥æœŸæ ¼å¼è½¬æ¢ä¸ºè¡¨åæ ¼å¼ (YYYY-MM-DD -> YYYYMMDD)
                date_obj = datetime.datetime.strptime(date_str, "%Y-%m-%d")
                table_name = f"power_data_{date_obj.strftime('%Y%m%d')}"
                print(f"ğŸ” æŸ¥è¯¢è¡¨: {table_name}")
                table_names.append(table_name)
                
            
            # éªŒè¯è¡¨æ˜¯å¦å­˜åœ¨
            existing_tables = self.db_manager.get_tables()
            valid_tables = [table for table in table_names if table in existing_tables]
            
            if not valid_tables:
                return {"data": [], "total": 0, "message": "æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ•°æ®è¡¨"}
            
            # æ„é€ UNIONæŸ¥è¯¢è¯­å¥ï¼šæŸ¥æ‰¾åŒ…å«æŒ‡å®šå…³é”®å­—å’Œ"å‡å€¼"çš„è®°å½•
            union_parts = []
            
            # ç¡®å®šç­›é€‰æ¡ä»¶ï¼šå¦‚æœæœ‰ç«™ç‚¹åç§°ï¼Œåˆ™æŒ‰ç«™ç‚¹åç§°æ¨¡ç³ŠåŒ¹é…ï¼Œå¦åˆ™æŒ‰'å‡å€¼'åŒ¹é…
            name_filter = f"channel_name LIKE '%{station_name}%'" if station_name and station_name.strip() else "channel_name LIKE '%å‡å€¼%'"
            
            for table in valid_tables:
                union_parts.append(f""" SELECT * FROM {table} WHERE {name_filter} AND type LIKE '%{data_type_keyword}%'""")
            if not union_parts:
                return {"data": [], "total": 0, "message": "æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„æ•°æ®"}
                
            union_query = " UNION ALL ".join(union_parts)
            print(f"ğŸš€ æ‰§è¡ŒUNIONæŸ¥è¯¢: {union_query}")
            final_query = f"""
                SELECT * FROM ({union_query}) as combined_data
                ORDER BY record_date DESC, record_time
            """
            
            # æ‰§è¡ŒæŸ¥è¯¢
            result = self.db_manager.complex_query(final_query)
            # print(f"âœ… æŸ¥è¯¢æˆåŠŸï¼Œå…± {len(result)} æ¡è®°å½•")
            # print(result)
            
            # æ„é€ è¿”å›ç»“æœ
            return {
                "data": result.get("data"),
                "total": result.get("total"),
                "message": "æŸ¥è¯¢æˆåŠŸ"
            }
            
        except Exception as e:
            print(f"âŒ æŸ¥è¯¢å¤šå¤©å‡å€¼æ•°æ®å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return {"data": [], "total": 0, "message": f"æŸ¥è¯¢å¤±è´¥: {str(e)}"}

    def query_city_daily_averages(self, date_list, data_type_keyword, city):
        """
        æŒ‰åŸå¸‚æŸ¥è¯¢èŠ‚ç‚¹å‡ä»·ï¼ˆä¼šåœ¨ç¼ºå¤±æ—¶è‡ªåŠ¨æŒ‰èŠ‚ç‚¹èšåˆå¹¶å†™å›ï¼‰
        """
        try:
            city = str(city).strip()
            if not city:
                return {"data": [], "total": 0, "message": "åŸå¸‚ä¸ºç©º"}

            table_names = []
            for date_str in date_list:
                date_obj = datetime.datetime.strptime(date_str, "%Y-%m-%d")
                table_names.append(f"power_data_{date_obj.strftime('%Y%m%d')}")

            existing_tables = self.db_manager.get_tables()
            valid_tables = [t for t in table_names if t in existing_tables]
            if not valid_tables:
                return {"data": [], "total": 0, "message": "æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ•°æ®è¡¨"}

            all_rows = []
            for table in valid_tables:
                # å…ˆæŸ¥å·²æœ‰åŸå¸‚å‡ä»·
                type_like = f"%{data_type_keyword}%"
                city_label = self._city_channel_name(city)
                sql = text(f"""
                    SELECT * FROM {table}
                    WHERE channel_name = :cn AND type LIKE :type_like
                """)
                with self.db_manager.engine.connect() as conn:
                    rows = conn.execute(sql, {"cn": city_label, "type_like": type_like}).fetchall()
                if rows and len(rows) >= 12:
                    all_rows.extend([dict(r._mapping) for r in rows])
                    continue

                # ä¸è¶³åˆ™æŒ‰èŠ‚ç‚¹é‡æ–°è®¡ç®—ï¼ˆå¹¶å†™å›ï¼‰
                date_str = table.replace("power_data_", "")
                date_str = f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:]}"
                computed = self.ensure_city_means_for_date(date_str, data_type_keyword, city=city, insert=True)
                all_rows.extend(computed)

            all_rows.sort(key=lambda r: (str(r.get("record_date", "")), str(r.get("record_time", ""))), reverse=True)
            return {
                "data": all_rows,
                "total": len(all_rows),
                "message": "æŸ¥è¯¢æˆåŠŸ"
            }
        except Exception as e:
            print(f"âŒ åŸå¸‚å‡å€¼æŸ¥è¯¢å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return {"data": [], "total": 0, "message": f"æŸ¥è¯¢å¤±è´¥: {str(e)}"}

    def query_price_difference(self, date_list, region="", station_name=None, city=None):
        """
        æŸ¥è¯¢ä»·å·®æ•°æ®ï¼ˆæ—¥å‰èŠ‚ç‚¹ç”µä»· - å®æ—¶èŠ‚ç‚¹ç”µä»·ï¼‰
        
        Args:
            date_list (list): æ—¥æœŸåˆ—è¡¨ï¼Œæ ¼å¼ä¸º "YYYY-MM-DD"
            region (str): åœ°åŒºå‰ç¼€ï¼Œå¦‚"äº‘å—_"ï¼Œé»˜è®¤ä¸ºç©º
            station_name (str): ç«™ç‚¹åç§°
            city (str): åŸå¸‚åç§°ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            dict: åŒ…å«ä»·å·®æŸ¥è¯¢ç»“æœçš„å­—å…¸
        """
        try:
            import pandas as pd
            
            # æ„é€ æ•°æ®ç±»å‹å…³é”®è¯
            dayahead_keyword = f"{region}æ—¥å‰èŠ‚ç‚¹ç”µä»·" if region else "æ—¥å‰èŠ‚ç‚¹ç”µä»·"
            realtime_keyword = f"{region}å®æ—¶èŠ‚ç‚¹ç”µä»·" if region else "å®æ—¶èŠ‚ç‚¹ç”µä»·"
            
            print(f"ğŸ” æŸ¥è¯¢ä»·å·®æ•°æ®:")
            print(f"  - æ—¥å‰èŠ‚ç‚¹ç”µä»·å…³é”®è¯: {dayahead_keyword}")
            print(f"  - å®æ—¶èŠ‚ç‚¹ç”µä»·å…³é”®è¯: {realtime_keyword}")
            print(f"  - æ—¥æœŸåˆ—è¡¨: {date_list}")
            print(f"  - ç«™ç‚¹ç­›é€‰: {station_name or 'é»˜è®¤(å‡å€¼)'}")
            if city:
                print(f"  - åŸå¸‚ç­›é€‰: {city}")
            
            # æŸ¥è¯¢æ—¥å‰èŠ‚ç‚¹ç”µä»·æ•°æ®
            dayahead_result = self.query_daily_averages(date_list, dayahead_keyword, station_name, city)
            dayahead_data = dayahead_result.get("data", [])
            
            # æŸ¥è¯¢å®æ—¶èŠ‚ç‚¹ç”µä»·æ•°æ®
            realtime_result = self.query_daily_averages(date_list, realtime_keyword, station_name, city)
            realtime_data = realtime_result.get("data", [])
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ä¸¤ä¸ªæ•°æ®
            if not dayahead_data:
                return {
                    "data": [],
                    "total": 0,
                    "message": f"æœªæ‰¾åˆ°æ—¥å‰èŠ‚ç‚¹ç”µä»·æ•°æ®ï¼ˆå…³é”®è¯: {dayahead_keyword}ï¼‰",
                    "has_dayahead": False,
                    "has_realtime": len(realtime_data) > 0
                }
            
            if not realtime_data:
                return {
                    "data": [],
                    "total": 0,
                    "message": f"æœªæ‰¾åˆ°å®æ—¶èŠ‚ç‚¹ç”µä»·æ•°æ®ï¼ˆå…³é”®è¯: {realtime_keyword}ï¼‰",
                    "has_dayahead": True,
                    "has_realtime": False
                }
            
            print(f"âœ… æ‰¾åˆ°æ—¥å‰æ•°æ®: {len(dayahead_data)} æ¡")
            print(f"âœ… æ‰¾åˆ°å®æ—¶æ•°æ®: {len(realtime_data)} æ¡")
            
            # è½¬æ¢ä¸ºDataFrameä»¥ä¾¿å¤„ç†
            dayahead_df = pd.DataFrame(dayahead_data)
            realtime_df = pd.DataFrame(realtime_data)
            
            # ç¡®ä¿å¿…è¦çš„åˆ—å­˜åœ¨
            required_columns = ['channel_name', 'record_date', 'record_time', 'value']
            if not all(col in dayahead_df.columns for col in required_columns):
                return {
                    "data": [],
                    "total": 0,
                    "message": "æ—¥å‰æ•°æ®ç¼ºå°‘å¿…è¦åˆ—",
                    "has_dayahead": True,
                    "has_realtime": True
                }
            
            if not all(col in realtime_df.columns for col in required_columns):
                return {
                    "data": [],
                    "total": 0,
                    "message": "å®æ—¶æ•°æ®ç¼ºå°‘å¿…è¦åˆ—",
                    "has_dayahead": True,
                    "has_realtime": True
                }
            
            # ç»Ÿä¸€æ ¼å¼åŒ–å­—æ®µä»¥ä¾¿åŒ¹é…
            # 1. æ ¼å¼åŒ–channel_nameï¼šå»é™¤ç©ºæ ¼ï¼Œç»Ÿä¸€å¤§å°å†™
            dayahead_df['channel_name_clean'] = dayahead_df['channel_name'].astype(str).str.strip()
            realtime_df['channel_name_clean'] = realtime_df['channel_name'].astype(str).str.strip()
            
            # 2. æ ¼å¼åŒ–record_dateï¼šç»Ÿä¸€ä¸ºå­—ç¬¦ä¸²æ ¼å¼ YYYY-MM-DD
            def format_date(date_val):
                if pd.isna(date_val):
                    return ""
                if isinstance(date_val, str):
                    return date_val.strip()
                if hasattr(date_val, 'strftime'):
                    return date_val.strftime('%Y-%m-%d')
                return str(date_val).strip()
            
            dayahead_df['record_date_clean'] = dayahead_df['record_date'].apply(format_date)
            realtime_df['record_date_clean'] = realtime_df['record_date'].apply(format_date)
            
            # 3. æ ¼å¼åŒ–record_timeï¼šç»Ÿä¸€æ—¶é—´æ ¼å¼
            def format_time(time_val):
                if pd.isna(time_val):
                    return ""
                
                # å¤„ç†timedeltaå¯¹è±¡ï¼ˆå¦‚ '0 days 00:00:00'ï¼‰
                if hasattr(time_val, 'total_seconds'):
                    total_seconds = int(time_val.total_seconds())
                    hour = total_seconds // 3600
                    return f"{hour:02d}:00"
                
                # å¦‚æœæ˜¯å­—ç¬¦ä¸²
                if isinstance(time_val, str):
                    time_str = time_val.strip()
                    # å¦‚æœåŒ…å«"days"ï¼Œè¯´æ˜æ˜¯timedeltaå­—ç¬¦ä¸²æ ¼å¼
                    if 'days' in time_str.lower():
                        # è§£ætimedeltaå­—ç¬¦ä¸²ï¼Œå¦‚ "0 days 01:00:00"
                        import re
                        match = re.search(r'(\d+):(\d+):(\d+)', time_str)
                        if match:
                            hours = int(match.group(1))
                            return f"{hours:02d}:00"
                    # å¦‚æœåŒ…å«å†’å·ï¼Œç›´æ¥è¿”å›
                    if ':' in time_str:
                        return time_str
                
                # å¦‚æœæ˜¯æ•°å­—ï¼Œè½¬æ¢ä¸ºHH:MMæ ¼å¼
                try:
                    if isinstance(time_val, (int, float)):
                        val = int(time_val)
                        # å¦‚æœæ˜¯ç§’æ•°ï¼ˆ>=3600ï¼‰ï¼Œè½¬æ¢ä¸ºå°æ—¶
                        if val >= 3600:
                            hour = val // 3600
                            return f"{hour:02d}:00"
                        # å¦‚æœæ˜¯å°æ—¶ï¼ˆ0-23ï¼‰ï¼Œç›´æ¥ä½¿ç”¨
                        if 0 <= val < 24:
                            return f"{val:02d}:00"
                        # å¦‚æœæ˜¯HHMMæ ¼å¼ï¼ˆ100-2400ï¼‰ï¼Œè½¬æ¢ä¸ºHH:MM
                        if 100 <= val <= 2400:
                            hour = val // 100
                            return f"{hour:02d}:00"
                        # å¦‚æœæ˜¯0ï¼Œè¿”å›00:00
                        if val == 0:
                            return "00:00"
                except:
                    pass
                return str(time_val).strip()
            
            dayahead_df['record_time_clean'] = dayahead_df['record_time'].apply(format_time)
            realtime_df['record_time_clean'] = realtime_df['record_time'].apply(format_time)
            
            # æ‰“å°å‰å‡ æ¡æ•°æ®ç”¨äºè°ƒè¯•
            print(f"ğŸ“Š æ—¥å‰æ•°æ®ç¤ºä¾‹:")
            print(f"  channel_name: {dayahead_df['channel_name_clean'].head(3).tolist()}")
            print(f"  record_date: {dayahead_df['record_date_clean'].head(3).tolist()}")
            print(f"  record_time: {dayahead_df['record_time_clean'].head(3).tolist()}")
            print(f"ğŸ“Š å®æ—¶æ•°æ®ç¤ºä¾‹:")
            print(f"  channel_name: {realtime_df['channel_name_clean'].head(3).tolist()}")
            print(f"  record_date: {realtime_df['record_date_clean'].head(3).tolist()}")
            print(f"  record_time: {realtime_df['record_time_clean'].head(3).tolist()}")
            
            # åˆ›å»ºåˆå¹¶é”®ï¼šä»·å·®æŸ¥è¯¢åªä½¿ç”¨record_dateå’Œrecord_timeåŒ¹é…
            # å› ä¸ºæ—¥å‰å’Œå®æ—¶çš„channel_nameå¯èƒ½ä¸åŒï¼ˆå¦‚"æ—¥å‰èŠ‚ç‚¹ç”µä»·æŸ¥è¯¢_å‡å€¼" vs "å®æ—¶èŠ‚ç‚¹ç”µä»·æŸ¥è¯¢_å‡å€¼"ï¼‰
            # ä½†å¦‚æœæ˜¯ç›¸åŒæ—¶é—´ç‚¹çš„å‡å€¼æ•°æ®ï¼Œåº”è¯¥åŒ¹é…
            # å¦‚æœchannel_nameç›¸åŒï¼Œä¹ŸåŒ…å«åœ¨åˆå¹¶é”®ä¸­ï¼›å¦‚æœä¸åŒï¼Œåªä½¿ç”¨æ—¥æœŸå’Œæ—¶é—´
            dayahead_df['merge_key'] = (
                dayahead_df['record_date_clean'] + '_' +
                dayahead_df['record_time_clean']
            )
            realtime_df['merge_key'] = (
                realtime_df['record_date_clean'] + '_' +
                realtime_df['record_time_clean']
            )
            
            # æ‰“å°åˆå¹¶é”®ç¤ºä¾‹
            print(f"ğŸ“Š åˆå¹¶é”®ç¤ºä¾‹ï¼ˆæ—¥å‰ï¼‰: {dayahead_df['merge_key'].head(3).tolist()}")
            print(f"ğŸ“Š åˆå¹¶é”®ç¤ºä¾‹ï¼ˆå®æ—¶ï¼‰: {realtime_df['merge_key'].head(3).tolist()}")
            print(f"ğŸ“Š åˆå¹¶é”®å”¯ä¸€å€¼æ•°é‡ï¼ˆæ—¥å‰ï¼‰: {dayahead_df['merge_key'].nunique()}")
            print(f"ğŸ“Š åˆå¹¶é”®å”¯ä¸€å€¼æ•°é‡ï¼ˆå®æ—¶ï¼‰: {realtime_df['merge_key'].nunique()}")
            
            # åˆå¹¶æ•°æ®
            merged_df = pd.merge(
                dayahead_df[['merge_key', 'channel_name', 'record_date', 'record_time', 'value', 'sheet_name']],
                realtime_df[['merge_key', 'value']],
                on='merge_key',
                how='inner',
                suffixes=('_dayahead', '_realtime')
            )
            
            print(f"ğŸ“Š åˆå¹¶ç»“æœ: {len(merged_df)} æ¡åŒ¹é…è®°å½•")
            print(f"ğŸ“Š æ—¥å‰æ•°æ®å”¯ä¸€åˆå¹¶é”®æ•°: {dayahead_df['merge_key'].nunique()}")
            print(f"ğŸ“Š å®æ—¶æ•°æ®å”¯ä¸€åˆå¹¶é”®æ•°: {realtime_df['merge_key'].nunique()}")
            
            if len(merged_df) == 0:
                # æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
                dayahead_keys = set(dayahead_df['merge_key'].unique())
                realtime_keys = set(realtime_df['merge_key'].unique())
                missing_in_realtime = dayahead_keys - realtime_keys
                missing_in_dayahead = realtime_keys - dayahead_keys
                
                error_msg = "æ—¥å‰å’Œå®æ—¶æ•°æ®æ— æ³•åŒ¹é…ã€‚"
                if len(missing_in_realtime) > 0:
                    error_msg += f" æ—¥å‰æ•°æ®ä¸­æœ‰ {len(missing_in_realtime)} ä¸ªé”®åœ¨å®æ—¶æ•°æ®ä¸­æ‰¾ä¸åˆ°ï¼ˆç¤ºä¾‹: {list(missing_in_realtime)[:3]}ï¼‰ã€‚"
                if len(missing_in_dayahead) > 0:
                    error_msg += f" å®æ—¶æ•°æ®ä¸­æœ‰ {len(missing_in_dayahead)} ä¸ªé”®åœ¨æ—¥å‰æ•°æ®ä¸­æ‰¾ä¸åˆ°ï¼ˆç¤ºä¾‹: {list(missing_in_dayahead)[:3]}ï¼‰ã€‚"
                
                return {
                    "data": [],
                    "total": 0,
                    "message": error_msg,
                    "has_dayahead": True,
                    "has_realtime": True
                }
            
            # è®¡ç®—ä»·å·®ï¼šä¸¤ä¸ªè¡¨å¯¹åº”çš„valueç›¸å‡ï¼ˆæ—¥å‰èŠ‚ç‚¹ç”µä»· - å®æ—¶èŠ‚ç‚¹ç”µä»·ï¼‰
            # ç¡®ä¿valueåˆ—æ˜¯æ•°å€¼ç±»å‹
            dayahead_values = pd.to_numeric(merged_df['value_dayahead'], errors='coerce')
            realtime_values = pd.to_numeric(merged_df['value_realtime'], errors='coerce')
            # è®¡ç®—ä»·å·®ï¼šæ—¥å‰ - å®æ—¶ï¼Œå¹¶ä¿ç•™ä¸¤ä½å°æ•°
            merged_df['value'] = (dayahead_values - realtime_values).round(2)
            
            # å°†channel_nameæ”¹ä¸º"ä»·å·®"
            merged_df['channel_name'] = 'ä»·å·®'
            
            print(f"ğŸ“Š ä»·å·®è®¡ç®—ç¤ºä¾‹:")
            print(f"  æ—¥å‰å€¼: {dayahead_values.head(3).tolist()}")
            print(f"  å®æ—¶å€¼: {realtime_values.head(3).tolist()}")
            print(f"  ä»·å·®å€¼ï¼ˆä¿ç•™ä¸¤ä½å°æ•°ï¼‰: {merged_df['value'].head(3).tolist()}")
            
            # åˆ é™¤ä¸´æ—¶åˆ—
            merged_df = merged_df.drop(columns=['merge_key', 'value_dayahead', 'value_realtime'])
            
            # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
            difference_data = merged_df.to_dict('records')
            
            print(f"âœ… ä»·å·®è®¡ç®—å®Œæˆï¼Œå…± {len(difference_data)} æ¡è®°å½•")
            
            return {
                "data": difference_data,
                "total": len(difference_data),
                "message": "ä»·å·®æŸ¥è¯¢æˆåŠŸ",
                "has_dayahead": True,
                "has_realtime": True
            }
            
        except Exception as e:
            print(f"âŒ æŸ¥è¯¢ä»·å·®æ•°æ®å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return {
                "data": [],
                "total": 0,
                "message": f"æŸ¥è¯¢å¤±è´¥: {str(e)}",
                "has_dayahead": False,
                "has_realtime": False
            }

    def _process_outage_as_table(self, df, data_date, sheet_name):
        """å°†è¡¨æ ¼æ•°æ®æ˜ å°„ä¸ºåœç”µè®°å½•ï¼Œé€‚é…æ–‡ä»¶æ ¼å¼"""
        records = []
        df = df.dropna(how="all")  # åˆ é™¤ç©ºè¡Œ
        
        # å¤„ç†è¡¨å¤´ï¼ˆç¡®ä¿åˆ—åæ­£ç¡®æ˜ å°„ï¼‰
        df.columns = [str(c).strip() for c in df.iloc[0]]  # ç¬¬ä¸€è¡Œä½œåˆ—å
        df = df[1:]  # å»æ‰æ ‡é¢˜è¡Œ
        # æ¸…æ´—åˆ—åï¼Œå»é™¤ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦
        df.columns = [str(col).strip().replace('\n', '').replace(' ', '') for col in df.columns]
        # éªŒè¯å¿…è¦åˆ—æ˜¯å¦å­˜åœ¨
        required_cols = ["è®¾å¤‡åç§°", "ç”µå‹ç­‰çº§", "è®¾å¤‡ç±»å‹", "è®¾å¤‡ç¼–å·", 
                        "è®¡åˆ’åœç”µæ—¥æœŸ", "å®é™…åœç”µæ—¥æœŸ", "è®¡åˆ’å¤ç”µæ—¶é—´", "å®é™…å¤ç”µæ—¶é—´"]
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            raise ValueError(f"ç¼ºå¤±å¿…è¦åˆ—: {missing_cols}")
        
        # éå†æ¯ä¸€è¡Œæ•°æ®
        for idx, row in df.iterrows():
            # è·³è¿‡ç©ºè¡Œå’Œæ ‡é¢˜è¡Œï¼ˆå¦‚æœæœ‰æ®‹ç•™ï¼‰
            device_name = str(row.get("è®¾å¤‡åç§°", "")).strip()
            if not device_name:
                continue
            
            # æ„å»ºè®°å½•å­—å…¸
            record = {
                "device_name": device_name,
                "record_date": data_date,
                "sheet_name": sheet_name,
                "voltage_level": str(row.get("ç”µå‹ç­‰çº§", "")).strip() or None,  # ç©ºå€¼å¤„ç†ä¸ºNone
                "device_type": str(row.get("è®¾å¤‡ç±»å‹", "")).strip(),
                "device_code": str(row.get("è®¾å¤‡ç¼–å·", "")).strip(),
                # æ—¶é—´å­—æ®µä¿æŒåŸå§‹æ ¼å¼ï¼ˆæ•°æ®åº“æ’å…¥æ—¶ä¼šç”¨STR_TO_DATEè½¬æ¢ï¼‰
                "planned_power_off_time": str(row.get("è®¡åˆ’åœç”µæ—¥æœŸ", "")).strip(),
                "actual_power_off_time": str(row.get("å®é™…åœç”µæ—¥æœŸ", "")).strip(),
                "planned_power_on_time": str(row.get("è®¡åˆ’å¤ç”µæ—¶é—´", "")).strip(),
                "actual_power_on_time": str(row.get("å®é™…å¤ç”µæ—¶é—´", "")).strip(),
            }
            
            # éªŒè¯å…³é”®å­—æ®µ
            if not record["device_code"]:
                print(f"è·³è¿‡æ— æ•ˆè¡Œï¼ˆæ— è®¾å¤‡ç¼–å·ï¼‰ï¼š{idx}è¡Œ")
                continue
            if not all([record["planned_power_off_time"], record["planned_power_on_time"]]):
                print(f"è·³è¿‡æ— æ•ˆè¡Œï¼ˆæ—¶é—´ä¸å®Œæ•´ï¼‰ï¼š{idx}è¡Œ")
                continue
            
            records.append(record)
        
        return records
    
    def _process_internal_as_table(self, df, data_date, sheet_name):
        """å°†è¡¨æ ¼æ•°æ®æ˜ å°„ä¸ºå‘ç”µæœºå¹²é¢„è®°å½•ï¼Œé€‚é…æ–‡ä»¶æ ¼å¼"""
        records = []
        df = df.dropna(how="all")  # åˆ é™¤ç©ºè¡Œ
        
        # å¤„ç†è¡¨å¤´ï¼ˆç¡®ä¿åˆ—åæ­£ç¡®æ˜ å°„ï¼‰
        df.columns = [str(c).strip() for c in df.iloc[0]]  # ç¬¬ä¸€è¡Œä½œåˆ—å
        df = df[1:]  # å»æ‰æ ‡é¢˜è¡Œ
        # æ¸…æ´—åˆ—åï¼Œå»é™¤ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦
        df.columns = [str(col).strip().replace('\n', '').replace(' ', '') for col in df.columns]
        # éªŒè¯å¿…è¦åˆ—æ˜¯å¦å­˜åœ¨
        required_cols = ["å¯¹è±¡åç§°", "å¯¹è±¡id", "å¹²é¢„å¼€å§‹æ—¶é—´", "å¹²é¢„ç»“æŸæ—¶é—´", 
                        "å¹²é¢„å‰æœ€å¤§å€¼", "å¹²é¢„å‰æœ€å°å€¼", "å¹²é¢„åæœ€å¤§å€¼", "å¹²é¢„åæœ€å°å€¼", "å¹²é¢„åŸå› "]
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            raise ValueError(f"ç¼ºå¤±å¿…è¦åˆ—: {missing_cols}")
        
        # éå†æ¯ä¸€è¡Œæ•°æ®
        for idx, row in df.iterrows():
            # è·³è¿‡ç©ºè¡Œå’Œæ ‡é¢˜è¡Œï¼ˆå¦‚æœæœ‰æ®‹ç•™ï¼‰
            object_name = str(row.get("å¯¹è±¡åç§°", "")).strip()
            if not object_name:
                continue
            
            # æ„å»ºè®°å½•å­—å…¸
            record = {
                "record_date": data_date,
                "sheet_name": sheet_name,
                "object_name": object_name,
                "object_id": str(row.get("å¯¹è±¡id", "")).strip(),
                "intervention_start_time": str(row.get("å¹²é¢„å¼€å§‹æ—¶é—´", "")).strip(),
                "intervention_end_time": str(row.get("å¹²é¢„ç»“æŸæ—¶é—´", "")).strip(),
                "pre_intervention_max": row.get("å¹²é¢„å‰æœ€å¤§å€¼"),
                "pre_intervention_min": row.get("å¹²é¢„å‰æœ€å°å€¼"),
                "post_intervention_max": row.get("å¹²é¢„åæœ€å¤§å€¼"),
                "post_intervention_min": row.get("å¹²é¢„åæœ€å°å€¼"),
                "intervention_reason": str(row.get("å¹²é¢„åŸå› ", "")).strip(),
            }
            
            # éªŒè¯å…³é”®å­—æ®µ
            if not record["object_id"]:
                print(f"è·³è¿‡æ— æ•ˆè¡Œï¼ˆæ— å¯¹è±¡IDï¼‰ï¼š{idx}è¡Œ")
                continue
            if not all([record["intervention_start_time"], record["intervention_end_time"]]):
                print(f"è·³è¿‡æ— æ•ˆè¡Œï¼ˆæ—¶é—´ä¸å®Œæ•´ï¼‰ï¼š{idx}è¡Œ")
                continue
            
            # å°è¯•è½¬æ¢æ•°å€¼å­—æ®µ
            try:
                for field in ["pre_intervention_max", "pre_intervention_min", "post_intervention_max", "post_intervention_min"]:
                    if record[field] is not None and str(record[field]).strip() != "":
                        record[field] = float(record[field])
                    else:
                        record[field] = None
            except ValueError as e:
                print(f"è·³è¿‡æ— æ•ˆè¡Œï¼ˆæ•°å€¼è½¬æ¢å¤±è´¥ï¼‰ï¼š{idx}è¡Œ, é”™è¯¯: {e}")
                continue
            
            records.append(record)
        
        return records
    
    def _process_7_channel(self, df, data_date, sheet_name):
        """å°†è¡¨æ ¼æ•°æ®æ˜ å°„ä¸ºæœºç»„ç¾¤æ¯”ä¾‹è®°å½•ï¼Œé€‚é…æ‰€æœ‰å­—æ®µå¯ç©ºçš„è¡¨ç»“æ„"""
        records = []
        df = df.dropna(how="all")  # åˆ é™¤å…¨ç©ºè¡Œ
        # æ¸…æ´—åˆ—åï¼šå»é™¤ç©ºæ ¼ã€æ¢è¡Œç¬¦ï¼Œç¡®ä¿ä¸è¡¨å­—æ®µåŒ¹é…
        df.columns = [str(col).strip().replace('\n', '').replace(' ', '') for col in df.columns]
        
        # ç©ºDataFrameæ ¡éªŒ
        if df.empty:
            print(f"è­¦å‘Šï¼šsheet '{sheet_name}' æ— æœ‰æ•ˆæ•°æ®ï¼ˆæ‰€æœ‰è¡Œéƒ½æ˜¯ç©ºè¡Œï¼‰")
            return records 
        
        # éå†æ¯ä¸€è¡Œæ•°æ®ï¼ˆé€‚é…â€œæœºç»„ç¾¤å~æ‰€å æ¯”ä¾‹â€è¡¨å­—æ®µï¼‰
        for idx, row in df.iterrows():
            # æ„å»ºè®°å½•å­—å…¸ï¼šå¯¹åº”è¡¨ä¸­8ä¸ªä¸šåŠ¡å­—æ®µï¼Œæ‰€æœ‰å­—æ®µå…è®¸ä¸ºç©º
            record = {
                "record_date": data_date,  # å¤–éƒ¨ä¼ å…¥çš„æ—¥æœŸï¼ˆå¦‚æ•°æ®æ‰€å±æ—¥æœŸï¼‰
                "sheet_name": sheet_name,  # æ•°æ®æ¥æºè¡¨å
                "unit_group_name": str(row.get("æœºç»„ç¾¤å", "")).strip() or None,  # æœºç»„ç¾¤åï¼ˆç©ºå­—ç¬¦ä¸²è½¬Noneï¼‰
                "power_plant_id": str(row.get("ç”µå‚ID", "")).strip() or None,    # ç”µå‚ID
                "power_plant_name": str(row.get("ç”µå‚åç§°", "")).strip() or None,  # ç”µå‚åç§°
                "unit_id": str(row.get("æœºç»„ID", "")).strip() or None,            # æœºç»„ID
                "unit_name": str(row.get("æœºç»„åç§°", "")).strip() or None,          # æœºç»„åç§°
                "proportion": row.get("æ‰€å æ¯”ä¾‹"),                                 # æ‰€å æ¯”ä¾‹ï¼ˆæ•°å€¼å‹ï¼‰
                "record_time": str(row.get("è®°å½•æ—¶é—´", "")).strip() or None         # è®°å½•æ—¶é—´ï¼ˆåŸå§‹æ ¼å¼ï¼Œå¦‚20250918_15:45:00ï¼‰
            }
            
            # æ•°å€¼å­—æ®µè½¬æ¢ï¼šä»…å¤„ç†â€œæ‰€å æ¯”ä¾‹â€ï¼Œç©ºå€¼æˆ–éæ•°å€¼ç›´æ¥è®¾ä¸ºNoneï¼ˆä¸å¼ºåˆ¶æ ¡éªŒï¼‰
            try:
                if record["proportion"] is not None and str(record["proportion"]).strip():
                    record["proportion"] = float(record["proportion"])
                else:
                    record["proportion"] = None
            except ValueError as e:
                print(f"è¡Œ{idx}ï¼š'æ‰€å æ¯”ä¾‹'å­—æ®µéæœ‰æ•ˆæ•°å€¼ï¼Œè®¾ä¸ºNoneï¼Œé”™è¯¯ï¼š{e}")
                record["proportion"] = None
            
            # æ— å¼ºåˆ¶å…³é”®å­—æ®µæ ¡éªŒï¼ˆæ‰€æœ‰å­—æ®µå¯ç©ºï¼‰ï¼Œç›´æ¥æ·»åŠ è®°å½•
            records.append(record)
        
        return records
    
    def save_to_shubiandian_database(self, records, data_date):
        """ä¿å­˜è®¾å¤‡ç”µå‹ç­‰çº§æ•°æ®åˆ°å›ºå®šè¡¨ device_voltage_level"""
        if not records:
            print("âŒ æ²¡æœ‰å¯ä¿å­˜çš„è®°å½•")
            return False, None, 0, []

        # ğŸ§© 1. å¦‚æœä¼ å…¥çš„æ˜¯ DataFrameï¼Œè½¬æˆ list[dict]
        if isinstance(records, pd.DataFrame):
            records = records.to_dict(orient="records")

        if not isinstance(records, list):
            print(f"âŒ records ç±»å‹é”™è¯¯: {type(records)}ï¼Œåº”ä¸º list[dict]")
            return False, None, 0, []

        # ğŸ§© 2. è¿‡æ»¤æ— æ•ˆè®°å½•å¹¶é€‚é…è¡¨å­—æ®µ
        valid_records = []
        for i, r in enumerate(records):
            if not isinstance(r, dict):
                continue
            # æ·»åŠ  record_date å­—æ®µ
            r["record_date"] = data_date
            valid_records.append(r)
        for i, r in enumerate(records):
            if not isinstance(r, dict):
                continue


        # --- ä½¿ç”¨è®¾å¤‡ç”µå‹ç­‰çº§è¡¨çš„å›ºå®šè¡¨å ---
        table_name = "power_shubiandian"

        try:
            with self.db_manager.engine.begin() as conn:
                # --- åˆ›å»ºè¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰ï¼Œä¸¥æ ¼åŒ¹é…è®¾å¤‡ç”µå‹ç­‰çº§è¡¨ç»“æ„ ---
                create_table_sql = f"""
                CREATE TABLE IF NOT EXISTS `{table_name}` (
                    `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT 'è‡ªå¢ä¸»é”®ï¼Œå”¯ä¸€æ ‡è¯†ä¸€æ¡è®¾å¤‡ç”µå‹è®°å½•',
                    `record_date` date DEFAULT NULL COMMENT 'æ—¥æœŸï¼ˆå¦‚2025-09-18ï¼‰',
                    `device_name` varchar(300) DEFAULT NULL COMMENT 'è®¾å¤‡åç§°ï¼ˆå¦‚â€œ110kVç™½æ²™ç²¤æºªå…‰ä¼ç”µç«™...å¼€å…³ä½ç½®â€ï¼‰',
                    `voltage_level` varchar(50) DEFAULT NULL COMMENT 'ç”µå‹ç­‰çº§(kV)ï¼ˆå¦‚â€œ37kVâ€â€œ115kVâ€ï¼‰',
                    `sheet_name` varchar(255) DEFAULT NULL COMMENT 'æ•°æ®æ¥æºè¡¨åï¼ˆå¦‚â€œè®¾å¤‡ç”µå‹ç­‰çº§è¡¨20250918â€ï¼‰',
                    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT 'è®°å½•å…¥åº“æ—¶é—´',
                    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT 'è®°å½•æ›´æ–°æ—¶é—´',
                    PRIMARY KEY (`id`),
                    KEY `idx_device_name` (`device_name`) COMMENT 'è®¾å¤‡åç§°ç´¢å¼•',
                    KEY `idx_record_date` (`record_date`) COMMENT 'æ—¥æœŸç´¢å¼•',
                    KEY `idx_sheet_name` (`sheet_name`) COMMENT 'æ•°æ®æ¥æºç´¢å¼•'
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
                """
                conn.execute(text(create_table_sql))

                # --- æ’å…¥æ•°æ®ï¼Œå­—æ®µä¸è¡¨ç»“æ„ä¸¥æ ¼å¯¹åº” ---
                insert_sql = text(f"""
                INSERT INTO `{table_name}` (
                    `record_date`,
                    `device_name`,
                    `voltage_level`,
                    `sheet_name`
                ) VALUES (
                    :record_date,
                    :device_name,
                    :voltage_level,
                    :sheet_name
                )
                """)
                conn.execute(insert_sql, valid_records)

                # --- è·å–æ’å…¥ç»“æœï¼ˆé¢„è§ˆå‰10æ¡ï¼‰---
                preview_sql = text(f"""
                SELECT * FROM `{table_name}`
                WHERE `record_date` = :record_date
                ORDER BY `record_date`
                LIMIT 10;
                """)
                # preview_data = conn.execute(preview_sql, {"record_date": data_date}).fetchall()

            return True, table_name, len(valid_records), []

        except Exception as e:
            print(f"ä¿å­˜æ•°æ®æ—¶å‡ºé”™ï¼š{e}")
            return False, None, 0, []

    def save_to_generator_tech_database(self, records, data_date, sheet_name="æœºç»„æŠ€æœ¯å‚æ•°è¡¨"):
        """
        ä¿å­˜ç”µå‚æœºç»„æŠ€æœ¯å‚æ•°æ•°æ®åˆ°æ•°æ®åº“
        """
        if not records:
            print("âŒ æ²¡æœ‰å¯ä¿å­˜çš„è®°å½•")
            return False, None, 0, []

        if isinstance(records, pd.DataFrame):
            records = records.to_dict(orient="records")

        if not isinstance(records, list):
            print(f"âŒ records ç±»å‹é”™è¯¯: {type(records)}")
            return False, None, 0, []
        
        valid_records = []
        
        for i, r in enumerate(records):
            if not isinstance(r, dict):
                continue
            
            # ğŸ¯ ä½¿ç”¨è‹±æ–‡å­—æ®µåï¼ˆä¸è§£æå‡½æ•°è¾“å‡ºåŒ¹é…ï¼‰
            standardized_record = {
                "record_date": data_date,
                "power_plant_name": r.get("power_plant_name") or r.get("ç”µå‚åç§°"),
                "generator_name": r.get("generator_name") or r.get("æœºç»„åç§°"),
                "min_technical_output": r.get("min_technical_output") or r.get("æœ€å°æŠ€æœ¯å‡ºåŠ›(MW)"),
                "rated_output": r.get("rated_output") or r.get("é¢å®šå‡ºåŠ›(MW)"),
                "sheet_name": r.get("sheet_name") or sheet_name
            }
            
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ­£ç¡®å¤„ç†æœ€å°å‡ºåŠ›ä¸º0çš„æƒ…å†µ
            min_output = standardized_record["min_technical_output"]
            
            if min_output is not None:
                # å¦‚æœæ˜¯å­—ç¬¦ä¸²ç±»å‹
                if isinstance(min_output, str):
                    min_output_str = min_output.strip().lower()
                    # å¤„ç†å„ç§0å€¼è¡¨ç¤º
                    if min_output_str in ["0", "0.0", "0.00", "0.000", "0.0000", "0.00000"]:
                        standardized_record["min_technical_output"] = 0.0
                    elif min_output_str in ["none", "nan", "null", ""]:
                        # æ–°èƒ½æºç”µç«™æœ€å°å‡ºåŠ›åº”è¯¥ä¸º0ï¼Œä¸æ˜¯None
                        standardized_record["min_technical_output"] = 0.0
                    else:
                        try:
                            # å°è¯•è½¬æ¢ä¸ºæµ®ç‚¹æ•°
                            float_val = float(min_output)
                            standardized_record["min_technical_output"] = float_val
                        except (ValueError, TypeError):
                            # è½¬æ¢å¤±è´¥æ—¶ï¼Œæ–°èƒ½æºç”µç«™é»˜è®¤ä¸º0
                            standardized_record["min_technical_output"] = 0.0
                else:
                    # å·²ç»æ˜¯æ•°å­—ç±»å‹
                    try:
                        # ç¡®ä¿æ˜¯æµ®ç‚¹æ•°
                        float_val = float(min_output)
                        standardized_record["min_technical_output"] = float_val
                    except (ValueError, TypeError):
                        standardized_record["min_technical_output"] = 0.0
            else:
                # æœ€å°å‡ºåŠ›ä¸ºNoneæ—¶ï¼Œæ–°èƒ½æºç”µç«™é»˜è®¤ä¸º0
                standardized_record["min_technical_output"] = 0.0
            
            # ğŸ”§ å¤„ç†é¢å®šå‡ºåŠ›
            rated_output = standardized_record["rated_output"]
            if rated_output is not None:
                if isinstance(rated_output, str):
                    rated_str = rated_output.strip().lower()
                    if rated_str in ["none", "nan", "null", ""]:
                        standardized_record["rated_output"] = None
                    else:
                        try:
                            standardized_record["rated_output"] = float(rated_output)
                        except (ValueError, TypeError):
                            standardized_record["rated_output"] = None
                else:
                    try:
                        standardized_record["rated_output"] = float(rated_output)
                    except (ValueError, TypeError):
                        standardized_record["rated_output"] = None
            
            # å…³é”®å­—æ®µä¸èƒ½ä¸ºç©º
            if not standardized_record["generator_name"]:
                continue
                
            # ğŸ” è°ƒè¯•ï¼šæŸ¥çœ‹æ–°èƒ½æºç”µç«™çš„å¤„ç†ç»“æœ
            if i < 5 and standardized_record["min_technical_output"] == 0.0:
                print(f"ğŸ” æ–°èƒ½æºç”µç«™å¤„ç†: {standardized_record['generator_name']} æœ€å°å‡ºåŠ›è®¾ç½®ä¸º0.0")
                
            valid_records.append(standardized_record)

        if not valid_records:
            print("âŒ æ²¡æœ‰æœ‰æ•ˆè®°å½•å¯ä¿å­˜")
            return False, None, 0, []

        table_name = "generator_technical_parameters"

        try:
            with self.db_manager.engine.begin() as conn:
                # åˆ›å»ºè¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
                create_table_sql = f"""
                CREATE TABLE IF NOT EXISTS `{table_name}` (
                    `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT 'è‡ªå¢ä¸»é”®',
                    `record_date` date NOT NULL COMMENT 'æ•°æ®æ—¥æœŸ',
                    `power_plant_name` varchar(200) NOT NULL COMMENT 'ç”µå‚åç§°',
                    `generator_name` varchar(150) NOT NULL COMMENT 'æœºç»„åç§°',
                    `min_technical_output` decimal(10,4) DEFAULT NULL COMMENT 'æœ€å°æŠ€æœ¯å‡ºåŠ›(MW)',
                    `rated_output` decimal(10,4) DEFAULT NULL COMMENT 'é¢å®šå‡ºåŠ›(MW)',
                    `sheet_name` varchar(255) DEFAULT NULL COMMENT 'æ•°æ®æ¥æºè¡¨å',
                    `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT 'åˆ›å»ºæ—¶é—´',
                    `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT 'æ›´æ–°æ—¶é—´',
                    PRIMARY KEY (`id`),
                    UNIQUE KEY `uk_generator_date` (`generator_name`, `record_date`) COMMENT 'æœºç»„+æ—¥æœŸå”¯ä¸€ç´¢å¼•',
                    KEY `idx_power_plant` (`power_plant_name`) COMMENT 'ç”µå‚åç§°ç´¢å¼•',
                    KEY `idx_record_date` (`record_date`) COMMENT 'æ—¥æœŸç´¢å¼•',
                    KEY `idx_sheet_name` (`sheet_name`) COMMENT 'æ•°æ®æ¥æºç´¢å¼•'
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci COMMENT='ç”µå‚æœºç»„æŠ€æœ¯å‚æ•°è¡¨';
                """
                conn.execute(text(create_table_sql))

                # æ’å…¥æ•°æ®
                insert_sql = text(f"""
                INSERT INTO `{table_name}` (
                    `record_date`,
                    `power_plant_name`,
                    `generator_name`,
                    `min_technical_output`,
                    `rated_output`,
                    `sheet_name`
                ) VALUES (
                    :record_date,
                    :power_plant_name,
                    :generator_name,
                    :min_technical_output,
                    :rated_output,
                    :sheet_name
                )
                ON DUPLICATE KEY UPDATE
                    `power_plant_name` = VALUES(power_plant_name),
                    `min_technical_output` = VALUES(min_technical_output),
                    `rated_output` = VALUES(rated_output),
                    `sheet_name` = VALUES(sheet_name),
                    `update_time` = CURRENT_TIMESTAMP
                """)
                
                # ğŸ” è°ƒè¯•ï¼šæŸ¥çœ‹è¦æ’å…¥çš„æ•°æ®
                print("ğŸ” å‰5æ¡è¦æ’å…¥çš„æ•°æ®:")
                for i, rec in enumerate(valid_records[:5]):
                    print(f"  è®°å½•{i}: {rec['generator_name']} - æœ€å°å‡ºåŠ›: {rec['min_technical_output']} (ç±»å‹: {type(rec['min_technical_output'])})")
                
                result = conn.execute(insert_sql, valid_records)
                inserted_count = result.rowcount

                # è·å–æ’å…¥ç»“æœé¢„è§ˆ
                preview_sql = text(f"""
                SELECT 
                    `record_date`,
                    `power_plant_name`,
                    `generator_name`,
                    `min_technical_output`,
                    `rated_output`
                FROM `{table_name}`
                WHERE `record_date` = :record_date
                ORDER BY `id` DESC
                LIMIT 5;
                """)
                
                # è·å–æ•°æ®å¹¶è½¬æ¢ä¸ºæ™®é€šå­—å…¸åˆ—è¡¨
                preview_result = conn.execute(preview_sql, {"record_date": data_date})
                preview_data = []
                zero_min_output_count = 0
                
                for row in preview_result:
                    # è½¬æ¢SQLAlchemy Rowå¯¹è±¡ä¸ºæ™®é€šå­—å…¸
                    row_dict = {
                        "record_date": row.record_date.isoformat() if row.record_date else None,
                        "power_plant_name": row.power_plant_name,
                        "generator_name": row.generator_name,
                        "min_technical_output": float(row.min_technical_output) if row.min_technical_output is not None else None,
                        "rated_output": float(row.rated_output) if row.rated_output is not None else None
                    }
                    
                    # ç»Ÿè®¡æœ€å°å‡ºåŠ›ä¸º0çš„è®°å½•
                    if row.min_technical_output == 0 or row.min_technical_output == 0.0:
                        zero_min_output_count += 1
                    
                    preview_data.append(row_dict)

                print(f"âœ… æˆåŠŸä¿å­˜ {inserted_count} æ¡è®°å½•åˆ°è¡¨ `{table_name}`")
                print(f"ğŸ“Š ç»Ÿè®¡: æœ€å°å‡ºåŠ›ä¸º0çš„è®°å½•æœ‰ {zero_min_output_count} æ¡")
                
                if preview_data:
                    print("ğŸ“Š æ•°æ®é¢„è§ˆï¼ˆå‰5æ¡ï¼‰ï¼š")
                    for item in preview_data:
                        min_output_display = item['min_technical_output']
                        if min_output_display == 0 or min_output_display == 0.0:
                            min_output_display = "0.0"
                        print(f"   {item['power_plant_name']} - {item['generator_name']}: "
                            f"æœ€å°å‡ºåŠ›={min_output_display}MW, é¢å®šå‡ºåŠ›={item['rated_output']}MW")

                return True, table_name, inserted_count, preview_data

        except Exception as e:
            print(f"âŒ ä¿å­˜æ•°æ®æ—¶å‡ºé”™ï¼š{e}")
            import traceback
            traceback.print_exc()
            return False, None, 0, []
        
        
    def import_imformation_pred(self, excel_file):
        """è‡ªåŠ¨ç”Ÿæˆçš„å¯¼å…¥å‡½æ•°: ä¿¡æ¯æŠ«éœ²æŸ¥è¯¢é¢„æµ‹ä¿¡æ¯(2025-12-23).xlsx (ç±»)"""
        try:
            sheet_dict = pd.read_excel(excel_file, sheet_name=None, header=0)
        except Exception as e:
            print(f"âŒ æ— æ³•è¯»å–Excel: {e}")
            return False, None, 0, []

        power_data_records = []
        custom_table_records = []
        # â€œå¿…å¼€å¿…åœâ€ä¸¤å¼ è¡¨å»ºè®®å•ç‹¬å…¥åº“ï¼ˆä¿ç•™æ›´å¤šç»´åº¦ï¼‰ï¼Œé¿å…å¡è¿› power_data å¯¼è‡´å­—æ®µä¸¢å¤±ä¸”å¯¼å‡ºä¸æ¸…æ™°
        must_run_stop_group_constraint_records = []
        must_run_stop_unit_info_records = []
        data_date = None
        
        # å°è¯•ä»æ–‡ä»¶åæå–æ—¥æœŸ
        match = re.search(r'(\d{4}-\d{1,2}-\d{1,2})', str(excel_file))
        if match:
            data_date = datetime.datetime.strptime(match.group(1), '%Y-%m-%d').date()
        else:
            print(f"âš ï¸ æœªèƒ½åœ¨æ–‡ä»¶åä¸­è¯†åˆ«æ—¥æœŸï¼Œé»˜è®¤ä½¿ç”¨ä»Šæ—¥")
            data_date = datetime.date.today()

        # æ ¹æ®æ–‡ä»¶åè¯†åˆ«ç±»å‹
        file_name = str(excel_file)
        chinese_match = re.search(r'([\u4e00-\u9fff]+)', file_name)
        if chinese_match:
            data_type = chinese_match.group(1)
            print(f"ğŸ“ æ–‡ä»¶ç±»å‹è¯†åˆ«: {data_type}")
        else:
            data_type = "è‡ªåŠ¨å¯¼å…¥"
            print(f"âš ï¸ æœªèƒ½åœ¨æ–‡ä»¶åä¸­æ‰¾åˆ°æ±‰å­—ï¼Œé»˜è®¤ç±»å‹: {data_type}")

        sheet_names = list(sheet_dict.keys())

        # é€šè¿‡å¤„ç†å‡½æ•° docstring åå‘è§£æ sheet åç§°ï¼Œé¿å…ä¾èµ– Excel çš„ sheet é¡ºåºã€‚
        handlers_by_sheet = {}
        for attr in dir(self):
            if not attr.startswith("_process_imformation_pred_sheet_"):
                continue
            func = getattr(self, attr, None)
            if not callable(func):
                continue
            doc = (getattr(func, "__doc__", "") or "").strip()
            m = re.search(r"è‡ªåŠ¨ç”Ÿæˆçš„å¤„ç†å‡½æ•°:\s*(.*?)\(", doc)
            if not m:
                continue
            handlers_by_sheet[m.group(1).strip()] = func

        def resolve_handler(base_sheet_name, i):
            # 1) ç²¾ç¡®åŒ¹é…
            if base_sheet_name in handlers_by_sheet:
                return handlers_by_sheet[base_sheet_name], f"doc:{base_sheet_name}"
            # 2) æ¨¡ç³ŠåŒ¹é…ï¼šå–æœ€é•¿åŒ¹é…çš„ key
            best_func, best_key = None, None
            for k, f in handlers_by_sheet.items():
                if k and (k in base_sheet_name or base_sheet_name in k):
                    if best_key is None or len(k) > len(best_key):
                        best_key, best_func = k, f
            if best_func:
                return best_func, f"fuzzy:{best_key}"
            # 3) å›é€€ï¼šä»ç„¶æŒ‰ç´¢å¼•è°ƒç”¨ï¼ˆå…¼å®¹æ—§ç”Ÿæˆé€»è¾‘ï¼‰
            func_name = f"_process_imformation_pred_sheet_{i+1}"
            if hasattr(self, func_name):
                return getattr(self, func_name), f"index:{i+1}"
            return None, None

        def _looks_like_time_series_numeric(records):
            # æ—¶åºæ•°æ®å¿…é¡»æ»¡è¶³ï¼šæœ‰ record_time/valueï¼Œä¸” value å¤§æ¦‚ç‡ä¸ºæ•°å€¼ï¼ˆé¿å…æŠŠæ–‡æœ¬å‹æ—¶åºå¡è¿› power_data.value DECIMALï¼‰ã€‚
            if not records:
                return False
            first = records[0]
            if 'record_time' not in first or 'value' not in first:
                return False
            rt = first.get('record_time')
            if not (rt and isinstance(rt, str) and ':' in rt):
                return False

            sample = records[:50]
            ok = 0
            total = 0
            for r in sample:
                v = r.get('value')
                if v is None or (isinstance(v, float) and np.isnan(v)):
                    continue
                total += 1
                if isinstance(v, (int, float, np.number)) and not isinstance(v, bool):
                    ok += 1
                    continue
                if isinstance(v, str):
                    s = v.strip().replace(",", "")
                    try:
                        float(s)
                        ok += 1
                    except Exception:
                        pass
            # å¦‚æœé‡‡æ ·é‡Œå¤§éƒ¨åˆ†éç©º value å¯è½¬æˆæ•°å€¼ï¼Œåˆ™æŒ‰æ—¶åºå†™å…¥ power_data
            return total > 0 and (ok / total) >= 0.9

        # åŠ¨æ€å¤„ç†æ‰€æœ‰ Sheetï¼Œæ ¹æ®å†…å®¹æ¨¡å¼åˆ†å‘
        for i, sheet_name in enumerate(sheet_names):
            base_sheet_name = re.sub(r'\(\d{4}[-/]?\d{1,2}[-/]?\d{1,2}\)', '', str(sheet_name))
            base_sheet_name = re.sub(r'\d{4}[-/]?\d{1,2}[-/]?\d{1,2}', '', base_sheet_name).strip()

            # å¿…å¼€å¿…åœï¼ˆç¾¤ï¼‰çº¦æŸï¼šåŒ…å«æœºç»„ç¾¤/ç”µå‚/æœºç»„/æ•°æ®ç±»å‹ + 15åˆ†é’Ÿæ›²çº¿ï¼ˆå€¼å¯èƒ½æ˜¯å°æ•°/å®¹é‡ç­‰ï¼‰
            if base_sheet_name == "å¿…å¼€å¿…åœæœºç»„ï¼ˆç¾¤ï¼‰çº¦æŸé¢„æµ‹ä¿¡æ¯":
                try:
                    must_run_stop_group_constraint_records.extend(
                        self._process_must_run_stop_group_constraint_sheet(
                            sheet_dict[sheet_name], data_date, sheet_name, data_type
                        )
                    )
                except Exception as e:
                    print(f"âš ï¸ å¤„ç† Sheet '{sheet_name}' (å¿…å¼€å¿…åœæœºç»„ï¼ˆç¾¤ï¼‰çº¦æŸé¢„æµ‹ä¿¡æ¯) æ—¶å‡ºé”™: {e}")
                continue

            # å¿…å¼€å¿…åœæœºç»„ä¿¡æ¯ï¼šé€šå¸¸ä¸ºâ€œæ ‡ç­¾/ç±»å‹/åŸå› â€ç­‰æ–‡æœ¬æ—¶åºï¼ˆ15åˆ†é’Ÿï¼‰
            if base_sheet_name == "å¿…å¼€å¿…åœæœºç»„ä¿¡æ¯é¢„æµ‹ä¿¡æ¯":
                try:
                    must_run_stop_unit_info_records.extend(
                        self._process_must_run_stop_unit_info_sheet(
                            sheet_dict[sheet_name], data_date, sheet_name, data_type
                        )
                    )
                except Exception as e:
                    print(f"âš ï¸ å¤„ç† Sheet '{sheet_name}' (å¿…å¼€å¿…åœæœºç»„ä¿¡æ¯é¢„æµ‹ä¿¡æ¯) æ—¶å‡ºé”™: {e}")
                continue

            func, match_reason = resolve_handler(base_sheet_name, i)
            if not func:
                print(f"âš ï¸ æœªæ‰¾åˆ°å¤„ç† Sheet '{sheet_name}' (åŸºç¡€å '{base_sheet_name}') çš„å‡½æ•°")
                continue

            try:
                records = func(sheet_dict[sheet_name], data_date, sheet_name, data_type)

                # æ™ºèƒ½åˆ†å‘é€»è¾‘
                if records:
                    if _looks_like_time_series_numeric(records):
                        power_data_records.extend(records)
                    else:
                        custom_table_records.extend(records)

            except Exception as e:
                print(f"âš ï¸ å¤„ç† Sheet '{sheet_name}' (åŒ¹é… {match_reason}) æ—¶å‡ºé”™: {e}")

        if not power_data_records and not custom_table_records:
            # å…è®¸â€œåªæœ‰å¿…å¼€å¿…åœä¸¤å¼ è¡¨â€ä¹Ÿèƒ½å…¥åº“
            if not must_run_stop_group_constraint_records and not must_run_stop_unit_info_records:
                print("âŒ æ²¡æœ‰ç”Ÿæˆä»»ä½•æœ‰æ•ˆè®°å½•")
                return False, None, 0, []

        results = []
        
        # 1. ä¿å­˜æ—¶åºæ•°æ®åˆ° power_data
        if power_data_records:
            print(f"ğŸ“Š ä¿å­˜ {len(power_data_records)} æ¡æ—¶åºæ•°æ®åˆ° power_data")
            res_power = self.save_to_database(power_data_records, data_date)
            results.append(res_power)
            
        # 2. ä¿å­˜å…¶ä»–æ•°æ®åˆ°è‡ªå®šä¹‰è¡¨
        if custom_table_records:
            print(f"ğŸ“Š ä¿å­˜ {len(custom_table_records)} æ¡è‡ªå®šä¹‰æ•°æ®åˆ°ç‹¬ç«‹è¡¨")
            res_custom = self.save_to_imformation_pred_database(custom_table_records, data_date)
            results.append(res_custom)

        # 3. ä¿å­˜â€œå¿…å¼€å¿…åœæœºç»„ï¼ˆç¾¤ï¼‰çº¦æŸé¢„æµ‹ä¿¡æ¯â€
        if must_run_stop_group_constraint_records:
            print(f"ğŸ“Š ä¿å­˜ {len(must_run_stop_group_constraint_records)} æ¡å¿…å¼€å¿…åœæœºç»„ï¼ˆç¾¤ï¼‰çº¦æŸæ•°æ®åˆ°ç‹¬ç«‹è¡¨")
            res_mrsc = self.save_must_run_stop_group_constraint_ts(must_run_stop_group_constraint_records, data_date)
            results.append(res_mrsc)

        # 4. ä¿å­˜â€œå¿…å¼€å¿…åœæœºç»„ä¿¡æ¯é¢„æµ‹ä¿¡æ¯â€
        if must_run_stop_unit_info_records:
            print(f"ğŸ“Š ä¿å­˜ {len(must_run_stop_unit_info_records)} æ¡å¿…å¼€å¿…åœæœºç»„ä¿¡æ¯æ•°æ®åˆ°ç‹¬ç«‹è¡¨")
            res_mrui = self.save_must_run_stop_unit_info_ts(must_run_stop_unit_info_records, data_date)
            results.append(res_mrui)

        return tuple(results) if len(results) > 1 else (results[0] if results else False)

    def _process_must_run_stop_group_constraint_sheet(self, df, data_date, sheet_name, data_type):
        """
        è§£æâ€œå¿…å¼€å¿…åœæœºç»„ï¼ˆç¾¤ï¼‰çº¦æŸé¢„æµ‹ä¿¡æ¯â€ï¼š
        - å…ƒæ•°æ®åˆ—ï¼šæœºç»„ç¾¤å/æœºç»„å°æ•°/ç”µå‚ID/ç”µå‚åç§°/æœºç»„ID/æœºç»„åç§°/æ•°æ®ç±»å‹
        - 15åˆ†é’Ÿæ—¶é—´åˆ—ï¼š00:00..23:45
        ç›®æ ‡ï¼šä¿ç•™ç»´åº¦ï¼ŒæŒ‰ long è¡¨å…¥åº“ï¼Œä¾¿äºå¯¼å‡º/é€è§†ã€‚
        """
        records = []
        df = df.dropna(how="all")
        df.columns = [str(c).strip() for c in df.columns]

        time_cols = [c for c in df.columns if re.match(r"^\d{2}:\d{2}$", str(c))]
        if not time_cols:
            return records

        def _to_int(v):
            if v is None or (isinstance(v, float) and np.isnan(v)):
                return None
            if isinstance(v, (int, np.integer)) and not isinstance(v, bool):
                return int(v)
            if isinstance(v, float):
                if np.isnan(v):
                    return None
                return int(v)
            s = str(v).strip()
            if not s:
                return None
            try:
                return int(float(s.replace(",", "")))
            except Exception:
                return None

        def _to_float(v):
            if v is None or (isinstance(v, float) and np.isnan(v)):
                return None
            if isinstance(v, (int, float, np.number)) and not isinstance(v, bool):
                return float(v)
            s = str(v).strip()
            if not s:
                return None
            try:
                return float(s.replace(",", ""))
            except Exception:
                return None

        for _, row in df.iterrows():
            unit_group_name = str(row.get("æœºç»„ç¾¤å", "")).strip()
            if not unit_group_name:
                continue

            rec_base = {
                "record_date": data_date,
                "sheet_name": sheet_name,
                "type": data_type,
                "unit_group_name": unit_group_name,
                "unit_count": _to_int(row.get("æœºç»„å°æ•°")),
                "plant_id": _to_int(row.get("ç”µå‚ID")),
                "plant_name": str(row.get("ç”µå‚åç§°", "")).strip() or None,
                "unit_id": _to_int(row.get("æœºç»„ID")),
                "unit_name": str(row.get("æœºç»„åç§°", "")).strip() or None,
                "constraint_type": str(row.get("æ•°æ®ç±»å‹", "")).strip() or None,
            }

            for t in time_cols:
                v = row.get(t)
                if pd.isna(v):
                    continue

                v_num = _to_float(v)
                v_text = None if v_num is not None else str(v).strip()

                r = dict(rec_base)
                r["record_time"] = str(t).strip()
                r["value_num"] = v_num
                r["value_text"] = v_text
                records.append(r)

        return records

    def _process_must_run_stop_unit_info_sheet(self, df, data_date, sheet_name, data_type):
        """
        è§£æâ€œå¿…å¼€å¿…åœæœºç»„ä¿¡æ¯é¢„æµ‹ä¿¡æ¯â€ï¼š
        - å…ƒæ•°æ®åˆ—ï¼šç”µå‚åç§°/æœºç»„åç§°/æ•°æ®ç±»å‹ï¼ˆæ ‡ç­¾/ç±»å‹/åŸå› ...ï¼‰
        - 15åˆ†é’Ÿæ—¶é—´åˆ—ï¼š00:00..23:45
        å€¼é€šå¸¸ä¸ºæ–‡æœ¬ï¼ˆå¿…å¼€/å¿…åœ/åŸå› ç­‰ï¼‰ï¼ŒæŒ‰ long è¡¨å…¥åº“ã€‚
        """
        records = []
        df = df.dropna(how="all")
        df.columns = [str(c).strip() for c in df.columns]

        time_cols = [c for c in df.columns if re.match(r"^\d{2}:\d{2}$", str(c))]
        if not time_cols:
            return records

        for _, row in df.iterrows():
            plant_name = str(row.get("ç”µå‚åç§°", "")).strip()
            unit_name = str(row.get("æœºç»„åç§°", "")).strip()
            row_type = str(row.get("æ•°æ®ç±»å‹", "")).strip()
            if not (plant_name or unit_name):
                continue

            base = {
                "record_date": data_date,
                "sheet_name": sheet_name,
                "type": data_type,
                "plant_name": plant_name or None,
                "unit_name": unit_name or None,
                "row_type": row_type or None,
            }

            for t in time_cols:
                v = row.get(t)
                if pd.isna(v):
                    continue

                r = dict(base)
                r["record_time"] = str(t).strip()
                r["value_text"] = str(v).strip()
                records.append(r)

        return records

    def save_must_run_stop_group_constraint_ts(self, records, data_date):
        """å…¥åº“ï¼šå¿…å¼€å¿…åœæœºç»„ï¼ˆç¾¤ï¼‰çº¦æŸé¢„æµ‹ä¿¡æ¯ï¼ˆ15åˆ†é’Ÿ long è¡¨ï¼‰ã€‚"""
        table_name = "info_disclose_pred_must_run_stop_group_constraint_ts"
        preview_data = []

        try:
            with self.db_manager.engine.begin() as conn:
                conn.execute(
                    text(
                        f"""
                        CREATE TABLE IF NOT EXISTS `{table_name}` (
                          `id` BIGINT NOT NULL AUTO_INCREMENT,
                          `record_date` DATE NOT NULL,
                          `record_time` TIME NOT NULL,
                          `unit_group_name` VARCHAR(255) NULL,
                          `unit_count` INT NULL,
                          `plant_id` INT NULL,
                          `plant_name` VARCHAR(255) NULL,
                          `unit_id` INT NULL,
                          `unit_name` VARCHAR(255) NULL,
                          `constraint_type` VARCHAR(255) NULL,
                          `value_num` DECIMAL(18,4) NULL,
                          `value_text` VARCHAR(255) NULL,
                          `sheet_name` VARCHAR(255) NULL,
                          `type` VARCHAR(255) NULL,
                          `created_at` TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
                          PRIMARY KEY (`id`),
                          KEY `idx_record_date` (`record_date`),
                          KEY `idx_group` (`record_date`, `unit_group_name`(64))
                        ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
                        """
                    )
                )

                # åŒä¸€å¤©é‡å¯¼ï¼šåˆ é™¤æ—§æ•°æ®å†æ’å…¥ï¼ˆé¿å…é‡å¤ï¼‰
                conn.execute(text(f"DELETE FROM `{table_name}` WHERE record_date = :d"), {"d": data_date})

                if not records:
                    return True, table_name, 0, []

                cols = [
                    "record_date",
                    "record_time",
                    "unit_group_name",
                    "unit_count",
                    "plant_id",
                    "plant_name",
                    "unit_id",
                    "unit_name",
                    "constraint_type",
                    "value_num",
                    "value_text",
                    "sheet_name",
                    "type",
                ]
                stmt = text(
                    f"INSERT INTO `{table_name}` ({', '.join('`'+c+'`' for c in cols)}) "
                    f"VALUES ({', '.join(':'+c for c in cols)})"
                )

                batch_size = 500
                for i in range(0, len(records), batch_size):
                    conn.execute(stmt, records[i : i + batch_size])

                preview_data = records[:10]

            return True, table_name, len(records), preview_data
        except Exception as e:
            print(f"âŒ ä¿å­˜å¿…å¼€å¿…åœæœºç»„ï¼ˆç¾¤ï¼‰çº¦æŸæ•°æ®å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False, table_name, 0, []

    def save_must_run_stop_unit_info_ts(self, records, data_date):
        """å…¥åº“ï¼šå¿…å¼€å¿…åœæœºç»„ä¿¡æ¯é¢„æµ‹ä¿¡æ¯ï¼ˆ15åˆ†é’Ÿ long è¡¨ï¼Œæ–‡æœ¬ä¸ºä¸»ï¼‰ã€‚"""
        table_name = "info_disclose_pred_must_run_stop_unit_info_ts"
        preview_data = []

        try:
            with self.db_manager.engine.begin() as conn:
                conn.execute(
                    text(
                        f"""
                        CREATE TABLE IF NOT EXISTS `{table_name}` (
                          `id` BIGINT NOT NULL AUTO_INCREMENT,
                          `record_date` DATE NOT NULL,
                          `record_time` TIME NOT NULL,
                          `plant_name` VARCHAR(255) NULL,
                          `unit_name` VARCHAR(255) NULL,
                          `row_type` VARCHAR(255) NULL,
                          `value_text` TEXT NULL,
                          `sheet_name` VARCHAR(255) NULL,
                          `type` VARCHAR(255) NULL,
                          `created_at` TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
                          PRIMARY KEY (`id`),
                          KEY `idx_record_date` (`record_date`),
                          KEY `idx_unit` (`record_date`, `plant_name`(64), `unit_name`(64))
                        ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
                        """
                    )
                )

                conn.execute(text(f"DELETE FROM `{table_name}` WHERE record_date = :d"), {"d": data_date})

                if not records:
                    return True, table_name, 0, []

                cols = [
                    "record_date",
                    "record_time",
                    "plant_name",
                    "unit_name",
                    "row_type",
                    "value_text",
                    "sheet_name",
                    "type",
                ]
                stmt = text(
                    f"INSERT INTO `{table_name}` ({', '.join('`'+c+'`' for c in cols)}) "
                    f"VALUES ({', '.join(':'+c for c in cols)})"
                )

                batch_size = 500
                for i in range(0, len(records), batch_size):
                    conn.execute(stmt, records[i : i + batch_size])

                preview_data = records[:10]

            return True, table_name, len(records), preview_data
        except Exception as e:
            print(f"âŒ ä¿å­˜å¿…å¼€å¿…åœæœºç»„ä¿¡æ¯æ•°æ®å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False, table_name, 0, []

    def _process_imformation_pred_sheet_1(self, df, data_date, sheet_name, data_type):
        """è‡ªåŠ¨ç”Ÿæˆçš„å¤„ç†å‡½æ•°: è´Ÿè·é¢„æµ‹ä¿¡æ¯(2025-12-23) (æ¨¡å¼: time_series_matrix)"""
        records = []
        df = df.dropna(how='all')
        df.columns = [str(c).strip() for c in df.columns]
        
        # è¯†åˆ«æ—¶é—´åˆ—
        time_cols = [c for c in df.columns if re.match(r'^\d{1,2}:\d{2}$', c)]

        # è¿‡æ»¤æ‰â€œç±»å‹â€ç­‰éæŒ‡æ ‡è¡Œï¼Œé¿å…é€šé“åç§°è¢«æ±¡æŸ“
        if "ç±»å‹" in df.columns and "é€šé“åç§°" in df.columns:
            df = df[df["ç±»å‹"].astype(str).str.strip().isin(["é¢„æµ‹"])]
        
        for _, row in df.iterrows():
            # å‡è®¾ 'ç±»å‹' åˆ—æ˜¯æŒ‡æ ‡åç§°
            channel_name = str(row.get('é€šé“åç§°', 'Unknown')).strip()
            
            for t in time_cols:
                val = row[t]
                if pd.isna(val): continue
                
                records.append({
                    'record_date': data_date,
                    'record_time': t,
                    'channel_name': channel_name,
                    'value': val,
                    'sheet_name': sheet_name,
                    'type': data_type,
                    'created_at': datetime.datetime.now()
                })
        return records

    def _process_imformation_pred_sheet_2(self, df, data_date, sheet_name, data_type):
        """è‡ªåŠ¨ç”Ÿæˆçš„å¤„ç†å‡½æ•°: åœ°æ–¹ç”µé¢„æµ‹ä¿¡æ¯(2025-12-23) (æ¨¡å¼: time_series_matrix)"""
        records = []
        df = df.dropna(how='all')
        df.columns = [str(c).strip() for c in df.columns]
        
        # è¯†åˆ«æ—¶é—´åˆ—
        time_cols = [c for c in df.columns if re.match(r'^\d{1,2}:\d{2}$', c)]

        # è¿‡æ»¤æ‰â€œç±»å‹â€ç­‰éæŒ‡æ ‡è¡Œï¼Œé¿å…é€šé“åç§°è¢«æ±¡æŸ“
        if "ç±»å‹" in df.columns and "é€šé“åç§°" in df.columns:
            df = df[df["ç±»å‹"].astype(str).str.strip().isin(["é¢„æµ‹"])]
        
        for _, row in df.iterrows():
            # å‡è®¾ 'ç±»å‹' åˆ—æ˜¯æŒ‡æ ‡åç§°
            channel_name = str(row.get('é€šé“åç§°', 'Unknown')).strip()
            
            for t in time_cols:
                val = row[t]
                if pd.isna(val): continue
                
                records.append({
                    'record_date': data_date,
                    'record_time': t,
                    'channel_name': channel_name,
                    'value': val,
                    'sheet_name': sheet_name,
                    'type': data_type,
                    'created_at': datetime.datetime.now()
                })
        return records

    def _process_imformation_pred_sheet_3(self, df, data_date, sheet_name, data_type):
        """è‡ªåŠ¨ç”Ÿæˆçš„å¤„ç†å‡½æ•°: å‘ç”µæ€»å‡ºåŠ›é¢„æµ‹ä¿¡æ¯(2025-12-23) (æ¨¡å¼: time_series_matrix)"""
        records = []
        df = df.dropna(how='all')
        df.columns = [str(c).strip() for c in df.columns]
        
        # è¯†åˆ«æ—¶é—´åˆ—
        time_cols = [c for c in df.columns if re.match(r'^\d{1,2}:\d{2}$', c)]
        
        for _, row in df.iterrows():
            # å‡è®¾ 'ç±»å‹' åˆ—æ˜¯æŒ‡æ ‡åç§°
            channel_name = str(row.get('ç±»å‹', 'Unknown')).strip()
            
            for t in time_cols:
                val = row[t]
                if pd.isna(val): continue
                
                records.append({
                    'record_date': data_date,
                    'record_time': t,
                    'channel_name': channel_name,
                    'value': val,
                    'sheet_name': sheet_name,
                    'type': data_type,
                    'created_at': datetime.datetime.now()
                })
        return records

    def _process_imformation_pred_sheet_4(self, df, data_date, sheet_name, data_type):
        """è‡ªåŠ¨ç”Ÿæˆçš„å¤„ç†å‡½æ•°: ç°è´§æ–°èƒ½æºæ€»å‡ºåŠ›(2025-12-23) (æ¨¡å¼: time_series_matrix)"""
        records = []
        df = df.dropna(how='all')
        df.columns = [str(c).strip() for c in df.columns]
        
        # è¯†åˆ«æ—¶é—´åˆ—
        time_cols = [c for c in df.columns if re.match(r'^\d{1,2}:\d{2}$', c)]
        
        for _, row in df.iterrows():
            # å‡è®¾ 'ç±»å‹' åˆ—æ˜¯æŒ‡æ ‡åç§°
            channel_name = str(row.get('ç±»å‹', 'Unknown')).strip()
            
            for t in time_cols:
                val = row[t]
                if pd.isna(val): continue
                
                records.append({
                    'record_date': data_date,
                    'record_time': t,
                    'channel_name': channel_name,
                    'value': val,
                    'sheet_name': sheet_name,
                    'type': data_type,
                    'created_at': datetime.datetime.now()
                })
        return records

    def _process_imformation_pred_sheet_5(self, df, data_date, sheet_name, data_type):
        """è‡ªåŠ¨ç”Ÿæˆçš„å¤„ç†å‡½æ•°: ç»Ÿè°ƒæ–°èƒ½æºå‡ºåŠ›ä¿¡æ¯(2025-12-23) (æ¨¡å¼: time_series_matrix)"""
        records = []
        df = df.dropna(how='all')
        df.columns = [str(c).strip() for c in df.columns]
        
        # è¯†åˆ«æ—¶é—´åˆ—
        time_cols = [c for c in df.columns if re.match(r'^\d{1,2}:\d{2}$', c)]
        
        for _, row in df.iterrows():
            # å‡è®¾ 'ç±»å‹' åˆ—æ˜¯æŒ‡æ ‡åç§°
            channel_name = str(row.get('ç±»å‹', 'Unknown')).strip()
            
            for t in time_cols:
                val = row[t]
                if pd.isna(val): continue
                
                records.append({
                    'record_date': data_date,
                    'record_time': t,
                    'channel_name': channel_name,
                    'value': val,
                    'sheet_name': sheet_name,
                    'type': data_type,
                    'created_at': datetime.datetime.now()
                })
        return records

    def _process_imformation_pred_sheet_6(self, df, data_date, sheet_name, data_type):
        """è‡ªåŠ¨ç”Ÿæˆçš„å¤„ç†å‡½æ•°: æ°´ç”µï¼ˆå«æŠ½è“„ï¼‰æ€»å‡ºåŠ›é¢„æµ‹ä¿¡æ¯(2025-12-23) (æ¨¡å¼: time_series_matrix)"""
        records = []
        df = df.dropna(how='all')
        df.columns = [str(c).strip() for c in df.columns]
        
        # è¯†åˆ«æ—¶é—´åˆ—
        time_cols = [c for c in df.columns if re.match(r'^\d{1,2}:\d{2}$', c)]
        
        for _, row in df.iterrows():
            # å‡è®¾ 'ç±»å‹' åˆ—æ˜¯æŒ‡æ ‡åç§°
            channel_name = str(row.get('ç±»å‹', 'Unknown')).strip()
            
            for t in time_cols:
                val = row[t]
                if pd.isna(val): continue
                
                records.append({
                    'record_date': data_date,
                    'record_time': t,
                    'channel_name': channel_name,
                    'value': val,
                    'sheet_name': sheet_name,
                    'type': data_type,
                    'created_at': datetime.datetime.now()
                })
        return records

    def _process_imformation_pred_sheet_7(self, df, data_date, sheet_name, data_type):
        """è‡ªåŠ¨ç”Ÿæˆçš„å¤„ç†å‡½æ•°: æŠ½è“„ç”µç«™å‡ºåŠ›è®¡åˆ’(2025-12-23) (æ¨¡å¼: time_series_matrix)"""
        records = []
        df = df.dropna(how='all')
        df.columns = [str(c).strip() for c in df.columns]
        
        # è¯†åˆ«æ—¶é—´åˆ—
        time_cols = [c for c in df.columns if re.match(r'^\d{1,2}:\d{2}$', c)]
        
        for _, row in df.iterrows():
            # å‡è®¾ 'ç±»å‹' åˆ—æ˜¯æŒ‡æ ‡åç§°
            channel_name = str(row.get('ç±»å‹', 'Unknown')).strip()
            
            for t in time_cols:
                val = row[t]
                if pd.isna(val): continue
                
                records.append({
                    'record_date': data_date,
                    'record_time': t,
                    'channel_name': channel_name,
                    'value': val,
                    'sheet_name': sheet_name,
                    'type': data_type,
                    'created_at': datetime.datetime.now()
                })
        return records

    def _process_imformation_pred_sheet_8(self, df, data_date, sheet_name, data_type):
        """è‡ªåŠ¨ç”Ÿæˆçš„å¤„ç†å‡½æ•°: æœºç»„æ£€ä¿®é¢„æµ‹ä¿¡æ¯(2025-12-23) (æ¨¡å¼: generic_table)"""
        records = []
        df = df.dropna(how='all')
        df.columns = [str(c).strip() for c in df.columns]
        
        # é€šç”¨è¡¨æ ¼å¤„ç† (ç›´æ¥æ˜ å°„æ‰€æœ‰åˆ—)
        for _, row in df.iterrows():
            record = {
                'record_date': data_date,
                'sheet_name': sheet_name,
                'type': data_type,
                'created_at': datetime.datetime.now()
            }
            # åŠ¨æ€æ˜ å°„æ‰€æœ‰åˆ—
            for col in df.columns:
                val = row[col]
                if pd.notna(val):
                    record[col] = val
            records.append(record)
        return records

    def _process_imformation_pred_sheet_9(self, df, data_date, sheet_name, data_type):
        """è‡ªåŠ¨ç”Ÿæˆçš„å¤„ç†å‡½æ•°: è¾“å˜ç”µæ£€ä¿®é¢„æµ‹ä¿¡æ¯(2025-12-23) (æ¨¡å¼: generic_table)"""
        records = []
        df = df.dropna(how='all')
        df.columns = [str(c).strip() for c in df.columns]
        
        # é€šç”¨è¡¨æ ¼å¤„ç† (ç›´æ¥æ˜ å°„æ‰€æœ‰åˆ—)
        for _, row in df.iterrows():
            record = {
                'record_date': data_date,
                'sheet_name': sheet_name,
                'type': data_type,
                'created_at': datetime.datetime.now()
            }
            # åŠ¨æ€æ˜ å°„æ‰€æœ‰åˆ—
            for col in df.columns:
                val = row[col]
                if pd.notna(val):
                    record[col] = val
            records.append(record)
        return records

    def _process_imformation_pred_sheet_10(self, df, data_date, sheet_name, data_type):
        """è‡ªåŠ¨ç”Ÿæˆçš„å¤„ç†å‡½æ•°: æœºç»„æ£€ä¿®å®¹é‡é¢„æµ‹ä¿¡æ¯(2025-12-23) (æ¨¡å¼: generic_table)"""
        records = []
        df = df.dropna(how='all')
        df.columns = [str(c).strip() for c in df.columns]
        
        # é€šç”¨è¡¨æ ¼å¤„ç† (ç›´æ¥æ˜ å°„æ‰€æœ‰åˆ—)
        for _, row in df.iterrows():
            record = {
                'record_date': data_date,
                'sheet_name': sheet_name,
                'type': data_type,
                'created_at': datetime.datetime.now()
            }
            # åŠ¨æ€æ˜ å°„æ‰€æœ‰åˆ—
            for col in df.columns:
                val = row[col]
                if pd.notna(val):
                    record[col] = val
            records.append(record)
        return records

    def _process_imformation_pred_sheet_11(self, df, data_date, sheet_name, data_type):
        """è‡ªåŠ¨ç”Ÿæˆçš„å¤„ç†å‡½æ•°: å¤‡ç”¨é¢„æµ‹ä¿¡æ¯(2025-12-23) (æ¨¡å¼: time_series_matrix)"""
        records = []
        df = df.dropna(how='all')
        df.columns = [str(c).strip() for c in df.columns]
        
        # è¯†åˆ«æ—¶é—´åˆ—
        time_cols = [c for c in df.columns if re.match(r'^\d{1,2}:\d{2}$', c)]
        
        for _, row in df.iterrows():
            # å‡è®¾ 'æ•°æ®é¡¹' åˆ—æ˜¯æŒ‡æ ‡åç§°
            channel_name = str(row.get('æ•°æ®é¡¹', 'Unknown')).strip()
            
            for t in time_cols:
                val = row[t]
                if pd.isna(val): continue
                
                records.append({
                    'record_date': data_date,
                    'record_time': t,
                    'channel_name': channel_name,
                    'value': val,
                    'sheet_name': sheet_name,
                    'type': data_type,
                    'created_at': datetime.datetime.now()
                })
        return records

    def _process_imformation_pred_sheet_12(self, df, data_date, sheet_name, data_type):
        """è‡ªåŠ¨ç”Ÿæˆçš„å¤„ç†å‡½æ•°: é˜»å¡é¢„æµ‹ä¿¡æ¯(2025-12-23) (æ¨¡å¼: generic_table)"""
        records = []
        df = df.dropna(how='all')
        df.columns = [str(c).strip() for c in df.columns]
        
        # é€šç”¨è¡¨æ ¼å¤„ç† (ç›´æ¥æ˜ å°„æ‰€æœ‰åˆ—)
        for _, row in df.iterrows():
            record = {
                'record_date': data_date,
                'sheet_name': sheet_name,
                'type': data_type,
                'created_at': datetime.datetime.now()
            }
            # åŠ¨æ€æ˜ å°„æ‰€æœ‰åˆ—
            for col in df.columns:
                val = row[col]
                if pd.notna(val):
                    record[col] = val
            records.append(record)
        return records

    def _process_imformation_pred_sheet_13(self, df, data_date, sheet_name, data_type):
        """è‡ªåŠ¨ç”Ÿæˆçš„å¤„ç†å‡½æ•°: æ—¥å‰é˜»å¡æ–­é¢ä¿¡æ¯(2025-12-23) (æ¨¡å¼: time_series_matrix)"""
        records = []
        df = df.dropna(how='all')
        df.columns = [str(c).strip() for c in df.columns]
        
        # è¯†åˆ«æ—¶é—´åˆ—
        time_cols = [c for c in df.columns if re.match(r'^\d{1,2}:\d{2}$', c)]
        
        for _, row in df.iterrows():
            # å‡è®¾ 'æ–­é¢åç§°' åˆ—æ˜¯æŒ‡æ ‡åç§°
            channel_name = str(row.get('æ–­é¢åç§°', 'Unknown')).strip()
            
            for t in time_cols:
                val = row[t]
                if pd.isna(val): continue
                
                records.append({
                    'record_date': data_date,
                    'record_time': t,
                    'channel_name': channel_name,
                    'value': val,
                    'sheet_name': sheet_name,
                    'type': data_type,
                    'created_at': datetime.datetime.now()
                })
        return records

    def _process_imformation_pred_sheet_14(self, df, data_date, sheet_name, data_type):
        """è‡ªåŠ¨ç”Ÿæˆçš„å¤„ç†å‡½æ•°: å¿…å¼€å¿…åœæœºç»„ï¼ˆç¾¤ï¼‰çº¦æŸé¢„æµ‹ä¿¡æ¯(2025-12-23) (æ¨¡å¼: time_series_matrix)"""
        records = []
        df = df.dropna(how='all')
        df.columns = [str(c).strip() for c in df.columns]

        # è¯†åˆ«æ—¶é—´åˆ—
        time_cols = [c for c in df.columns if re.match(r'^\d{1,2}:\d{2}$', c)]

        for _, row in df.iterrows():
            channel_name = str(row.get('æœºç»„ç¾¤å', 'Unknown')).strip()

            for t in time_cols:
                val = row[t]
                if pd.isna(val):
                    continue
                # è¯¥è¡¨å¸¸è§ä¸ºæ–‡æœ¬çº¦æŸå€¼ï¼ˆå¦‚â€œå¿…å¼€/å¿…åœ/è‡ªç”±ä¼˜åŒ–â€ï¼‰ï¼Œä¸ºå†™å…¥ power_data.value(DECIMAL) åšæ•°å€¼åŒ–ã€‚
                if isinstance(val, str):
                    s = val.strip()
                    if s in {"å¿…å¼€", "å¿…é¡»å¼€æœº", "å¼€æœº"}:
                        val = 1
                    elif s in {"å¿…åœ", "å¿…é¡»åœæœº", "åœæœº"}:
                        val = -1
                    elif s in {"è‡ªç”±ä¼˜åŒ–", "æ— çº¦æŸ", "æ­£å¸¸"}:
                        val = 0
                    else:
                        # å…œåº•ï¼šå°è¯•æŠŠå­—ç¬¦ä¸²æ•°å€¼åŒ–
                        try:
                            val = float(s.replace(",", ""))
                        except Exception:
                            # ä¿ç•™åŸå€¼ï¼ˆè‹¥ä»ä¸ºå­—ç¬¦ä¸²ï¼Œå°†è¢« import_imformation_pred åˆ†æµåˆ°è‡ªå®šä¹‰è¡¨ï¼‰
                            val = s

                records.append({
                    'record_date': data_date,
                    'record_time': t,
                    'channel_name': channel_name,
                    'value': val,
                    'sheet_name': sheet_name,
                    'type': data_type,
                    'created_at': datetime.datetime.now()
                })
        return records

    def _process_imformation_pred_sheet_15(self, df, data_date, sheet_name, data_type):
        """è‡ªåŠ¨ç”Ÿæˆçš„å¤„ç†å‡½æ•°: å¿…å¼€å¿…åœæœºç»„ä¿¡æ¯é¢„æµ‹ä¿¡æ¯(2025-12-23) (æ¨¡å¼: time_series_matrix)"""
        records = []
        df = df.dropna(how='all')
        df.columns = [str(c).strip() for c in df.columns]

        # è¯†åˆ«æ—¶é—´åˆ—
        time_cols = [c for c in df.columns if re.match(r'^\d{1,2}:\d{2}$', c)]

        for _, row in df.iterrows():
            channel_name = (
                str(row.get('ç”µå‚åç§°', 'Unknown')).strip()
                + str(row.get('æœºç»„åç§°', 'Unknown')).strip()
                + str(row.get('æ•°æ®ç±»å‹', 'Unknown')).strip()
            )

            for t in time_cols:
                val = row[t]
                if pd.isna(val):
                    continue

                records.append({
                    'record_date': data_date,
                    'record_time': t,
                    'channel_name': channel_name,
                    'value': val,
                    'sheet_name': sheet_name,
                    'type': data_type,
                    'created_at': datetime.datetime.now()
                })
        return records

    def _process_imformation_pred_sheet_16(self, df, data_date, sheet_name, data_type):
        """è‡ªåŠ¨ç”Ÿæˆçš„å¤„ç†å‡½æ•°: å¼€åœæœºä¸æ»¡è¶³æœ€å°çº¦æŸæ—¶é—´æœºç»„ä¿¡æ¯(2025-12-23) (æ¨¡å¼: time_series_matrix)"""
        records = []
        df = df.dropna(how='all')
        df.columns = [str(c).strip() for c in df.columns]
        
        # è¯†åˆ«æ—¶é—´åˆ—
        time_cols = [c for c in df.columns if re.match(r'^\d{1,2}:\d{2}$', c)]
        
        for _, row in df.iterrows():
            # å‡è®¾ 'æœºç»„åç§°' åˆ—æ˜¯æŒ‡æ ‡åç§°
            channel_name = str(row.get('æœºç»„åç§°', 'Unknown')).strip()
            
            for t in time_cols:
                val = row[t]
                if val == "è‡ªç”±ä¼˜åŒ–":
                    val = 1
                else:
                    val = 0
                if pd.isna(val): continue
                
                records.append({
                    'record_date': data_date,
                    'record_time': t,
                    'channel_name': channel_name,
                    'value': val,
                    'sheet_name': sheet_name,
                    'type': data_type,
                    'created_at': datetime.datetime.now()
                })
        return records

    def _process_imformation_pred_sheet_17(self, df, data_date, sheet_name, data_type):
        """è‡ªåŠ¨ç”Ÿæˆçš„å¤„ç†å‡½æ•°: å¿…å¼€å¿…åœå®¹é‡é¢„æµ‹ä¿¡æ¯(2025-12-23) (æ¨¡å¼: standard_list)"""
        records = []
        df = df.dropna(how='all')
        df.columns = [str(c).strip() for c in df.columns]
        
        # æ ‡å‡†åˆ—è¡¨å¤„ç†
        for _, row in df.iterrows():
            # è§£ææ—¥æœŸï¼ˆéƒ¨åˆ†æ–‡ä»¶å¯èƒ½æ²¡æœ‰â€œæ—¥æœŸâ€åˆ—ï¼‰
            date_val = row.get('æ—¥æœŸ')
            r_date = pd.to_datetime(date_val).date() if pd.notna(date_val) else data_date
            channel_val = row.get('ç±»å‹')
            channel = str(channel_val).strip() if pd.notna(channel_val) else "Unknown"
            
            # éå†å¯èƒ½çš„æ•°å€¼åˆ—
            value_cols = ['åºå·', 'å¿…å¼€æœºç»„å®¹é‡(MW)', 'å¿…åœæœºç»„å®¹é‡(MW)']
            for col in value_cols:
                if col not in df.columns:
                    continue
                val = row.get(col)
                if pd.isna(val): continue
                
                # å¦‚æœæœ‰å¤šåˆ—æ•°å€¼ï¼Œå°†åˆ—åæ‹¼æ¥åˆ° channel_name
                final_channel = f'{channel}-{col}' if len(value_cols) > 1 else channel
                
                records.append({
                    'record_date': r_date,
                    'record_time': None,
                    'channel_name': final_channel,
                    'value': val,
                    'sheet_name': sheet_name,
                    'type': data_type,
                    'created_at': datetime.datetime.now()
                })
        return records

    def _process_imformation_pred_sheet_18(self, df, data_date, sheet_name, data_type):
        """è‡ªåŠ¨ç”Ÿæˆçš„å¤„ç†å‡½æ•°: æœºç»„å‡ºåŠ›å—é™æƒ…å†µ(2025-12-23) (æ¨¡å¼: generic_table)"""
        records = []
        df = df.dropna(how='all')
        df.columns = [str(c).strip() for c in df.columns]
        
        # é€šç”¨è¡¨æ ¼å¤„ç† (ç›´æ¥æ˜ å°„æ‰€æœ‰åˆ—)
        for _, row in df.iterrows():
            record = {
                'record_date': data_date,
                'sheet_name': sheet_name,
                'type': data_type,
                'created_at': datetime.datetime.now()
            }
            # åŠ¨æ€æ˜ å°„æ‰€æœ‰åˆ—
            for col in df.columns:
                val = row[col]
                if pd.notna(val):
                    record[col] = val
            records.append(record)
        return records

    def _process_imformation_pred_sheet_19(self, df, data_date, sheet_name, data_type):
        """è‡ªåŠ¨ç”Ÿæˆçš„å¤„ç†å‡½æ•°: å‚¨èƒ½æœºç»„æŒ‡å®šæ¨¡å¼æ¸…å•(2025-12-23) (æ¨¡å¼: generic_table)"""
        records = []
        df = df.dropna(how='all')
        df.columns = [str(c).strip() for c in df.columns]
        
        # é€šç”¨è¡¨æ ¼å¤„ç† (ç›´æ¥æ˜ å°„æ‰€æœ‰åˆ—)
        for _, row in df.iterrows():
            record = {
                'record_date': data_date,
                'sheet_name': sheet_name,
                'type': data_type,
                'created_at': datetime.datetime.now()
            }
            # åŠ¨æ€æ˜ å°„æ‰€æœ‰åˆ—
            for col in df.columns:
                val = row[col]
                if pd.notna(val):
                    record[col] = val
            records.append(record)
        return records

    def _process_imformation_pred_sheet_20(self, df, data_date, sheet_name, data_type):
        """è‡ªåŠ¨ç”Ÿæˆçš„å¤„ç†å‡½æ•°: æ—¥å‰å‡ºæ¸…æƒ…å†µ-æœºç»„è¯¦æƒ…ï¼ˆ2025-12-23ï¼‰ (æ¨¡å¼: time_series_matrix)"""
        records = []
        df = df.dropna(how='all')
        df.columns = [str(c).strip() for c in df.columns]
        
        # è¯†åˆ«æ—¶é—´åˆ—
        time_cols = [c for c in df.columns if re.match(r'^\d{1,2}:\d{2}$', c)]
        
        for _, row in df.iterrows():
            # å‡è®¾ 'ç”µå‚åç§°' åˆ—æ˜¯æŒ‡æ ‡åç§°
            channel_name = str(row.get('ç”µå‚åç§°', 'Unknown')).strip()
            
            for t in time_cols:
                val = row[t]
                if pd.isna(val): continue
                
                records.append({
                    'record_date': data_date,
                    'record_time': t,
                    'channel_name': channel_name,
                    'value': val,
                    'sheet_name': sheet_name,
                    'type': data_type,
                    'created_at': datetime.datetime.now()
                })
        return records

    def _process_imformation_pred_sheet_21(self, df, data_date, sheet_name, data_type):
        """è‡ªåŠ¨ç”Ÿæˆçš„å¤„ç†å‡½æ•°: æ—¥å‰å‡ºæ¸…æƒ…å†µ-èŠ‚ç‚¹è¯¦æƒ…ï¼ˆ2025-12-23ï¼‰ (æ¨¡å¼: time_series_matrix)"""
        records = []
        df = df.dropna(how='all')
        df.columns = [str(c).strip() for c in df.columns]
        
        # è¯†åˆ«æ—¶é—´åˆ—
        time_cols = [c for c in df.columns if re.match(r'^\d{1,2}:\d{2}$', c)]
        
        for _, row in df.iterrows():
            # å‡è®¾ 'åœ°åŒº' åˆ—æ˜¯æŒ‡æ ‡åç§°
            channel_name = str(row.get('åœ°åŒº', 'Unknown')).strip()
            
            for t in time_cols:
                val = row[t]
                if pd.isna(val): continue
                
                records.append({
                    'record_date': data_date,
                    'record_time': t,
                    'channel_name': channel_name,
                    'value': val,
                    'sheet_name': sheet_name,
                    'type': data_type,
                    'created_at': datetime.datetime.now()
                })
        return records
